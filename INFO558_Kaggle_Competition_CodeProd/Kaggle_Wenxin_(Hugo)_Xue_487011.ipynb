{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Kaggle_Wenxin (Hugo) Xue_487011.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HP1JCbL6yean"
      },
      "source": [
        "# **Kaggle Project Code**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cENqwAsurByy"
      },
      "source": [
        "**Wenxin (Hugo) Xue**  \n",
        "\n",
        "**SID: 487011**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IHLRWKOraEN"
      },
      "source": [
        "## Log in Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0qi09L2Qj-P"
      },
      "source": [
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    COLAB = True\n",
        "    print(\"Note: using Google CoLab\")\n",
        "    %tensorflow_version 2.x\n",
        "except:\n",
        "    print(\"Note: not using Google CoLab\")\n",
        "    COLAB = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftQrLF0iroyW"
      },
      "source": [
        "## Deploy GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCaVZj4OMpfy",
        "outputId": "85f91f7a-3dc0-414d-d5e9-649dabd418ec"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Nov 29 17:28:49 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6-EWjzaM-Qf",
        "outputId": "b45c280d-b3c4-4543-afc8-9746866a50ae"
      },
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 27.3 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DaWKT54r2K5"
      },
      "source": [
        "## import required packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgOLyd_bQikh"
      },
      "source": [
        "import sys\n",
        "\n",
        "import tensorflow.keras\n",
        "import pandas as pd\n",
        "import sklearn as sk\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import keras_preprocessing\n",
        "from keras_preprocessing import image\n",
        "from keras_preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from tensorflow.keras.applications.resnet_v2 import ResNet152V2\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.applications.vgg19 import VGG19\n",
        "from tensorflow.keras.applications.xception import Xception\n",
        "from tensorflow.keras.applications import NASNetMobile\n",
        "from tensorflow.keras.applications import ResNet50V2, ResNet101V2\n",
        "from tensorflow.keras.applications import InceptionResNetV2\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.applications import DenseNet121\n",
        "\n",
        "from tensorflow.keras.layers import Input, Dropout, Reshape\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.layers import LeakyReLU, PReLU\n",
        "from tensorflow.keras.layers import UpSampling2D, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.layers import Flatten, BatchNormalization\n",
        "from tensorflow.keras.layers import Activation, ZeroPadding2D\n",
        "from tensorflow.keras.models import Sequential, Model, load_model, save_model\n",
        "from tensorflow.keras.initializers import he_normal\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.metrics import RootMeanSquaredError\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8uzvotasOh1"
      },
      "source": [
        "## Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gxnl_30fHoJr"
      },
      "source": [
        "# file='/content/drive/My Drive/Colab Notebooks/assignment_yourname_class1.ipynb'\n",
        "PATH = \"/content/drive/My Drive/Colab Notebooks/applications-of-deep-learning-wustlfall-2021/city/\"\n",
        "PATH_TRAIN = os.path.join(PATH, \"train.csv\")\n",
        "PATH_TEST = os.path.join(PATH, \"test.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbbMgpWDRe07"
      },
      "source": [
        "df_train = pd.read_csv(PATH_TRAIN)\n",
        "df_test = pd.read_csv(PATH_TEST)\n",
        "\n",
        "df_train['filename'] = df_train.id.astype(str) + \".jpg\"\n",
        "df_test['filename'] = df_test.id.astype(str) + \".jpg\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXHt3-ynsT7R"
      },
      "source": [
        "### 1. use train_test_split to sampling dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1m2hqUHolcf6",
        "outputId": "5e629f3d-c0d9-420f-b5df-56417c3f210b"
      },
      "source": [
        "WIDTH = 150\n",
        "HEIGHT = 150\n",
        "\n",
        "# Split into train/test\n",
        "x_train, x_test, y_train, y_test = train_test_split(    \n",
        "    df_train['filename'].values, df_train['sqft'].values, test_size=0.1) \n",
        "\n",
        "df_train_cut = pd.DataFrame({\"filename\":x_train,\"sqft\":y_train})\n",
        "df_validate_cut = pd.DataFrame({\"filename\":x_test,\"sqft\":y_test})\n",
        "print(f\"Training shape: {df_train_cut.shape}\")\n",
        "print(f\"Validate shape: {df_validate_cut.shape}\")\n",
        "\n",
        "training_datagen = ImageDataGenerator(\n",
        "  rescale = 1./255,\n",
        "  horizontal_flip=True,\n",
        "  # vertical_flip=True,\n",
        "  fill_mode='nearest')\n",
        "\n",
        "train_generator = training_datagen.flow_from_dataframe(\n",
        "        dataframe=df_train_cut,\n",
        "        directory=PATH,\n",
        "        x_col=\"filename\",\n",
        "        y_col=\"sqft\",\n",
        "        target_size=(HEIGHT, WIDTH),\n",
        "        batch_size=32, # Keeping the training batch size small USUALLY increases performance\n",
        "        class_mode='raw')\n",
        "\n",
        "validation_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "val_generator = validation_datagen.flow_from_dataframe(\n",
        "        dataframe=df_validate_cut,\n",
        "        directory=PATH,\n",
        "        x_col=\"filename\",\n",
        "        y_col=\"sqft\",\n",
        "        target_size=(HEIGHT, WIDTH),\n",
        "        batch_size=256, # Make the validation batch size as large as you have memory for\n",
        "        class_mode='raw')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training shape: (21598, 2)\n",
            "Validate shape: (2400, 2)\n",
            "Found 21598 validated image filenames.\n",
            "Found 2400 validated image filenames.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5xBxXLwsfY2"
      },
      "source": [
        "### 2. simply split train and test dataset by 9:1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5nVQz98RVEMw",
        "outputId": "5486457b-75e2-4ab4-eca4-50a3891f62ff"
      },
      "source": [
        "TRAIN_PCT = 0.9\n",
        "TRAIN_CUT = int(len(df_train) * TRAIN_PCT)\n",
        "\n",
        "df_train_cut = df_train[0:TRAIN_CUT]\n",
        "df_validate_cut = df_train[TRAIN_CUT:]\n",
        "\n",
        "print(f\"Training size: {len(df_train_cut)}\")\n",
        "print(f\"Validate size: {len(df_validate_cut)}\")\n",
        "\n",
        "WIDTH = 150\n",
        "HEIGHT = 150\n",
        "\n",
        "training_datagen = ImageDataGenerator(\n",
        "  rescale = 1./255,\n",
        "  horizontal_flip=True,\n",
        "  # vertical_flip=True,\n",
        "  fill_mode='nearest')\n",
        "\n",
        "train_generator = training_datagen.flow_from_dataframe(\n",
        "        dataframe=df_train_cut,\n",
        "        directory=PATH,\n",
        "        x_col=\"filename\",\n",
        "        y_col=\"sqft\",\n",
        "        target_size=(HEIGHT, WIDTH),\n",
        "        batch_size=32, # Keeping the training batch size small USUALLY increases performance\n",
        "        class_mode='raw')\n",
        "\n",
        "validation_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "val_generator = validation_datagen.flow_from_dataframe(\n",
        "        dataframe=df_validate_cut,\n",
        "        directory=PATH,\n",
        "        x_col=\"filename\",\n",
        "        y_col=\"sqft\",\n",
        "        target_size=(HEIGHT, WIDTH),\n",
        "        batch_size=256, # Make the validation batch size as large as you have memory for\n",
        "        class_mode='raw')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training size: 21598\n",
            "Validate size: 2400\n",
            "Found 21598 validated image filenames.\n",
            "Found 2400 validated image filenames.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2ygA6wGsy22"
      },
      "source": [
        "## Try to build different types of model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6gRYmzvWcnM",
        "outputId": "e6403c5d-4d72-4da3-b27b-8f1a69dcacb8"
      },
      "source": [
        "# sequential model\n",
        "# model = Sequential(\n",
        "#     [\n",
        "#      Conv2D(32, (3,3), activation='relu', input_shape=(HEIGHT, WIDTH, 3),padding=\"same\"),\n",
        "#      MaxPooling2D(pool_size=(2,2)),\n",
        "#      Conv2D(64, (3,3), activation='relu', padding=\"same\"),\n",
        "#      Conv2D(64, (3,3), activation='relu'),\n",
        "#      MaxPooling2D(pool_size=(2,2)),\n",
        "#      Conv2D(128, (3,3), activation='relu',padding=\"same\"),\n",
        "#      Conv2D(128, (3,3), activation='relu'),\n",
        "#      MaxPooling2D(pool_size=(4,4)),\n",
        "#      Flatten(),\n",
        "#      Dense(1024, activation='relu'),\n",
        "#      Dense(512, activation='relu'),\n",
        "#      Dense(256, activation='relu'),\n",
        "#      Dense(128, activation='relu'),\n",
        "#      Dense(64, activation='relu'),\n",
        "#      Dense(1, activation='linear')\n",
        "#     ]\n",
        "# )\n",
        "# model.summary()\n",
        "################################################################################\n",
        "# Xception-1 rmse=>500\n",
        "input_tensor = Input(shape=(HEIGHT, WIDTH, 3))\n",
        "base_model = Xception(\n",
        "    include_top=False, weights='imagenet', input_tensor=input_tensor,\n",
        "    classifier_activation='relu'\n",
        ")\n",
        "\n",
        "base_model.trainable=True\n",
        "x=base_model.layers[-1].output\n",
        "x=GlobalAveragePooling2D()(x)\n",
        "\n",
        "x = Dense(1536,activation='relu')(x)\n",
        "x = Dense(1536,activation='relu')(x)\n",
        "x = Dense(728,activation='relu')(x)\n",
        "x = Dense(728,activation='relu')(x)\n",
        "\n",
        "output=Dense(1,activation='linear')(x)\n",
        "model=Model(inputs=input_tensor,outputs=output)\n",
        "model.summary() \n",
        "###############################################################################\n",
        "# Xception-2\n",
        "# input_tensor = Input(shape=(HEIGHT, WIDTH, 3))\n",
        "# base_model = Xception(\n",
        "#     include_top=False, weights='imagenet', input_tensor=input_tensor,\n",
        "#     classifier_activation='relu'\n",
        "# )\n",
        "\n",
        "# base_model.trainable=True\n",
        "# x=base_model.output\n",
        "# x=GlobalAveragePooling2D()(x)\n",
        "\n",
        "# x = Dense(1024,activation='relu')(x)\n",
        "# x = Dense(1024,activation='relu')(x)\n",
        "# x = Dense(512,activation='relu')(x)\n",
        "# x = Dense(64,activation='relu')(x)\n",
        "\n",
        "# output=Dense(1,activation='linear')(x)\n",
        "# model=Model(inputs=input_tensor,outputs=output)\n",
        "# model.summary()\n",
        "#############################################################################\n",
        "# VGG16-2\n",
        "# input_tensor = Input(shape=(HEIGHT, WIDTH, 3))\n",
        "# base_model = VGG16(\n",
        "#     include_top=False, weights='imagenet', input_tensor=input_tensor,\n",
        "#     classifier_activation='relu'\n",
        "# )\n",
        "\n",
        "# base_model.trainable=True\n",
        "# x=base_model.output\n",
        "# x=GlobalAveragePooling2D()(x)\n",
        "\n",
        "# x = Dense(512,activation='relu')(x)\n",
        "# x = Dense(512,activation='relu')(x)\n",
        "# x = Dense(256,activation='relu')(x)\n",
        "# x = Dense(256,activation='relu')(x)\n",
        "# x = Dense(64,activation='relu')(x)\n",
        "\n",
        "# output=Dense(1,activation='linear')(x)\n",
        "# model=Model(inputs=input_tensor,outputs=output)\n",
        "# model.summary()\n",
        "##########################################################\n",
        "# input_tensor = Input(shape=(HEIGHT, WIDTH, 3))\n",
        "# base_model = DenseNet121(\n",
        "#     include_top=False, weights='imagenet', input_tensor=input_tensor\n",
        "# )\n",
        "\n",
        "# base_model.trainable=True\n",
        "# x=base_model.output\n",
        "# x=GlobalAveragePooling2D()(x)\n",
        "\n",
        "# x = Dense(512,activation='relu')(x)\n",
        "# x = Dense(512,activation='relu')(x)\n",
        "# x = Dense(256,activation='relu')(x)\n",
        "# x = Dense(256,activation='relu')(x)\n",
        "# x = Dense(64,activation='relu')(x)\n",
        "# x = Dense(64,activation='relu')(x)\n",
        "\n",
        "# output=Dense(1,activation='linear')(x)\n",
        "# model=Model(inputs=input_tensor,outputs=output)\n",
        "# model.summary()\n",
        "####################################################\n",
        "# input_tensor = Input(shape=(HEIGHT, WIDTH, 3))\n",
        "# base_model = ResNet152V2(\n",
        "#     include_top=False, weights='imagenet', input_tensor=input_tensor,\n",
        "#     classifier_activation='relu'\n",
        "# )\n",
        "\n",
        "# x=base_model.output\n",
        "# x=GlobalAveragePooling2D()(x)\n",
        "\n",
        "# x = Dense(1024,activation='relu')(x)\n",
        "# x = Dense(1024,activation='relu')(x)\n",
        "# # x = Dense(64,activation='relu')(x)\n",
        "\n",
        "# output=Dense(1,activation='linear')(x)\n",
        "# model=Model(inputs=input_tensor,outputs=output)\n",
        "# model.summary()\n",
        "###########################################################\n",
        "# input_tensor = Input(shape=(HEIGHT, WIDTH, 3))\n",
        "# base_model = Xception(\n",
        "#     include_top=False, weights=None, input_tensor=input_tensor,\n",
        "#     classifier_activation='relu'\n",
        "# )\n",
        "\n",
        "# x=base_model.output\n",
        "# x=GlobalAveragePooling2D()(x)\n",
        "\n",
        "# x = Dense(1024,activation='relu')(x)\n",
        "# x = Dense(1024,activation='relu')(x)\n",
        "\n",
        "# output=Dense(1)(x)\n",
        "# model=Model(inputs=input_tensor,outputs=output)\n",
        "# model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "83689472/83683744 [==============================] - 1s 0us/step\n",
            "83697664/83683744 [==============================] - 1s 0us/step\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 150, 150, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " block1_conv1 (Conv2D)          (None, 74, 74, 32)   864         ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " block1_conv1_bn (BatchNormaliz  (None, 74, 74, 32)  128         ['block1_conv1[0][0]']           \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " block1_conv1_act (Activation)  (None, 74, 74, 32)   0           ['block1_conv1_bn[0][0]']        \n",
            "                                                                                                  \n",
            " block1_conv2 (Conv2D)          (None, 72, 72, 64)   18432       ['block1_conv1_act[0][0]']       \n",
            "                                                                                                  \n",
            " block1_conv2_bn (BatchNormaliz  (None, 72, 72, 64)  256         ['block1_conv2[0][0]']           \n",
            " ation)                                                                                           \n",
            "                                                                                                  \n",
            " block1_conv2_act (Activation)  (None, 72, 72, 64)   0           ['block1_conv2_bn[0][0]']        \n",
            "                                                                                                  \n",
            " block2_sepconv1 (SeparableConv  (None, 72, 72, 128)  8768       ['block1_conv2_act[0][0]']       \n",
            " 2D)                                                                                              \n",
            "                                                                                                  \n",
            " block2_sepconv1_bn (BatchNorma  (None, 72, 72, 128)  512        ['block2_sepconv1[0][0]']        \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " block2_sepconv2_act (Activatio  (None, 72, 72, 128)  0          ['block2_sepconv1_bn[0][0]']     \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " block2_sepconv2 (SeparableConv  (None, 72, 72, 128)  17536      ['block2_sepconv2_act[0][0]']    \n",
            " 2D)                                                                                              \n",
            "                                                                                                  \n",
            " block2_sepconv2_bn (BatchNorma  (None, 72, 72, 128)  512        ['block2_sepconv2[0][0]']        \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 36, 36, 128)  8192        ['block1_conv2_act[0][0]']       \n",
            "                                                                                                  \n",
            " block2_pool (MaxPooling2D)     (None, 36, 36, 128)  0           ['block2_sepconv2_bn[0][0]']     \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 36, 36, 128)  512        ['conv2d[0][0]']                 \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " add (Add)                      (None, 36, 36, 128)  0           ['block2_pool[0][0]',            \n",
            "                                                                  'batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " block3_sepconv1_act (Activatio  (None, 36, 36, 128)  0          ['add[0][0]']                    \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " block3_sepconv1 (SeparableConv  (None, 36, 36, 256)  33920      ['block3_sepconv1_act[0][0]']    \n",
            " 2D)                                                                                              \n",
            "                                                                                                  \n",
            " block3_sepconv1_bn (BatchNorma  (None, 36, 36, 256)  1024       ['block3_sepconv1[0][0]']        \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " block3_sepconv2_act (Activatio  (None, 36, 36, 256)  0          ['block3_sepconv1_bn[0][0]']     \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " block3_sepconv2 (SeparableConv  (None, 36, 36, 256)  67840      ['block3_sepconv2_act[0][0]']    \n",
            " 2D)                                                                                              \n",
            "                                                                                                  \n",
            " block3_sepconv2_bn (BatchNorma  (None, 36, 36, 256)  1024       ['block3_sepconv2[0][0]']        \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 18, 18, 256)  32768       ['add[0][0]']                    \n",
            "                                                                                                  \n",
            " block3_pool (MaxPooling2D)     (None, 18, 18, 256)  0           ['block3_sepconv2_bn[0][0]']     \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 18, 18, 256)  1024       ['conv2d_1[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " add_1 (Add)                    (None, 18, 18, 256)  0           ['block3_pool[0][0]',            \n",
            "                                                                  'batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " block4_sepconv1_act (Activatio  (None, 18, 18, 256)  0          ['add_1[0][0]']                  \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " block4_sepconv1 (SeparableConv  (None, 18, 18, 728)  188672     ['block4_sepconv1_act[0][0]']    \n",
            " 2D)                                                                                              \n",
            "                                                                                                  \n",
            " block4_sepconv1_bn (BatchNorma  (None, 18, 18, 728)  2912       ['block4_sepconv1[0][0]']        \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " block4_sepconv2_act (Activatio  (None, 18, 18, 728)  0          ['block4_sepconv1_bn[0][0]']     \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " block4_sepconv2 (SeparableConv  (None, 18, 18, 728)  536536     ['block4_sepconv2_act[0][0]']    \n",
            " 2D)                                                                                              \n",
            "                                                                                                  \n",
            " block4_sepconv2_bn (BatchNorma  (None, 18, 18, 728)  2912       ['block4_sepconv2[0][0]']        \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)              (None, 9, 9, 728)    186368      ['add_1[0][0]']                  \n",
            "                                                                                                  \n",
            " block4_pool (MaxPooling2D)     (None, 9, 9, 728)    0           ['block4_sepconv2_bn[0][0]']     \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 9, 9, 728)   2912        ['conv2d_2[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " add_2 (Add)                    (None, 9, 9, 728)    0           ['block4_pool[0][0]',            \n",
            "                                                                  'batch_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " block5_sepconv1_act (Activatio  (None, 9, 9, 728)   0           ['add_2[0][0]']                  \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " block5_sepconv1 (SeparableConv  (None, 9, 9, 728)   536536      ['block5_sepconv1_act[0][0]']    \n",
            " 2D)                                                                                              \n",
            "                                                                                                  \n",
            " block5_sepconv1_bn (BatchNorma  (None, 9, 9, 728)   2912        ['block5_sepconv1[0][0]']        \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " block5_sepconv2_act (Activatio  (None, 9, 9, 728)   0           ['block5_sepconv1_bn[0][0]']     \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " block5_sepconv2 (SeparableConv  (None, 9, 9, 728)   536536      ['block5_sepconv2_act[0][0]']    \n",
            " 2D)                                                                                              \n",
            "                                                                                                  \n",
            " block5_sepconv2_bn (BatchNorma  (None, 9, 9, 728)   2912        ['block5_sepconv2[0][0]']        \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " block5_sepconv3_act (Activatio  (None, 9, 9, 728)   0           ['block5_sepconv2_bn[0][0]']     \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " block5_sepconv3 (SeparableConv  (None, 9, 9, 728)   536536      ['block5_sepconv3_act[0][0]']    \n",
            " 2D)                                                                                              \n",
            "                                                                                                  \n",
            " block5_sepconv3_bn (BatchNorma  (None, 9, 9, 728)   2912        ['block5_sepconv3[0][0]']        \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " add_3 (Add)                    (None, 9, 9, 728)    0           ['block5_sepconv3_bn[0][0]',     \n",
            "                                                                  'add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " block6_sepconv1_act (Activatio  (None, 9, 9, 728)   0           ['add_3[0][0]']                  \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " block6_sepconv1 (SeparableConv  (None, 9, 9, 728)   536536      ['block6_sepconv1_act[0][0]']    \n",
            " 2D)                                                                                              \n",
            "                                                                                                  \n",
            " block6_sepconv1_bn (BatchNorma  (None, 9, 9, 728)   2912        ['block6_sepconv1[0][0]']        \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " block6_sepconv2_act (Activatio  (None, 9, 9, 728)   0           ['block6_sepconv1_bn[0][0]']     \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " block6_sepconv2 (SeparableConv  (None, 9, 9, 728)   536536      ['block6_sepconv2_act[0][0]']    \n",
            " 2D)                                                                                              \n",
            "                                                                                                  \n",
            " block6_sepconv2_bn (BatchNorma  (None, 9, 9, 728)   2912        ['block6_sepconv2[0][0]']        \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " block6_sepconv3_act (Activatio  (None, 9, 9, 728)   0           ['block6_sepconv2_bn[0][0]']     \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " block6_sepconv3 (SeparableConv  (None, 9, 9, 728)   536536      ['block6_sepconv3_act[0][0]']    \n",
            " 2D)                                                                                              \n",
            "                                                                                                  \n",
            " block6_sepconv3_bn (BatchNorma  (None, 9, 9, 728)   2912        ['block6_sepconv3[0][0]']        \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " add_4 (Add)                    (None, 9, 9, 728)    0           ['block6_sepconv3_bn[0][0]',     \n",
            "                                                                  'add_3[0][0]']                  \n",
            "                                                                                                  \n",
            " block7_sepconv1_act (Activatio  (None, 9, 9, 728)   0           ['add_4[0][0]']                  \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " block7_sepconv1 (SeparableConv  (None, 9, 9, 728)   536536      ['block7_sepconv1_act[0][0]']    \n",
            " 2D)                                                                                              \n",
            "                                                                                                  \n",
            " block7_sepconv1_bn (BatchNorma  (None, 9, 9, 728)   2912        ['block7_sepconv1[0][0]']        \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " block7_sepconv2_act (Activatio  (None, 9, 9, 728)   0           ['block7_sepconv1_bn[0][0]']     \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " block7_sepconv2 (SeparableConv  (None, 9, 9, 728)   536536      ['block7_sepconv2_act[0][0]']    \n",
            " 2D)                                                                                              \n",
            "                                                                                                  \n",
            " block7_sepconv2_bn (BatchNorma  (None, 9, 9, 728)   2912        ['block7_sepconv2[0][0]']        \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " block7_sepconv3_act (Activatio  (None, 9, 9, 728)   0           ['block7_sepconv2_bn[0][0]']     \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " block7_sepconv3 (SeparableConv  (None, 9, 9, 728)   536536      ['block7_sepconv3_act[0][0]']    \n",
            " 2D)                                                                                              \n",
            "                                                                                                  \n",
            " block7_sepconv3_bn (BatchNorma  (None, 9, 9, 728)   2912        ['block7_sepconv3[0][0]']        \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " add_5 (Add)                    (None, 9, 9, 728)    0           ['block7_sepconv3_bn[0][0]',     \n",
            "                                                                  'add_4[0][0]']                  \n",
            "                                                                                                  \n",
            " block8_sepconv1_act (Activatio  (None, 9, 9, 728)   0           ['add_5[0][0]']                  \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " block8_sepconv1 (SeparableConv  (None, 9, 9, 728)   536536      ['block8_sepconv1_act[0][0]']    \n",
            " 2D)                                                                                              \n",
            "                                                                                                  \n",
            " block8_sepconv1_bn (BatchNorma  (None, 9, 9, 728)   2912        ['block8_sepconv1[0][0]']        \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " block8_sepconv2_act (Activatio  (None, 9, 9, 728)   0           ['block8_sepconv1_bn[0][0]']     \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " block8_sepconv2 (SeparableConv  (None, 9, 9, 728)   536536      ['block8_sepconv2_act[0][0]']    \n",
            " 2D)                                                                                              \n",
            "                                                                                                  \n",
            " block8_sepconv2_bn (BatchNorma  (None, 9, 9, 728)   2912        ['block8_sepconv2[0][0]']        \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " block8_sepconv3_act (Activatio  (None, 9, 9, 728)   0           ['block8_sepconv2_bn[0][0]']     \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " block8_sepconv3 (SeparableConv  (None, 9, 9, 728)   536536      ['block8_sepconv3_act[0][0]']    \n",
            " 2D)                                                                                              \n",
            "                                                                                                  \n",
            " block8_sepconv3_bn (BatchNorma  (None, 9, 9, 728)   2912        ['block8_sepconv3[0][0]']        \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " add_6 (Add)                    (None, 9, 9, 728)    0           ['block8_sepconv3_bn[0][0]',     \n",
            "                                                                  'add_5[0][0]']                  \n",
            "                                                                                                  \n",
            " block9_sepconv1_act (Activatio  (None, 9, 9, 728)   0           ['add_6[0][0]']                  \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " block9_sepconv1 (SeparableConv  (None, 9, 9, 728)   536536      ['block9_sepconv1_act[0][0]']    \n",
            " 2D)                                                                                              \n",
            "                                                                                                  \n",
            " block9_sepconv1_bn (BatchNorma  (None, 9, 9, 728)   2912        ['block9_sepconv1[0][0]']        \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " block9_sepconv2_act (Activatio  (None, 9, 9, 728)   0           ['block9_sepconv1_bn[0][0]']     \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " block9_sepconv2 (SeparableConv  (None, 9, 9, 728)   536536      ['block9_sepconv2_act[0][0]']    \n",
            " 2D)                                                                                              \n",
            "                                                                                                  \n",
            " block9_sepconv2_bn (BatchNorma  (None, 9, 9, 728)   2912        ['block9_sepconv2[0][0]']        \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " block9_sepconv3_act (Activatio  (None, 9, 9, 728)   0           ['block9_sepconv2_bn[0][0]']     \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " block9_sepconv3 (SeparableConv  (None, 9, 9, 728)   536536      ['block9_sepconv3_act[0][0]']    \n",
            " 2D)                                                                                              \n",
            "                                                                                                  \n",
            " block9_sepconv3_bn (BatchNorma  (None, 9, 9, 728)   2912        ['block9_sepconv3[0][0]']        \n",
            " lization)                                                                                        \n",
            "                                                                                                  \n",
            " add_7 (Add)                    (None, 9, 9, 728)    0           ['block9_sepconv3_bn[0][0]',     \n",
            "                                                                  'add_6[0][0]']                  \n",
            "                                                                                                  \n",
            " block10_sepconv1_act (Activati  (None, 9, 9, 728)   0           ['add_7[0][0]']                  \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " block10_sepconv1 (SeparableCon  (None, 9, 9, 728)   536536      ['block10_sepconv1_act[0][0]']   \n",
            " v2D)                                                                                             \n",
            "                                                                                                  \n",
            " block10_sepconv1_bn (BatchNorm  (None, 9, 9, 728)   2912        ['block10_sepconv1[0][0]']       \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " block10_sepconv2_act (Activati  (None, 9, 9, 728)   0           ['block10_sepconv1_bn[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " block10_sepconv2 (SeparableCon  (None, 9, 9, 728)   536536      ['block10_sepconv2_act[0][0]']   \n",
            " v2D)                                                                                             \n",
            "                                                                                                  \n",
            " block10_sepconv2_bn (BatchNorm  (None, 9, 9, 728)   2912        ['block10_sepconv2[0][0]']       \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " block10_sepconv3_act (Activati  (None, 9, 9, 728)   0           ['block10_sepconv2_bn[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " block10_sepconv3 (SeparableCon  (None, 9, 9, 728)   536536      ['block10_sepconv3_act[0][0]']   \n",
            " v2D)                                                                                             \n",
            "                                                                                                  \n",
            " block10_sepconv3_bn (BatchNorm  (None, 9, 9, 728)   2912        ['block10_sepconv3[0][0]']       \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " add_8 (Add)                    (None, 9, 9, 728)    0           ['block10_sepconv3_bn[0][0]',    \n",
            "                                                                  'add_7[0][0]']                  \n",
            "                                                                                                  \n",
            " block11_sepconv1_act (Activati  (None, 9, 9, 728)   0           ['add_8[0][0]']                  \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " block11_sepconv1 (SeparableCon  (None, 9, 9, 728)   536536      ['block11_sepconv1_act[0][0]']   \n",
            " v2D)                                                                                             \n",
            "                                                                                                  \n",
            " block11_sepconv1_bn (BatchNorm  (None, 9, 9, 728)   2912        ['block11_sepconv1[0][0]']       \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " block11_sepconv2_act (Activati  (None, 9, 9, 728)   0           ['block11_sepconv1_bn[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " block11_sepconv2 (SeparableCon  (None, 9, 9, 728)   536536      ['block11_sepconv2_act[0][0]']   \n",
            " v2D)                                                                                             \n",
            "                                                                                                  \n",
            " block11_sepconv2_bn (BatchNorm  (None, 9, 9, 728)   2912        ['block11_sepconv2[0][0]']       \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " block11_sepconv3_act (Activati  (None, 9, 9, 728)   0           ['block11_sepconv2_bn[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " block11_sepconv3 (SeparableCon  (None, 9, 9, 728)   536536      ['block11_sepconv3_act[0][0]']   \n",
            " v2D)                                                                                             \n",
            "                                                                                                  \n",
            " block11_sepconv3_bn (BatchNorm  (None, 9, 9, 728)   2912        ['block11_sepconv3[0][0]']       \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " add_9 (Add)                    (None, 9, 9, 728)    0           ['block11_sepconv3_bn[0][0]',    \n",
            "                                                                  'add_8[0][0]']                  \n",
            "                                                                                                  \n",
            " block12_sepconv1_act (Activati  (None, 9, 9, 728)   0           ['add_9[0][0]']                  \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " block12_sepconv1 (SeparableCon  (None, 9, 9, 728)   536536      ['block12_sepconv1_act[0][0]']   \n",
            " v2D)                                                                                             \n",
            "                                                                                                  \n",
            " block12_sepconv1_bn (BatchNorm  (None, 9, 9, 728)   2912        ['block12_sepconv1[0][0]']       \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " block12_sepconv2_act (Activati  (None, 9, 9, 728)   0           ['block12_sepconv1_bn[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " block12_sepconv2 (SeparableCon  (None, 9, 9, 728)   536536      ['block12_sepconv2_act[0][0]']   \n",
            " v2D)                                                                                             \n",
            "                                                                                                  \n",
            " block12_sepconv2_bn (BatchNorm  (None, 9, 9, 728)   2912        ['block12_sepconv2[0][0]']       \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " block12_sepconv3_act (Activati  (None, 9, 9, 728)   0           ['block12_sepconv2_bn[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " block12_sepconv3 (SeparableCon  (None, 9, 9, 728)   536536      ['block12_sepconv3_act[0][0]']   \n",
            " v2D)                                                                                             \n",
            "                                                                                                  \n",
            " block12_sepconv3_bn (BatchNorm  (None, 9, 9, 728)   2912        ['block12_sepconv3[0][0]']       \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " add_10 (Add)                   (None, 9, 9, 728)    0           ['block12_sepconv3_bn[0][0]',    \n",
            "                                                                  'add_9[0][0]']                  \n",
            "                                                                                                  \n",
            " block13_sepconv1_act (Activati  (None, 9, 9, 728)   0           ['add_10[0][0]']                 \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " block13_sepconv1 (SeparableCon  (None, 9, 9, 728)   536536      ['block13_sepconv1_act[0][0]']   \n",
            " v2D)                                                                                             \n",
            "                                                                                                  \n",
            " block13_sepconv1_bn (BatchNorm  (None, 9, 9, 728)   2912        ['block13_sepconv1[0][0]']       \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " block13_sepconv2_act (Activati  (None, 9, 9, 728)   0           ['block13_sepconv1_bn[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " block13_sepconv2 (SeparableCon  (None, 9, 9, 1024)  752024      ['block13_sepconv2_act[0][0]']   \n",
            " v2D)                                                                                             \n",
            "                                                                                                  \n",
            " block13_sepconv2_bn (BatchNorm  (None, 9, 9, 1024)  4096        ['block13_sepconv2[0][0]']       \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)              (None, 5, 5, 1024)   745472      ['add_10[0][0]']                 \n",
            "                                                                                                  \n",
            " block13_pool (MaxPooling2D)    (None, 5, 5, 1024)   0           ['block13_sepconv2_bn[0][0]']    \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 5, 5, 1024)  4096        ['conv2d_3[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " add_11 (Add)                   (None, 5, 5, 1024)   0           ['block13_pool[0][0]',           \n",
            "                                                                  'batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " block14_sepconv1 (SeparableCon  (None, 5, 5, 1536)  1582080     ['add_11[0][0]']                 \n",
            " v2D)                                                                                             \n",
            "                                                                                                  \n",
            " block14_sepconv1_bn (BatchNorm  (None, 5, 5, 1536)  6144        ['block14_sepconv1[0][0]']       \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " block14_sepconv1_act (Activati  (None, 5, 5, 1536)  0           ['block14_sepconv1_bn[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " block14_sepconv2 (SeparableCon  (None, 5, 5, 2048)  3159552     ['block14_sepconv1_act[0][0]']   \n",
            " v2D)                                                                                             \n",
            "                                                                                                  \n",
            " block14_sepconv2_bn (BatchNorm  (None, 5, 5, 2048)  8192        ['block14_sepconv2[0][0]']       \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " block14_sepconv2_act (Activati  (None, 5, 5, 2048)  0           ['block14_sepconv2_bn[0][0]']    \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " global_average_pooling2d (Glob  (None, 2048)        0           ['block14_sepconv2_act[0][0]']   \n",
            " alAveragePooling2D)                                                                              \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 1536)         3147264     ['global_average_pooling2d[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 1536)         2360832     ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 728)          1118936     ['dense_1[0][0]']                \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 728)          530712      ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 1)            729         ['dense_3[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 28,019,953\n",
            "Trainable params: 27,965,425\n",
            "Non-trainable params: 54,528\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUEsqAAvtdj2"
      },
      "source": [
        "### Model Training\n",
        "- metrics: rmse \n",
        "- learning rate: 1e-4 or 1e-5\n",
        "- Adam\n",
        "- model_checkpoint: save best model\n",
        "- lr_reduce: 1e-7"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqn5C1LyJUHH",
        "outputId": "bcd7a67b-281d-49d8-9289-08da7a454036"
      },
      "source": [
        "STEP_SIZE_VALID=val_generator.n//val_generator.batch_size\n",
        "\n",
        "model.compile(loss = 'mean_squared_error', optimizer=Adam(learning_rate=1e-5), metrics=[RootMeanSquaredError(name=\"rmse\")])\n",
        "model_checkpoint=ModelCheckpoint(os.path.join(PATH, 'xception_bestmodel.h5'),monitor=\"val_loss\",save_best_only=True,mode=\"min\",verbose=1)\n",
        "lr_reduce = ReduceLROnPlateau(monitor='val_loss',factor=0.6,patience=50,verbose=1,mode='auto',min_lr=1e-7)\n",
        "callback = [model_checkpoint,lr_reduce]\n",
        "model.fit(train_generator, epochs=500, steps_per_epoch=250, validation_data = val_generator, \n",
        "          callbacks=callback, verbose = 1, validation_steps=STEP_SIZE_VALID)\n",
        "\n",
        "model.load_weights(os.path.join(PATH, 'xception_bestmodel.h5'))\n",
        "\n",
        "submit_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "submit_generator = submit_datagen.flow_from_dataframe(\n",
        "        dataframe=df_test,\n",
        "        directory=PATH,\n",
        "        x_col=\"filename\",\n",
        "        batch_size = 1,\n",
        "        shuffle = False,\n",
        "        target_size=(HEIGHT, WIDTH),\n",
        "        class_mode=None)\n",
        "\n",
        "submit_generator.reset()\n",
        "best_pred = model.predict(submit_generator,steps=len(df_test))\n",
        "best_df_submit = pd.DataFrame({\"id\":df_test['id'],'sqft':best_pred[:,0].flatten()})\n",
        "best_df_submit.to_csv(\"/content/drive/My Drive/Colab Notebooks/applications-of-deep-learning-wustlfall-2021/best_submit.csv\",index = False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 31025524.0000 - rmse: 5570.0557\n",
            "Epoch 00001: val_loss improved from inf to 5490572.50000, saving model to /content/drive/My Drive/Colab Notebooks/applications-of-deep-learning-wustlfall-2021/city/xception_bestmodel.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  layer_config = serialize_layer_fn(layer)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r250/250 [==============================] - 69s 253ms/step - loss: 31025524.0000 - rmse: 5570.0557 - val_loss: 5490572.5000 - val_rmse: 2343.1970 - lr: 1.0000e-05\n",
            "Epoch 2/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 2532085.2500 - rmse: 1591.2527\n",
            "Epoch 00002: val_loss improved from 5490572.50000 to 1150573.12500, saving model to /content/drive/My Drive/Colab Notebooks/applications-of-deep-learning-wustlfall-2021/city/xception_bestmodel.h5\n",
            "250/250 [==============================] - 62s 248ms/step - loss: 2532085.2500 - rmse: 1591.2527 - val_loss: 1150573.1250 - val_rmse: 1072.6477 - lr: 1.0000e-05\n",
            "Epoch 3/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 1055374.6250 - rmse: 1027.3142\n",
            "Epoch 00003: val_loss improved from 1150573.12500 to 1027355.87500, saving model to /content/drive/My Drive/Colab Notebooks/applications-of-deep-learning-wustlfall-2021/city/xception_bestmodel.h5\n",
            "250/250 [==============================] - 62s 247ms/step - loss: 1055374.6250 - rmse: 1027.3142 - val_loss: 1027355.8750 - val_rmse: 1013.5856 - lr: 1.0000e-05\n",
            "Epoch 4/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 896910.6250 - rmse: 947.0536\n",
            "Epoch 00004: val_loss improved from 1027355.87500 to 951962.25000, saving model to /content/drive/My Drive/Colab Notebooks/applications-of-deep-learning-wustlfall-2021/city/xception_bestmodel.h5\n",
            "250/250 [==============================] - 62s 248ms/step - loss: 896910.6250 - rmse: 947.0536 - val_loss: 951962.2500 - val_rmse: 975.6855 - lr: 1.0000e-05\n",
            "Epoch 5/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 854888.0000 - rmse: 924.6015\n",
            "Epoch 00005: val_loss improved from 951962.25000 to 918114.68750, saving model to /content/drive/My Drive/Colab Notebooks/applications-of-deep-learning-wustlfall-2021/city/xception_bestmodel.h5\n",
            "250/250 [==============================] - 62s 247ms/step - loss: 854888.0000 - rmse: 924.6015 - val_loss: 918114.6875 - val_rmse: 958.1830 - lr: 1.0000e-05\n",
            "Epoch 6/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 766454.2500 - rmse: 875.4737\n",
            "Epoch 00006: val_loss improved from 918114.68750 to 893115.75000, saving model to /content/drive/My Drive/Colab Notebooks/applications-of-deep-learning-wustlfall-2021/city/xception_bestmodel.h5\n",
            "250/250 [==============================] - 62s 249ms/step - loss: 766454.2500 - rmse: 875.4737 - val_loss: 893115.7500 - val_rmse: 945.0480 - lr: 1.0000e-05\n",
            "Epoch 7/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 717820.3750 - rmse: 847.2428\n",
            "Epoch 00007: val_loss improved from 893115.75000 to 805284.81250, saving model to /content/drive/My Drive/Colab Notebooks/applications-of-deep-learning-wustlfall-2021/city/xception_bestmodel.h5\n",
            "250/250 [==============================] - 62s 248ms/step - loss: 717820.3750 - rmse: 847.2428 - val_loss: 805284.8125 - val_rmse: 897.3766 - lr: 1.0000e-05\n",
            "Epoch 8/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 694917.6250 - rmse: 833.6172\n",
            "Epoch 00008: val_loss improved from 805284.81250 to 795305.43750, saving model to /content/drive/My Drive/Colab Notebooks/applications-of-deep-learning-wustlfall-2021/city/xception_bestmodel.h5\n",
            "250/250 [==============================] - 63s 251ms/step - loss: 694917.6250 - rmse: 833.6172 - val_loss: 795305.4375 - val_rmse: 891.7990 - lr: 1.0000e-05\n",
            "Epoch 9/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 628751.1875 - rmse: 792.9384\n",
            "Epoch 00009: val_loss improved from 795305.43750 to 760814.25000, saving model to /content/drive/My Drive/Colab Notebooks/applications-of-deep-learning-wustlfall-2021/city/xception_bestmodel.h5\n",
            "250/250 [==============================] - 63s 248ms/step - loss: 628751.1875 - rmse: 792.9384 - val_loss: 760814.2500 - val_rmse: 872.2466 - lr: 1.0000e-05\n",
            "Epoch 10/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 598131.5625 - rmse: 773.3896\n",
            "Epoch 00010: val_loss did not improve from 760814.25000\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 598131.5625 - rmse: 773.3896 - val_loss: 838976.7500 - val_rmse: 915.9567 - lr: 1.0000e-05\n",
            "Epoch 11/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 568073.1250 - rmse: 753.7062\n",
            "Epoch 00011: val_loss improved from 760814.25000 to 742927.75000, saving model to /content/drive/My Drive/Colab Notebooks/applications-of-deep-learning-wustlfall-2021/city/xception_bestmodel.h5\n",
            "250/250 [==============================] - 62s 249ms/step - loss: 568073.1250 - rmse: 753.7062 - val_loss: 742927.7500 - val_rmse: 861.9326 - lr: 1.0000e-05\n",
            "Epoch 12/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 556700.8125 - rmse: 746.1238\n",
            "Epoch 00012: val_loss improved from 742927.75000 to 709942.00000, saving model to /content/drive/My Drive/Colab Notebooks/applications-of-deep-learning-wustlfall-2021/city/xception_bestmodel.h5\n",
            "250/250 [==============================] - 62s 249ms/step - loss: 556700.8125 - rmse: 746.1238 - val_loss: 709942.0000 - val_rmse: 842.5806 - lr: 1.0000e-05\n",
            "Epoch 13/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 512023.9062 - rmse: 715.5584\n",
            "Epoch 00013: val_loss did not improve from 709942.00000\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 512023.9062 - rmse: 715.5584 - val_loss: 805528.1875 - val_rmse: 897.5122 - lr: 1.0000e-05\n",
            "Epoch 14/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 493482.1562 - rmse: 702.4828\n",
            "Epoch 00014: val_loss improved from 709942.00000 to 698331.75000, saving model to /content/drive/My Drive/Colab Notebooks/applications-of-deep-learning-wustlfall-2021/city/xception_bestmodel.h5\n",
            "250/250 [==============================] - 72s 287ms/step - loss: 493482.1562 - rmse: 702.4828 - val_loss: 698331.7500 - val_rmse: 835.6624 - lr: 1.0000e-05\n",
            "Epoch 15/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 446510.1250 - rmse: 668.2141\n",
            "Epoch 00015: val_loss did not improve from 698331.75000\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 446510.1250 - rmse: 668.2141 - val_loss: 707314.7500 - val_rmse: 841.0201 - lr: 1.0000e-05\n",
            "Epoch 16/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 467299.4688 - rmse: 683.5930\n",
            "Epoch 00016: val_loss improved from 698331.75000 to 684664.43750, saving model to /content/drive/My Drive/Colab Notebooks/applications-of-deep-learning-wustlfall-2021/city/xception_bestmodel.h5\n",
            "250/250 [==============================] - 62s 246ms/step - loss: 467299.4688 - rmse: 683.5930 - val_loss: 684664.4375 - val_rmse: 827.4445 - lr: 1.0000e-05\n",
            "Epoch 17/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 423692.4062 - rmse: 650.9166\n",
            "Epoch 00017: val_loss improved from 684664.43750 to 662265.43750, saving model to /content/drive/My Drive/Colab Notebooks/applications-of-deep-learning-wustlfall-2021/city/xception_bestmodel.h5\n",
            "250/250 [==============================] - 62s 247ms/step - loss: 423692.4062 - rmse: 650.9166 - val_loss: 662265.4375 - val_rmse: 813.7969 - lr: 1.0000e-05\n",
            "Epoch 18/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 398538.0000 - rmse: 631.2986\n",
            "Epoch 00018: val_loss did not improve from 662265.43750\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 398538.0000 - rmse: 631.2986 - val_loss: 765953.2500 - val_rmse: 875.1875 - lr: 1.0000e-05\n",
            "Epoch 19/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 393720.0625 - rmse: 627.4711\n",
            "Epoch 00019: val_loss improved from 662265.43750 to 659462.62500, saving model to /content/drive/My Drive/Colab Notebooks/applications-of-deep-learning-wustlfall-2021/city/xception_bestmodel.h5\n",
            "250/250 [==============================] - 62s 247ms/step - loss: 393720.0625 - rmse: 627.4711 - val_loss: 659462.6250 - val_rmse: 812.0731 - lr: 1.0000e-05\n",
            "Epoch 20/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 391349.0938 - rmse: 625.5790\n",
            "Epoch 00020: val_loss did not improve from 659462.62500\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 391349.0938 - rmse: 625.5790 - val_loss: 796106.7500 - val_rmse: 892.2481 - lr: 1.0000e-05\n",
            "Epoch 21/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 372095.7188 - rmse: 609.9965\n",
            "Epoch 00021: val_loss improved from 659462.62500 to 650698.00000, saving model to /content/drive/My Drive/Colab Notebooks/applications-of-deep-learning-wustlfall-2021/city/xception_bestmodel.h5\n",
            "250/250 [==============================] - 62s 247ms/step - loss: 372095.7188 - rmse: 609.9965 - val_loss: 650698.0000 - val_rmse: 806.6585 - lr: 1.0000e-05\n",
            "Epoch 22/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 344294.0625 - rmse: 586.7657\n",
            "Epoch 00022: val_loss did not improve from 650698.00000\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 344294.0625 - rmse: 586.7657 - val_loss: 657947.8125 - val_rmse: 811.1398 - lr: 1.0000e-05\n",
            "Epoch 23/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 332217.6875 - rmse: 576.3832\n",
            "Epoch 00023: val_loss did not improve from 650698.00000\n",
            "250/250 [==============================] - 58s 230ms/step - loss: 332217.6875 - rmse: 576.3832 - val_loss: 734163.8125 - val_rmse: 856.8336 - lr: 1.0000e-05\n",
            "Epoch 24/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 321943.6875 - rmse: 567.4008\n",
            "Epoch 00024: val_loss did not improve from 650698.00000\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 321943.6875 - rmse: 567.4008 - val_loss: 663579.7500 - val_rmse: 814.6040 - lr: 1.0000e-05\n",
            "Epoch 25/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 309558.9688 - rmse: 556.3802\n",
            "Epoch 00025: val_loss did not improve from 650698.00000\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 309558.9688 - rmse: 556.3802 - val_loss: 655155.0000 - val_rmse: 809.4164 - lr: 1.0000e-05\n",
            "Epoch 26/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 287067.2188 - rmse: 535.7865\n",
            "Epoch 00026: val_loss did not improve from 650698.00000\n",
            "250/250 [==============================] - 58s 230ms/step - loss: 287067.2188 - rmse: 535.7865 - val_loss: 654977.2500 - val_rmse: 809.3066 - lr: 1.0000e-05\n",
            "Epoch 27/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 274961.9375 - rmse: 524.3681\n",
            "Epoch 00027: val_loss improved from 650698.00000 to 634265.75000, saving model to /content/drive/My Drive/Colab Notebooks/applications-of-deep-learning-wustlfall-2021/city/xception_bestmodel.h5\n",
            "250/250 [==============================] - 62s 248ms/step - loss: 274961.9375 - rmse: 524.3681 - val_loss: 634265.7500 - val_rmse: 796.4080 - lr: 1.0000e-05\n",
            "Epoch 28/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 261627.6250 - rmse: 511.4955\n",
            "Epoch 00028: val_loss did not improve from 634265.75000\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 261627.6250 - rmse: 511.4955 - val_loss: 638179.8125 - val_rmse: 798.8616 - lr: 1.0000e-05\n",
            "Epoch 29/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 254679.5000 - rmse: 504.6578Epoch 30/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 256860.9219 - rmse: 506.8145\n",
            "Epoch 00030: val_loss did not improve from 634265.75000\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 256860.9219 - rmse: 506.8145 - val_loss: 650298.2500 - val_rmse: 806.4107 - lr: 1.0000e-05\n",
            "Epoch 31/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 231863.5938 - rmse: 481.5222\n",
            "Epoch 00031: val_loss improved from 634265.75000 to 624533.43750, saving model to /content/drive/My Drive/Colab Notebooks/applications-of-deep-learning-wustlfall-2021/city/xception_bestmodel.h5\n",
            "250/250 [==============================] - 62s 246ms/step - loss: 231863.5938 - rmse: 481.5222 - val_loss: 624533.4375 - val_rmse: 790.2743 - lr: 1.0000e-05\n",
            "Epoch 32/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 227243.1094 - rmse: 476.7002\n",
            "Epoch 00032: val_loss did not improve from 624533.43750\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 227243.1094 - rmse: 476.7002 - val_loss: 633455.7500 - val_rmse: 795.8993 - lr: 1.0000e-05\n",
            "Epoch 33/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 221624.6406 - rmse: 470.7703\n",
            "Epoch 00033: val_loss improved from 624533.43750 to 616740.43750, saving model to /content/drive/My Drive/Colab Notebooks/applications-of-deep-learning-wustlfall-2021/city/xception_bestmodel.h5\n",
            "250/250 [==============================] - 62s 246ms/step - loss: 221624.6406 - rmse: 470.7703 - val_loss: 616740.4375 - val_rmse: 785.3282 - lr: 1.0000e-05\n",
            "Epoch 34/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 217948.1562 - rmse: 466.8492\n",
            "Epoch 00034: val_loss did not improve from 616740.43750\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 217948.1562 - rmse: 466.8492 - val_loss: 640538.7500 - val_rmse: 800.3366 - lr: 1.0000e-05\n",
            "Epoch 35/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 203022.8906 - rmse: 450.5806\n",
            "Epoch 00035: val_loss did not improve from 616740.43750\n",
            "250/250 [==============================] - 58s 230ms/step - loss: 203022.8906 - rmse: 450.5806 - val_loss: 688040.0625 - val_rmse: 829.4818 - lr: 1.0000e-05\n",
            "Epoch 36/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 201615.6406 - rmse: 449.0163\n",
            "Epoch 00036: val_loss did not improve from 616740.43750\n",
            "250/250 [==============================] - 58s 230ms/step - loss: 201615.6406 - rmse: 449.0163 - val_loss: 655012.2500 - val_rmse: 809.3282 - lr: 1.0000e-05\n",
            "Epoch 37/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 171739.6562 - rmse: 414.4148\n",
            "Epoch 00037: val_loss did not improve from 616740.43750\n",
            "250/250 [==============================] - 58s 230ms/step - loss: 171739.6562 - rmse: 414.4148 - val_loss: 617295.7500 - val_rmse: 785.6817 - lr: 1.0000e-05\n",
            "Epoch 38/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 193667.8594 - rmse: 440.0771\n",
            "Epoch 00038: val_loss did not improve from 616740.43750\n",
            "250/250 [==============================] - 68s 270ms/step - loss: 193667.8594 - rmse: 440.0771 - val_loss: 655328.3750 - val_rmse: 809.5236 - lr: 1.0000e-05\n",
            "Epoch 39/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 174699.3750 - rmse: 417.9705\n",
            "Epoch 00039: val_loss did not improve from 616740.43750\n",
            "250/250 [==============================] - 58s 230ms/step - loss: 174699.3750 - rmse: 417.9705 - val_loss: 631884.2500 - val_rmse: 794.9114 - lr: 1.0000e-05\n",
            "Epoch 40/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 161006.5000 - rmse: 401.2562\n",
            "Epoch 00040: val_loss did not improve from 616740.43750\n",
            "250/250 [==============================] - 58s 230ms/step - loss: 161006.5000 - rmse: 401.2562 - val_loss: 623990.1875 - val_rmse: 789.9305 - lr: 1.0000e-05\n",
            "Epoch 41/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 164168.9375 - rmse: 405.1776\n",
            "Epoch 00041: val_loss did not improve from 616740.43750\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 164168.9375 - rmse: 405.1776 - val_loss: 639399.5625 - val_rmse: 799.6246 - lr: 1.0000e-05\n",
            "Epoch 42/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 159476.7500 - rmse: 399.3454\n",
            "Epoch 00042: val_loss did not improve from 616740.43750\n",
            "250/250 [==============================] - 58s 230ms/step - loss: 159476.7500 - rmse: 399.3454 - val_loss: 655574.5625 - val_rmse: 809.6756 - lr: 1.0000e-05\n",
            "Epoch 43/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 165272.0938 - rmse: 406.5367\n",
            "Epoch 00043: val_loss improved from 616740.43750 to 614250.12500, saving model to /content/drive/My Drive/Colab Notebooks/applications-of-deep-learning-wustlfall-2021/city/xception_bestmodel.h5\n",
            "250/250 [==============================] - 62s 247ms/step - loss: 165272.0938 - rmse: 406.5367 - val_loss: 614250.1250 - val_rmse: 783.7411 - lr: 1.0000e-05\n",
            "Epoch 44/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 158949.3281 - rmse: 398.6845\n",
            "Epoch 00044: val_loss improved from 614250.12500 to 611622.68750, saving model to /content/drive/My Drive/Colab Notebooks/applications-of-deep-learning-wustlfall-2021/city/xception_bestmodel.h5\n",
            "250/250 [==============================] - 62s 247ms/step - loss: 158949.3281 - rmse: 398.6845 - val_loss: 611622.6875 - val_rmse: 782.0631 - lr: 1.0000e-05\n",
            "Epoch 45/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 159127.7188 - rmse: 398.9081\n",
            "Epoch 00045: val_loss did not improve from 611622.68750\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 159127.7188 - rmse: 398.9081 - val_loss: 626414.3125 - val_rmse: 791.4634 - lr: 1.0000e-05\n",
            "Epoch 46/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 148586.2812 - rmse: 385.4689\n",
            "Epoch 00046: val_loss did not improve from 611622.68750\n",
            "250/250 [==============================] - 58s 230ms/step - loss: 148586.2812 - rmse: 385.4689 - val_loss: 617854.2500 - val_rmse: 786.0370 - lr: 1.0000e-05\n",
            "Epoch 47/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 141098.4062 - rmse: 375.6307\n",
            "Epoch 00047: val_loss did not improve from 611622.68750\n",
            "250/250 [==============================] - 58s 230ms/step - loss: 141098.4062 - rmse: 375.6307 - val_loss: 619012.5000 - val_rmse: 786.7735 - lr: 1.0000e-05\n",
            "Epoch 48/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 134682.8594 - rmse: 366.9916\n",
            "Epoch 00048: val_loss improved from 611622.68750 to 593942.31250, saving model to /content/drive/My Drive/Colab Notebooks/applications-of-deep-learning-wustlfall-2021/city/xception_bestmodel.h5\n",
            "250/250 [==============================] - 62s 248ms/step - loss: 134682.8594 - rmse: 366.9916 - val_loss: 593942.3125 - val_rmse: 770.6765 - lr: 1.0000e-05\n",
            "Epoch 49/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 137714.5156 - rmse: 371.0991\n",
            "Epoch 00049: val_loss did not improve from 593942.31250\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 137714.5156 - rmse: 371.0991 - val_loss: 629653.1250 - val_rmse: 793.5068 - lr: 1.0000e-05\n",
            "Epoch 50/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 127352.6641 - rmse: 356.8651\n",
            "Epoch 00050: val_loss did not improve from 593942.31250\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 127352.6641 - rmse: 356.8651 - val_loss: 635871.9375 - val_rmse: 797.4158 - lr: 1.0000e-05\n",
            "Epoch 51/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 144360.9219 - rmse: 379.9486\n",
            "Epoch 00051: val_loss did not improve from 593942.31250\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 144360.9219 - rmse: 379.9486 - val_loss: 640203.0000 - val_rmse: 800.1268 - lr: 1.0000e-05\n",
            "Epoch 52/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 125021.8359 - rmse: 353.5843\n",
            "Epoch 00052: val_loss did not improve from 593942.31250\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 125021.8359 - rmse: 353.5843 - val_loss: 606616.1250 - val_rmse: 778.8557 - lr: 1.0000e-05\n",
            "Epoch 53/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 115132.9531 - rmse: 339.3125\n",
            "Epoch 00053: val_loss did not improve from 593942.31250\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 115132.9531 - rmse: 339.3125 - val_loss: 610191.0625 - val_rmse: 781.1473 - lr: 1.0000e-05\n",
            "Epoch 54/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 124276.7500 - rmse: 352.5291\n",
            "Epoch 00054: val_loss did not improve from 593942.31250\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 124276.7500 - rmse: 352.5291 - val_loss: 650624.6250 - val_rmse: 806.6130 - lr: 1.0000e-05\n",
            "Epoch 55/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 112082.8438 - rmse: 334.7878\n",
            "Epoch 00055: val_loss did not improve from 593942.31250\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 112082.8438 - rmse: 334.7878 - val_loss: 598393.1250 - val_rmse: 773.5587 - lr: 1.0000e-05\n",
            "Epoch 56/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 114043.5391 - rmse: 337.7033\n",
            "Epoch 00056: val_loss did not improve from 593942.31250\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 114043.5391 - rmse: 337.7033 - val_loss: 691954.3125 - val_rmse: 831.8379 - lr: 1.0000e-05\n",
            "Epoch 57/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 117732.1875 - rmse: 343.1212\n",
            "Epoch 00057: val_loss improved from 593942.31250 to 574129.87500, saving model to /content/drive/My Drive/Colab Notebooks/applications-of-deep-learning-wustlfall-2021/city/xception_bestmodel.h5\n",
            "250/250 [==============================] - 62s 248ms/step - loss: 117732.1875 - rmse: 343.1212 - val_loss: 574129.8750 - val_rmse: 757.7136 - lr: 1.0000e-05\n",
            "Epoch 58/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 113263.6875 - rmse: 336.5467\n",
            "Epoch 00058: val_loss did not improve from 574129.87500\n",
            "250/250 [==============================] - 59s 233ms/step - loss: 113263.6875 - rmse: 336.5467 - val_loss: 700542.6250 - val_rmse: 836.9843 - lr: 1.0000e-05\n",
            "Epoch 59/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 126125.9922 - rmse: 355.1422\n",
            "Epoch 00059: val_loss did not improve from 574129.87500\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 126125.9922 - rmse: 355.1422 - val_loss: 584126.2500 - val_rmse: 764.2815 - lr: 1.0000e-05\n",
            "Epoch 60/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 111840.7656 - rmse: 334.4260\n",
            "Epoch 00060: val_loss did not improve from 574129.87500\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 111840.7656 - rmse: 334.4260 - val_loss: 603424.1875 - val_rmse: 776.8038 - lr: 1.0000e-05\n",
            "Epoch 61/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 104915.0000 - rmse: 323.9059\n",
            "Epoch 00061: val_loss did not improve from 574129.87500\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 104915.0000 - rmse: 323.9059 - val_loss: 591112.6250 - val_rmse: 768.8384 - lr: 1.0000e-05\n",
            "Epoch 62/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 112728.0625 - rmse: 335.7500\n",
            "Epoch 00062: val_loss did not improve from 574129.87500\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 112728.0625 - rmse: 335.7500 - val_loss: 583880.4375 - val_rmse: 764.1207 - lr: 1.0000e-05\n",
            "Epoch 63/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 113138.0469 - rmse: 336.3600\n",
            "Epoch 00063: val_loss did not improve from 574129.87500\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 113138.0469 - rmse: 336.3600 - val_loss: 591729.7500 - val_rmse: 769.2397 - lr: 1.0000e-05\n",
            "Epoch 64/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 116768.0781 - rmse: 341.7134\n",
            "Epoch 00064: val_loss did not improve from 574129.87500\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 116768.0781 - rmse: 341.7134 - val_loss: 585075.3125 - val_rmse: 764.9022 - lr: 1.0000e-05\n",
            "Epoch 65/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 100595.6562 - rmse: 317.1682\n",
            "Epoch 00065: val_loss did not improve from 574129.87500\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 100595.6562 - rmse: 317.1682 - val_loss: 589167.0000 - val_rmse: 767.5721 - lr: 1.0000e-05\n",
            "Epoch 66/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 99789.7969 - rmse: 315.8952\n",
            "Epoch 00066: val_loss did not improve from 574129.87500\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 99789.7969 - rmse: 315.8952 - val_loss: 585894.1250 - val_rmse: 765.4372 - lr: 1.0000e-05\n",
            "Epoch 67/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 114613.9766 - rmse: 338.5469\n",
            "Epoch 00067: val_loss did not improve from 574129.87500\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 114613.9766 - rmse: 338.5469 - val_loss: 592083.8125 - val_rmse: 769.4698 - lr: 1.0000e-05\n",
            "Epoch 68/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 87435.7578 - rmse: 295.6954\n",
            "Epoch 00068: val_loss did not improve from 574129.87500\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 87435.7578 - rmse: 295.6954 - val_loss: 602263.2500 - val_rmse: 776.0562 - lr: 1.0000e-05\n",
            "Epoch 69/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 92746.0703 - rmse: 304.5424\n",
            "Epoch 00069: val_loss improved from 574129.87500 to 563600.25000, saving model to /content/drive/My Drive/Colab Notebooks/applications-of-deep-learning-wustlfall-2021/city/xception_bestmodel.h5\n",
            "250/250 [==============================] - 62s 249ms/step - loss: 92746.0703 - rmse: 304.5424 - val_loss: 563600.2500 - val_rmse: 750.7332 - lr: 1.0000e-05\n",
            "Epoch 70/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 87253.6016 - rmse: 295.3872\n",
            "Epoch 00070: val_loss did not improve from 563600.25000\n",
            "250/250 [==============================] - 59s 234ms/step - loss: 87253.6016 - rmse: 295.3872 - val_loss: 578021.1875 - val_rmse: 760.2770 - lr: 1.0000e-05\n",
            "Epoch 71/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 95095.7578 - rmse: 308.3760\n",
            "Epoch 00071: val_loss did not improve from 563600.25000\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 95095.7578 - rmse: 308.3760 - val_loss: 570485.2500 - val_rmse: 755.3047 - lr: 1.0000e-05\n",
            "Epoch 72/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 93502.7031 - rmse: 305.7821\n",
            "Epoch 00072: val_loss did not improve from 563600.25000\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 93502.7031 - rmse: 305.7821 - val_loss: 570219.3125 - val_rmse: 755.1287 - lr: 1.0000e-05\n",
            "Epoch 73/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 97876.5625 - rmse: 312.8523\n",
            "Epoch 00073: val_loss did not improve from 563600.25000\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 97876.5625 - rmse: 312.8523 - val_loss: 566157.6875 - val_rmse: 752.4345 - lr: 1.0000e-05\n",
            "Epoch 74/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 88773.6406 - rmse: 297.9491\n",
            "Epoch 00074: val_loss did not improve from 563600.25000\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 88773.6406 - rmse: 297.9491 - val_loss: 596866.1875 - val_rmse: 772.5712 - lr: 1.0000e-05\n",
            "Epoch 75/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 86467.7812 - rmse: 294.0540\n",
            "Epoch 00075: val_loss did not improve from 563600.25000\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 86467.7812 - rmse: 294.0540 - val_loss: 593080.3125 - val_rmse: 770.1171 - lr: 1.0000e-05\n",
            "Epoch 76/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 82542.9375 - rmse: 287.3029\n",
            "Epoch 00076: val_loss improved from 563600.25000 to 560714.75000, saving model to /content/drive/My Drive/Colab Notebooks/applications-of-deep-learning-wustlfall-2021/city/xception_bestmodel.h5\n",
            "250/250 [==============================] - 62s 247ms/step - loss: 82542.9375 - rmse: 287.3029 - val_loss: 560714.7500 - val_rmse: 748.8089 - lr: 1.0000e-05\n",
            "Epoch 77/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 81488.2422 - rmse: 285.4615\n",
            "Epoch 00077: val_loss did not improve from 560714.75000\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 81488.2422 - rmse: 285.4615 - val_loss: 564367.7500 - val_rmse: 751.2441 - lr: 1.0000e-05\n",
            "Epoch 78/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 90796.0156 - rmse: 301.3238\n",
            "Epoch 00078: val_loss did not improve from 560714.75000\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 90796.0156 - rmse: 301.3238 - val_loss: 565498.0000 - val_rmse: 751.9960 - lr: 1.0000e-05\n",
            "Epoch 79/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 74359.9766 - rmse: 272.6902\n",
            "Epoch 00079: val_loss did not improve from 560714.75000\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 74359.9766 - rmse: 272.6902 - val_loss: 575238.2500 - val_rmse: 758.4446 - lr: 1.0000e-05\n",
            "Epoch 80/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 93079.8359 - rmse: 305.0899\n",
            "Epoch 00080: val_loss did not improve from 560714.75000\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 93079.8359 - rmse: 305.0899 - val_loss: 565705.2500 - val_rmse: 752.1338 - lr: 1.0000e-05\n",
            "Epoch 81/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 80600.5391 - rmse: 283.9023\n",
            "Epoch 00081: val_loss did not improve from 560714.75000\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 80600.5391 - rmse: 283.9023 - val_loss: 562827.4375 - val_rmse: 750.2183 - lr: 1.0000e-05\n",
            "Epoch 82/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 83200.1484 - rmse: 288.4444\n",
            "Epoch 00082: val_loss improved from 560714.75000 to 558888.56250, saving model to /content/drive/My Drive/Colab Notebooks/applications-of-deep-learning-wustlfall-2021/city/xception_bestmodel.h5\n",
            "250/250 [==============================] - 62s 246ms/step - loss: 83200.1484 - rmse: 288.4444 - val_loss: 558888.5625 - val_rmse: 747.5885 - lr: 1.0000e-05\n",
            "Epoch 83/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 85254.1719 - rmse: 291.9832\n",
            "Epoch 00083: val_loss did not improve from 558888.56250\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 85254.1719 - rmse: 291.9832 - val_loss: 560142.3125 - val_rmse: 748.4266 - lr: 1.0000e-05\n",
            "Epoch 84/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 81333.5078 - rmse: 285.1903\n",
            "Epoch 00084: val_loss improved from 558888.56250 to 554133.62500, saving model to /content/drive/My Drive/Colab Notebooks/applications-of-deep-learning-wustlfall-2021/city/xception_bestmodel.h5\n",
            "250/250 [==============================] - 62s 247ms/step - loss: 81333.5078 - rmse: 285.1903 - val_loss: 554133.6250 - val_rmse: 744.4016 - lr: 1.0000e-05\n",
            "Epoch 85/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 77174.4141 - rmse: 277.8028\n",
            "Epoch 00085: val_loss did not improve from 554133.62500\n",
            "250/250 [==============================] - 58s 233ms/step - loss: 77174.4141 - rmse: 277.8028 - val_loss: 589402.4375 - val_rmse: 767.7255 - lr: 1.0000e-05\n",
            "Epoch 86/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 85384.6094 - rmse: 292.2065\n",
            "Epoch 00086: val_loss did not improve from 554133.62500\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 85384.6094 - rmse: 292.2065 - val_loss: 581169.7500 - val_rmse: 762.3449 - lr: 1.0000e-05\n",
            "Epoch 87/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 72850.8516 - rmse: 269.9090\n",
            "Epoch 00087: val_loss improved from 554133.62500 to 547591.12500, saving model to /content/drive/My Drive/Colab Notebooks/applications-of-deep-learning-wustlfall-2021/city/xception_bestmodel.h5\n",
            "250/250 [==============================] - 68s 272ms/step - loss: 72850.8516 - rmse: 269.9090 - val_loss: 547591.1250 - val_rmse: 739.9940 - lr: 1.0000e-05\n",
            "Epoch 88/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 82713.8906 - rmse: 287.6002\n",
            "Epoch 00088: val_loss did not improve from 547591.12500\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 82713.8906 - rmse: 287.6002 - val_loss: 606174.9375 - val_rmse: 778.5724 - lr: 1.0000e-05\n",
            "Epoch 89/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 85721.9141 - rmse: 292.7831\n",
            "Epoch 00089: val_loss did not improve from 547591.12500\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 85721.9141 - rmse: 292.7831 - val_loss: 555023.1250 - val_rmse: 744.9987 - lr: 1.0000e-05\n",
            "Epoch 90/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 72011.4688 - rmse: 268.3495\n",
            "Epoch 00090: val_loss did not improve from 547591.12500\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 72011.4688 - rmse: 268.3495 - val_loss: 554914.0000 - val_rmse: 744.9255 - lr: 1.0000e-05\n",
            "Epoch 91/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 71954.2656 - rmse: 268.2429\n",
            "Epoch 00091: val_loss did not improve from 547591.12500\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 71954.2656 - rmse: 268.2429 - val_loss: 564230.2500 - val_rmse: 751.1526 - lr: 1.0000e-05\n",
            "Epoch 92/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 67535.2188 - rmse: 259.8754\n",
            "Epoch 00092: val_loss did not improve from 547591.12500\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 67535.2188 - rmse: 259.8754 - val_loss: 549907.6875 - val_rmse: 741.5576 - lr: 1.0000e-05\n",
            "Epoch 93/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 69761.8516 - rmse: 264.1247\n",
            "Epoch 00093: val_loss did not improve from 547591.12500\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 69761.8516 - rmse: 264.1247 - val_loss: 561617.1875 - val_rmse: 749.4113 - lr: 1.0000e-05\n",
            "Epoch 94/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 69519.0625 - rmse: 263.6647\n",
            "Epoch 00094: val_loss did not improve from 547591.12500\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 69519.0625 - rmse: 263.6647 - val_loss: 567968.5625 - val_rmse: 753.6369 - lr: 1.0000e-05\n",
            "Epoch 95/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 71860.5469 - rmse: 268.0682\n",
            "Epoch 00095: val_loss improved from 547591.12500 to 546397.75000, saving model to /content/drive/My Drive/Colab Notebooks/applications-of-deep-learning-wustlfall-2021/city/xception_bestmodel.h5\n",
            "250/250 [==============================] - 62s 248ms/step - loss: 71860.5469 - rmse: 268.0682 - val_loss: 546397.7500 - val_rmse: 739.1873 - lr: 1.0000e-05\n",
            "Epoch 96/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 77896.1875 - rmse: 279.0989\n",
            "Epoch 00096: val_loss did not improve from 546397.75000\n",
            "250/250 [==============================] - 58s 233ms/step - loss: 77896.1875 - rmse: 279.0989 - val_loss: 589610.7500 - val_rmse: 767.8611 - lr: 1.0000e-05\n",
            "Epoch 97/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 69915.0469 - rmse: 264.4145\n",
            "Epoch 00097: val_loss did not improve from 546397.75000\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 69915.0469 - rmse: 264.4145 - val_loss: 594050.2500 - val_rmse: 770.7465 - lr: 1.0000e-05\n",
            "Epoch 98/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 75752.9453 - rmse: 275.2325\n",
            "Epoch 00098: val_loss did not improve from 546397.75000\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 75752.9453 - rmse: 275.2325 - val_loss: 565388.0625 - val_rmse: 751.9229 - lr: 1.0000e-05\n",
            "Epoch 99/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 72953.3828 - rmse: 270.0988\n",
            "Epoch 00099: val_loss improved from 546397.75000 to 535360.93750, saving model to /content/drive/My Drive/Colab Notebooks/applications-of-deep-learning-wustlfall-2021/city/xception_bestmodel.h5\n",
            "250/250 [==============================] - 62s 248ms/step - loss: 72953.3828 - rmse: 270.0988 - val_loss: 535360.9375 - val_rmse: 731.6836 - lr: 1.0000e-05\n",
            "Epoch 100/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 64133.6055 - rmse: 253.2461\n",
            "Epoch 00100: val_loss did not improve from 535360.93750\n",
            "250/250 [==============================] - 59s 232ms/step - loss: 64133.6055 - rmse: 253.2461 - val_loss: 548783.7500 - val_rmse: 740.7994 - lr: 1.0000e-05\n",
            "Epoch 101/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 68737.6641 - rmse: 262.1787\n",
            "Epoch 00101: val_loss did not improve from 535360.93750\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 68737.6641 - rmse: 262.1787 - val_loss: 551377.7500 - val_rmse: 742.5482 - lr: 1.0000e-05\n",
            "Epoch 102/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 65743.1641 - rmse: 256.4043\n",
            "Epoch 00102: val_loss did not improve from 535360.93750\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 65743.1641 - rmse: 256.4043 - val_loss: 543441.3125 - val_rmse: 737.1847 - lr: 1.0000e-05\n",
            "Epoch 103/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 66744.4375 - rmse: 258.3494\n",
            "Epoch 00103: val_loss improved from 535360.93750 to 535096.81250, saving model to /content/drive/My Drive/Colab Notebooks/applications-of-deep-learning-wustlfall-2021/city/xception_bestmodel.h5\n",
            "250/250 [==============================] - 62s 247ms/step - loss: 66744.4375 - rmse: 258.3494 - val_loss: 535096.8125 - val_rmse: 731.5031 - lr: 1.0000e-05\n",
            "Epoch 104/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 72183.9531 - rmse: 268.6707\n",
            "Epoch 00104: val_loss did not improve from 535096.81250\n",
            "250/250 [==============================] - 58s 233ms/step - loss: 72183.9531 - rmse: 268.6707 - val_loss: 562850.7500 - val_rmse: 750.2338 - lr: 1.0000e-05\n",
            "Epoch 105/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 65800.1875 - rmse: 256.5155\n",
            "Epoch 00105: val_loss improved from 535096.81250 to 534353.50000, saving model to /content/drive/My Drive/Colab Notebooks/applications-of-deep-learning-wustlfall-2021/city/xception_bestmodel.h5\n",
            "250/250 [==============================] - 63s 250ms/step - loss: 65800.1875 - rmse: 256.5155 - val_loss: 534353.5000 - val_rmse: 730.9949 - lr: 1.0000e-05\n",
            "Epoch 106/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 66188.0547 - rmse: 257.2704\n",
            "Epoch 00106: val_loss did not improve from 534353.50000\n",
            "250/250 [==============================] - 58s 233ms/step - loss: 66188.0547 - rmse: 257.2704 - val_loss: 536190.1250 - val_rmse: 732.2500 - lr: 1.0000e-05\n",
            "Epoch 107/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 71671.0234 - rmse: 267.7144\n",
            "Epoch 00107: val_loss did not improve from 534353.50000\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 71671.0234 - rmse: 267.7144 - val_loss: 567585.8125 - val_rmse: 753.3829 - lr: 1.0000e-05\n",
            "Epoch 108/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 68155.7500 - rmse: 261.0666\n",
            "Epoch 00108: val_loss did not improve from 534353.50000\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 68155.7500 - rmse: 261.0666 - val_loss: 540037.7500 - val_rmse: 734.8726 - lr: 1.0000e-05\n",
            "Epoch 109/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 65006.2969 - rmse: 254.9633\n",
            "Epoch 00109: val_loss did not improve from 534353.50000\n",
            "250/250 [==============================] - 68s 271ms/step - loss: 65006.2969 - rmse: 254.9633 - val_loss: 544382.3750 - val_rmse: 737.8227 - lr: 1.0000e-05\n",
            "Epoch 110/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 60592.3438 - rmse: 246.1551\n",
            "Epoch 00110: val_loss did not improve from 534353.50000\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 60592.3438 - rmse: 246.1551 - val_loss: 557800.4375 - val_rmse: 746.8604 - lr: 1.0000e-05\n",
            "Epoch 111/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 72436.5391 - rmse: 269.1404\n",
            "Epoch 00111: val_loss did not improve from 534353.50000\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 72436.5391 - rmse: 269.1404 - val_loss: 558889.7500 - val_rmse: 747.5893 - lr: 1.0000e-05\n",
            "Epoch 112/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 61260.7227 - rmse: 247.5090\n",
            "Epoch 00112: val_loss improved from 534353.50000 to 523115.50000, saving model to /content/drive/My Drive/Colab Notebooks/applications-of-deep-learning-wustlfall-2021/city/xception_bestmodel.h5\n",
            "250/250 [==============================] - 62s 247ms/step - loss: 61260.7227 - rmse: 247.5090 - val_loss: 523115.5000 - val_rmse: 723.2672 - lr: 1.0000e-05\n",
            "Epoch 113/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 57977.6328 - rmse: 240.7854\n",
            "Epoch 00113: val_loss did not improve from 523115.50000\n",
            "250/250 [==============================] - 58s 233ms/step - loss: 57977.6328 - rmse: 240.7854 - val_loss: 546224.0000 - val_rmse: 739.0697 - lr: 1.0000e-05\n",
            "Epoch 114/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 61116.0273 - rmse: 247.2166\n",
            "Epoch 00114: val_loss did not improve from 523115.50000\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 61116.0273 - rmse: 247.2166 - val_loss: 546347.3750 - val_rmse: 739.1531 - lr: 1.0000e-05\n",
            "Epoch 115/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 65113.2930 - rmse: 255.1731\n",
            "Epoch 00115: val_loss did not improve from 523115.50000\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 65113.2930 - rmse: 255.1731 - val_loss: 526627.5000 - val_rmse: 725.6910 - lr: 1.0000e-05\n",
            "Epoch 116/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 59365.9531 - rmse: 243.6513\n",
            "Epoch 00116: val_loss did not improve from 523115.50000\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 59365.9531 - rmse: 243.6513 - val_loss: 552170.1250 - val_rmse: 743.0815 - lr: 1.0000e-05\n",
            "Epoch 117/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 63651.5391 - rmse: 252.2926\n",
            "Epoch 00117: val_loss did not improve from 523115.50000\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 63651.5391 - rmse: 252.2926 - val_loss: 529252.5000 - val_rmse: 727.4974 - lr: 1.0000e-05\n",
            "Epoch 118/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 58888.9883 - rmse: 242.6705\n",
            "Epoch 00118: val_loss did not improve from 523115.50000\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 58888.9883 - rmse: 242.6705 - val_loss: 541165.7500 - val_rmse: 735.6396 - lr: 1.0000e-05\n",
            "Epoch 119/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 53199.6836 - rmse: 230.6506\n",
            "Epoch 00119: val_loss improved from 523115.50000 to 522575.65625, saving model to /content/drive/My Drive/Colab Notebooks/applications-of-deep-learning-wustlfall-2021/city/xception_bestmodel.h5\n",
            "250/250 [==============================] - 62s 248ms/step - loss: 53199.6836 - rmse: 230.6506 - val_loss: 522575.6562 - val_rmse: 722.8939 - lr: 1.0000e-05\n",
            "Epoch 120/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 59023.4141 - rmse: 242.9473\n",
            "Epoch 00120: val_loss did not improve from 522575.65625\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 59023.4141 - rmse: 242.9473 - val_loss: 523223.7188 - val_rmse: 723.3420 - lr: 1.0000e-05\n",
            "Epoch 121/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 56753.9258 - rmse: 238.2308\n",
            "Epoch 00121: val_loss did not improve from 522575.65625\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 56753.9258 - rmse: 238.2308 - val_loss: 525408.2500 - val_rmse: 724.8505 - lr: 1.0000e-05\n",
            "Epoch 122/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 59198.6406 - rmse: 243.3077\n",
            "Epoch 00122: val_loss did not improve from 522575.65625\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 59198.6406 - rmse: 243.3077 - val_loss: 538442.7500 - val_rmse: 733.7866 - lr: 1.0000e-05\n",
            "Epoch 123/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 64407.0742 - rmse: 253.7855\n",
            "Epoch 00123: val_loss did not improve from 522575.65625\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 64407.0742 - rmse: 253.7855 - val_loss: 535252.5625 - val_rmse: 731.6096 - lr: 1.0000e-05\n",
            "Epoch 124/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 59926.6211 - rmse: 244.7991\n",
            "Epoch 00124: val_loss did not improve from 522575.65625\n",
            "250/250 [==============================] - 58s 230ms/step - loss: 59926.6211 - rmse: 244.7991 - val_loss: 538823.3125 - val_rmse: 734.0458 - lr: 1.0000e-05\n",
            "Epoch 125/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 55285.7969 - rmse: 235.1293\n",
            "Epoch 00125: val_loss improved from 522575.65625 to 509897.21875, saving model to /content/drive/My Drive/Colab Notebooks/applications-of-deep-learning-wustlfall-2021/city/xception_bestmodel.h5\n",
            "250/250 [==============================] - 62s 248ms/step - loss: 55285.7969 - rmse: 235.1293 - val_loss: 509897.2188 - val_rmse: 714.0709 - lr: 1.0000e-05\n",
            "Epoch 126/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 59232.9297 - rmse: 243.3782\n",
            "Epoch 00126: val_loss did not improve from 509897.21875\n",
            "250/250 [==============================] - 58s 233ms/step - loss: 59232.9297 - rmse: 243.3782 - val_loss: 524764.1250 - val_rmse: 724.4060 - lr: 1.0000e-05\n",
            "Epoch 127/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 57514.3320 - rmse: 239.8215\n",
            "Epoch 00127: val_loss did not improve from 509897.21875\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 57514.3320 - rmse: 239.8215 - val_loss: 528215.3125 - val_rmse: 726.7842 - lr: 1.0000e-05\n",
            "Epoch 128/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 57380.5078 - rmse: 239.5423\n",
            "Epoch 00128: val_loss did not improve from 509897.21875\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 57380.5078 - rmse: 239.5423 - val_loss: 532411.7500 - val_rmse: 729.6655 - lr: 1.0000e-05\n",
            "Epoch 129/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 51439.4727 - rmse: 226.8027\n",
            "Epoch 00129: val_loss did not improve from 509897.21875\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 51439.4727 - rmse: 226.8027 - val_loss: 511791.3438 - val_rmse: 715.3959 - lr: 1.0000e-05\n",
            "Epoch 130/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 58233.9492 - rmse: 241.3171\n",
            "Epoch 00130: val_loss did not improve from 509897.21875\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 58233.9492 - rmse: 241.3171 - val_loss: 603283.2500 - val_rmse: 776.7131 - lr: 1.0000e-05\n",
            "Epoch 131/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 52344.2422 - rmse: 228.7886\n",
            "Epoch 00131: val_loss improved from 509897.21875 to 508022.50000, saving model to /content/drive/My Drive/Colab Notebooks/applications-of-deep-learning-wustlfall-2021/city/xception_bestmodel.h5\n",
            "250/250 [==============================] - 62s 248ms/step - loss: 52344.2422 - rmse: 228.7886 - val_loss: 508022.5000 - val_rmse: 712.7570 - lr: 1.0000e-05\n",
            "Epoch 132/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 55220.9297 - rmse: 234.9913\n",
            "Epoch 00132: val_loss did not improve from 508022.50000\n",
            "250/250 [==============================] - 59s 234ms/step - loss: 55220.9297 - rmse: 234.9913 - val_loss: 548508.7500 - val_rmse: 740.6138 - lr: 1.0000e-05\n",
            "Epoch 133/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 51061.7695 - rmse: 225.9685\n",
            "Epoch 00133: val_loss did not improve from 508022.50000\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 51061.7695 - rmse: 225.9685 - val_loss: 517314.1250 - val_rmse: 719.2455 - lr: 1.0000e-05\n",
            "Epoch 134/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 52029.4219 - rmse: 228.0996\n",
            "Epoch 00134: val_loss did not improve from 508022.50000\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 52029.4219 - rmse: 228.0996 - val_loss: 528951.7500 - val_rmse: 727.2907 - lr: 1.0000e-05\n",
            "Epoch 135/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 50869.1758 - rmse: 225.5420\n",
            "Epoch 00135: val_loss did not improve from 508022.50000\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 50869.1758 - rmse: 225.5420 - val_loss: 514292.1250 - val_rmse: 717.1416 - lr: 1.0000e-05\n",
            "Epoch 136/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 49014.5352 - rmse: 221.3923\n",
            "Epoch 00136: val_loss did not improve from 508022.50000\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 49014.5352 - rmse: 221.3923 - val_loss: 550973.0000 - val_rmse: 742.2755 - lr: 1.0000e-05\n",
            "Epoch 137/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 51651.8281 - rmse: 227.2704\n",
            "Epoch 00137: val_loss did not improve from 508022.50000\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 51651.8281 - rmse: 227.2704 - val_loss: 521834.0000 - val_rmse: 722.3808 - lr: 1.0000e-05\n",
            "Epoch 138/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 53524.8477 - rmse: 231.3544\n",
            "Epoch 00138: val_loss did not improve from 508022.50000\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 53524.8477 - rmse: 231.3544 - val_loss: 526087.7500 - val_rmse: 725.3190 - lr: 1.0000e-05\n",
            "Epoch 139/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 51043.9883 - rmse: 225.9292\n",
            "Epoch 00139: val_loss did not improve from 508022.50000\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 51043.9883 - rmse: 225.9292 - val_loss: 521013.6562 - val_rmse: 721.8127 - lr: 1.0000e-05\n",
            "Epoch 140/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 48362.9609 - rmse: 219.9158\n",
            "Epoch 00140: val_loss did not improve from 508022.50000\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 48362.9609 - rmse: 219.9158 - val_loss: 519322.0000 - val_rmse: 720.6400 - lr: 1.0000e-05\n",
            "Epoch 141/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 49734.3828 - rmse: 223.0121\n",
            "Epoch 00141: val_loss did not improve from 508022.50000\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 49734.3828 - rmse: 223.0121 - val_loss: 534220.4375 - val_rmse: 730.9038 - lr: 1.0000e-05\n",
            "Epoch 142/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 50491.8984 - rmse: 224.7040\n",
            "Epoch 00142: val_loss did not improve from 508022.50000\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 50491.8984 - rmse: 224.7040 - val_loss: 521772.2188 - val_rmse: 722.3380 - lr: 1.0000e-05\n",
            "Epoch 143/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 54728.7305 - rmse: 233.9417\n",
            "Epoch 00143: val_loss improved from 508022.50000 to 500468.62500, saving model to /content/drive/My Drive/Colab Notebooks/applications-of-deep-learning-wustlfall-2021/city/xception_bestmodel.h5\n",
            "250/250 [==============================] - 62s 246ms/step - loss: 54728.7305 - rmse: 233.9417 - val_loss: 500468.6250 - val_rmse: 707.4380 - lr: 1.0000e-05\n",
            "Epoch 144/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 50338.4336 - rmse: 224.3623\n",
            "Epoch 00144: val_loss did not improve from 500468.62500\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 50338.4336 - rmse: 224.3623 - val_loss: 536463.0000 - val_rmse: 732.4363 - lr: 1.0000e-05\n",
            "Epoch 145/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 53326.0273 - rmse: 230.9243\n",
            "Epoch 00145: val_loss did not improve from 500468.62500\n",
            "250/250 [==============================] - 58s 230ms/step - loss: 53326.0273 - rmse: 230.9243 - val_loss: 504406.8750 - val_rmse: 710.2161 - lr: 1.0000e-05\n",
            "Epoch 146/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 48032.1211 - rmse: 219.1623\n",
            "Epoch 00146: val_loss improved from 500468.62500 to 493944.50000, saving model to /content/drive/My Drive/Colab Notebooks/applications-of-deep-learning-wustlfall-2021/city/xception_bestmodel.h5\n",
            "250/250 [==============================] - 61s 246ms/step - loss: 48032.1211 - rmse: 219.1623 - val_loss: 493944.5000 - val_rmse: 702.8118 - lr: 1.0000e-05\n",
            "Epoch 147/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 47241.0859 - rmse: 217.3501\n",
            "Epoch 00147: val_loss did not improve from 493944.50000\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 47241.0859 - rmse: 217.3501 - val_loss: 505671.7812 - val_rmse: 711.1060 - lr: 1.0000e-05\n",
            "Epoch 148/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 49243.6875 - rmse: 221.9092\n",
            "Epoch 00148: val_loss did not improve from 493944.50000\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 49243.6875 - rmse: 221.9092 - val_loss: 516515.5625 - val_rmse: 718.6902 - lr: 1.0000e-05\n",
            "Epoch 149/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 49413.4648 - rmse: 222.2914\n",
            "Epoch 00149: val_loss did not improve from 493944.50000\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 49413.4648 - rmse: 222.2914 - val_loss: 516410.3750 - val_rmse: 718.6170 - lr: 1.0000e-05\n",
            "Epoch 150/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 51563.1016 - rmse: 227.0751\n",
            "Epoch 00150: val_loss did not improve from 493944.50000\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 51563.1016 - rmse: 227.0751 - val_loss: 554437.6875 - val_rmse: 744.6057 - lr: 1.0000e-05\n",
            "Epoch 151/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 50449.4453 - rmse: 224.6095\n",
            "Epoch 00151: val_loss did not improve from 493944.50000\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 50449.4453 - rmse: 224.6095 - val_loss: 523373.5625 - val_rmse: 723.4456 - lr: 1.0000e-05\n",
            "Epoch 152/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 47363.3867 - rmse: 217.6313\n",
            "Epoch 00152: val_loss did not improve from 493944.50000\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 47363.3867 - rmse: 217.6313 - val_loss: 551052.1250 - val_rmse: 742.3289 - lr: 1.0000e-05\n",
            "Epoch 153/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 51224.1602 - rmse: 226.3275\n",
            "Epoch 00153: val_loss did not improve from 493944.50000\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 51224.1602 - rmse: 226.3275 - val_loss: 510710.5625 - val_rmse: 714.6401 - lr: 1.0000e-05\n",
            "Epoch 154/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 44090.2539 - rmse: 209.9768\n",
            "Epoch 00154: val_loss did not improve from 493944.50000\n",
            "250/250 [==============================] - 58s 230ms/step - loss: 44090.2539 - rmse: 209.9768 - val_loss: 522645.7812 - val_rmse: 722.9424 - lr: 1.0000e-05\n",
            "Epoch 155/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 44012.6055 - rmse: 209.7918\n",
            "Epoch 00155: val_loss did not improve from 493944.50000\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 44012.6055 - rmse: 209.7918 - val_loss: 499891.1562 - val_rmse: 707.0298 - lr: 1.0000e-05\n",
            "Epoch 156/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 42977.1289 - rmse: 207.3093\n",
            "Epoch 00156: val_loss did not improve from 493944.50000\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 42977.1289 - rmse: 207.3093 - val_loss: 507590.6562 - val_rmse: 712.4540 - lr: 1.0000e-05\n",
            "Epoch 157/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 48278.6445 - rmse: 219.7240\n",
            "Epoch 00157: val_loss did not improve from 493944.50000\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 48278.6445 - rmse: 219.7240 - val_loss: 526182.3125 - val_rmse: 725.3842 - lr: 1.0000e-05\n",
            "Epoch 158/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 48378.9727 - rmse: 219.9522\n",
            "Epoch 00158: val_loss did not improve from 493944.50000\n",
            "250/250 [==============================] - 58s 230ms/step - loss: 48378.9727 - rmse: 219.9522 - val_loss: 508966.3438 - val_rmse: 713.4188 - lr: 1.0000e-05\n",
            "Epoch 159/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 44674.5078 - rmse: 211.3634\n",
            "Epoch 00159: val_loss did not improve from 493944.50000\n",
            "250/250 [==============================] - 68s 271ms/step - loss: 44674.5078 - rmse: 211.3634 - val_loss: 497040.2188 - val_rmse: 705.0108 - lr: 1.0000e-05\n",
            "Epoch 160/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 43824.1406 - rmse: 209.3422\n",
            "Epoch 00160: val_loss did not improve from 493944.50000\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 43824.1406 - rmse: 209.3422 - val_loss: 520537.5625 - val_rmse: 721.4829 - lr: 1.0000e-05\n",
            "Epoch 161/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 43770.2734 - rmse: 209.2135\n",
            "Epoch 00161: val_loss did not improve from 493944.50000\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 43770.2734 - rmse: 209.2135 - val_loss: 496067.8750 - val_rmse: 704.3209 - lr: 1.0000e-05\n",
            "Epoch 162/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 47858.1719 - rmse: 218.7651\n",
            "Epoch 00162: val_loss did not improve from 493944.50000\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 47858.1719 - rmse: 218.7651 - val_loss: 505418.0625 - val_rmse: 710.9276 - lr: 1.0000e-05\n",
            "Epoch 163/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 47197.8516 - rmse: 217.2507\n",
            "Epoch 00163: val_loss did not improve from 493944.50000\n",
            "250/250 [==============================] - 58s 230ms/step - loss: 47197.8516 - rmse: 217.2507 - val_loss: 500356.8750 - val_rmse: 707.3591 - lr: 1.0000e-05\n",
            "Epoch 164/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 42196.6602 - rmse: 205.4183\n",
            "Epoch 00164: val_loss did not improve from 493944.50000\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 42196.6602 - rmse: 205.4183 - val_loss: 525909.6250 - val_rmse: 725.1962 - lr: 1.0000e-05\n",
            "Epoch 165/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 45190.7266 - rmse: 212.5811\n",
            "Epoch 00165: val_loss improved from 493944.50000 to 493679.50000, saving model to /content/drive/My Drive/Colab Notebooks/applications-of-deep-learning-wustlfall-2021/city/xception_bestmodel.h5\n",
            "250/250 [==============================] - 61s 246ms/step - loss: 45190.7266 - rmse: 212.5811 - val_loss: 493679.5000 - val_rmse: 702.6233 - lr: 1.0000e-05\n",
            "Epoch 166/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 41977.6875 - rmse: 204.8846\n",
            "Epoch 00166: val_loss did not improve from 493679.50000\n",
            "250/250 [==============================] - 58s 233ms/step - loss: 41977.6875 - rmse: 204.8846 - val_loss: 506323.9375 - val_rmse: 711.5645 - lr: 1.0000e-05\n",
            "Epoch 167/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 44765.7656 - rmse: 211.5792\n",
            "Epoch 00167: val_loss improved from 493679.50000 to 491119.28125, saving model to /content/drive/My Drive/Colab Notebooks/applications-of-deep-learning-wustlfall-2021/city/xception_bestmodel.h5\n",
            "250/250 [==============================] - 62s 247ms/step - loss: 44765.7656 - rmse: 211.5792 - val_loss: 491119.2812 - val_rmse: 700.7990 - lr: 1.0000e-05\n",
            "Epoch 168/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 43430.7109 - rmse: 208.4004\n",
            "Epoch 00168: val_loss did not improve from 491119.28125\n",
            "250/250 [==============================] - 59s 232ms/step - loss: 43430.7109 - rmse: 208.4004 - val_loss: 512303.2188 - val_rmse: 715.7536 - lr: 1.0000e-05\n",
            "Epoch 169/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 45336.7227 - rmse: 212.9242\n",
            "Epoch 00169: val_loss did not improve from 491119.28125\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 45336.7227 - rmse: 212.9242 - val_loss: 515819.9375 - val_rmse: 718.2061 - lr: 1.0000e-05\n",
            "Epoch 170/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 44843.6484 - rmse: 211.7632\n",
            "Epoch 00170: val_loss improved from 491119.28125 to 483599.21875, saving model to /content/drive/My Drive/Colab Notebooks/applications-of-deep-learning-wustlfall-2021/city/xception_bestmodel.h5\n",
            "250/250 [==============================] - 62s 248ms/step - loss: 44843.6484 - rmse: 211.7632 - val_loss: 483599.2188 - val_rmse: 695.4130 - lr: 1.0000e-05\n",
            "Epoch 171/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 43611.0742 - rmse: 208.8326\n",
            "Epoch 00171: val_loss did not improve from 483599.21875\n",
            "250/250 [==============================] - 59s 235ms/step - loss: 43611.0742 - rmse: 208.8326 - val_loss: 534313.0625 - val_rmse: 730.9672 - lr: 1.0000e-05\n",
            "Epoch 172/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 43172.3906 - rmse: 207.7797\n",
            "Epoch 00172: val_loss did not improve from 483599.21875\n",
            "250/250 [==============================] - 58s 230ms/step - loss: 43172.3906 - rmse: 207.7797 - val_loss: 490241.2812 - val_rmse: 700.1723 - lr: 1.0000e-05\n",
            "Epoch 173/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 45217.4141 - rmse: 212.6439\n",
            "Epoch 00173: val_loss did not improve from 483599.21875\n",
            "250/250 [==============================] - 58s 230ms/step - loss: 45217.4141 - rmse: 212.6439 - val_loss: 484512.3750 - val_rmse: 696.0692 - lr: 1.0000e-05\n",
            "Epoch 174/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 43344.2422 - rmse: 208.1928\n",
            "Epoch 00174: val_loss did not improve from 483599.21875\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 43344.2422 - rmse: 208.1928 - val_loss: 500484.1562 - val_rmse: 707.4490 - lr: 1.0000e-05\n",
            "Epoch 175/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 45676.4453 - rmse: 213.7205\n",
            "Epoch 00175: val_loss did not improve from 483599.21875\n",
            "250/250 [==============================] - 58s 230ms/step - loss: 45676.4453 - rmse: 213.7205 - val_loss: 501249.3438 - val_rmse: 707.9896 - lr: 1.0000e-05\n",
            "Epoch 176/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 41809.1719 - rmse: 204.4729\n",
            "Epoch 00176: val_loss did not improve from 483599.21875\n",
            "250/250 [==============================] - 58s 230ms/step - loss: 41809.1719 - rmse: 204.4729 - val_loss: 502778.7188 - val_rmse: 709.0689 - lr: 1.0000e-05\n",
            "Epoch 177/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 42750.9688 - rmse: 206.7631\n",
            "Epoch 00177: val_loss did not improve from 483599.21875\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 42750.9688 - rmse: 206.7631 - val_loss: 499932.0625 - val_rmse: 707.0587 - lr: 1.0000e-05\n",
            "Epoch 178/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 40561.0391 - rmse: 201.3977\n",
            "Epoch 00178: val_loss did not improve from 483599.21875\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 40561.0391 - rmse: 201.3977 - val_loss: 488574.0000 - val_rmse: 698.9807 - lr: 1.0000e-05\n",
            "Epoch 179/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 40112.6602 - rmse: 200.2814\n",
            "Epoch 00179: val_loss improved from 483599.21875 to 481762.62500, saving model to /content/drive/My Drive/Colab Notebooks/applications-of-deep-learning-wustlfall-2021/city/xception_bestmodel.h5\n",
            "250/250 [==============================] - 62s 247ms/step - loss: 40112.6602 - rmse: 200.2814 - val_loss: 481762.6250 - val_rmse: 694.0912 - lr: 1.0000e-05\n",
            "Epoch 180/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 38645.8203 - rmse: 196.5854\n",
            "Epoch 00180: val_loss did not improve from 481762.62500\n",
            "250/250 [==============================] - 58s 233ms/step - loss: 38645.8203 - rmse: 196.5854 - val_loss: 534499.5625 - val_rmse: 731.0947 - lr: 1.0000e-05\n",
            "Epoch 181/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 40408.3789 - rmse: 201.0184\n",
            "Epoch 00181: val_loss improved from 481762.62500 to 480006.00000, saving model to /content/drive/My Drive/Colab Notebooks/applications-of-deep-learning-wustlfall-2021/city/xception_bestmodel.h5\n",
            "250/250 [==============================] - 62s 247ms/step - loss: 40408.3789 - rmse: 201.0184 - val_loss: 480006.0000 - val_rmse: 692.8246 - lr: 1.0000e-05\n",
            "Epoch 182/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 39776.2852 - rmse: 199.4399\n",
            "Epoch 00182: val_loss improved from 480006.00000 to 476617.56250, saving model to /content/drive/My Drive/Colab Notebooks/applications-of-deep-learning-wustlfall-2021/city/xception_bestmodel.h5\n",
            "250/250 [==============================] - 63s 250ms/step - loss: 39776.2852 - rmse: 199.4399 - val_loss: 476617.5625 - val_rmse: 690.3749 - lr: 1.0000e-05\n",
            "Epoch 183/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 43047.1211 - rmse: 207.4780\n",
            "Epoch 00183: val_loss did not improve from 476617.56250\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 43047.1211 - rmse: 207.4780 - val_loss: 495856.0625 - val_rmse: 704.1705 - lr: 1.0000e-05\n",
            "Epoch 184/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 39246.8633 - rmse: 198.1082\n",
            "Epoch 00184: val_loss did not improve from 476617.56250\n",
            "250/250 [==============================] - 58s 230ms/step - loss: 39246.8633 - rmse: 198.1082 - val_loss: 529949.5625 - val_rmse: 727.9763 - lr: 1.0000e-05\n",
            "Epoch 185/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 43719.5898 - rmse: 209.0923\n",
            "Epoch 00185: val_loss did not improve from 476617.56250\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 43719.5898 - rmse: 209.0923 - val_loss: 486681.1562 - val_rmse: 697.6254 - lr: 1.0000e-05\n",
            "Epoch 186/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 39401.2891 - rmse: 198.4976\n",
            "Epoch 00186: val_loss did not improve from 476617.56250\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 39401.2891 - rmse: 198.4976 - val_loss: 488566.5625 - val_rmse: 698.9753 - lr: 1.0000e-05\n",
            "Epoch 187/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 37998.6172 - rmse: 194.9323\n",
            "Epoch 00187: val_loss did not improve from 476617.56250\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 37998.6172 - rmse: 194.9323 - val_loss: 493490.9375 - val_rmse: 702.4891 - lr: 1.0000e-05\n",
            "Epoch 188/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 44580.4766 - rmse: 211.1409\n",
            "Epoch 00188: val_loss did not improve from 476617.56250\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 44580.4766 - rmse: 211.1409 - val_loss: 520603.0000 - val_rmse: 721.5283 - lr: 1.0000e-05\n",
            "Epoch 189/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 38376.5234 - rmse: 195.8993\n",
            "Epoch 00189: val_loss did not improve from 476617.56250\n",
            "250/250 [==============================] - 58s 230ms/step - loss: 38376.5234 - rmse: 195.8993 - val_loss: 479322.0000 - val_rmse: 692.3308 - lr: 1.0000e-05\n",
            "Epoch 190/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 37497.8242 - rmse: 193.6435\n",
            "Epoch 00190: val_loss did not improve from 476617.56250\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 37497.8242 - rmse: 193.6435 - val_loss: 480050.6562 - val_rmse: 692.8569 - lr: 1.0000e-05\n",
            "Epoch 191/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 39616.2148 - rmse: 199.0382\n",
            "Epoch 00191: val_loss did not improve from 476617.56250\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 39616.2148 - rmse: 199.0382 - val_loss: 491603.0000 - val_rmse: 701.1440 - lr: 1.0000e-05\n",
            "Epoch 192/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 37195.1953 - rmse: 192.8606\n",
            "Epoch 00192: val_loss did not improve from 476617.56250\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 37195.1953 - rmse: 192.8606 - val_loss: 476950.1250 - val_rmse: 690.6157 - lr: 1.0000e-05\n",
            "Epoch 193/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 35880.6211 - rmse: 189.4218\n",
            "Epoch 00193: val_loss improved from 476617.56250 to 471654.65625, saving model to /content/drive/My Drive/Colab Notebooks/applications-of-deep-learning-wustlfall-2021/city/xception_bestmodel.h5\n",
            "250/250 [==============================] - 62s 249ms/step - loss: 35880.6211 - rmse: 189.4218 - val_loss: 471654.6562 - val_rmse: 686.7712 - lr: 1.0000e-05\n",
            "Epoch 194/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 38998.0781 - rmse: 197.4793\n",
            "Epoch 00194: val_loss did not improve from 471654.65625\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 38998.0781 - rmse: 197.4793 - val_loss: 503280.8750 - val_rmse: 709.4229 - lr: 1.0000e-05\n",
            "Epoch 195/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 34165.1953 - rmse: 184.8383\n",
            "Epoch 00195: val_loss did not improve from 471654.65625\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 34165.1953 - rmse: 184.8383 - val_loss: 479187.2812 - val_rmse: 692.2335 - lr: 1.0000e-05\n",
            "Epoch 196/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 38320.9922 - rmse: 195.7575\n",
            "Epoch 00196: val_loss did not improve from 471654.65625\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 38320.9922 - rmse: 195.7575 - val_loss: 489993.5000 - val_rmse: 699.9954 - lr: 1.0000e-05\n",
            "Epoch 197/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 36849.6055 - rmse: 191.9625\n",
            "Epoch 00197: val_loss did not improve from 471654.65625\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 36849.6055 - rmse: 191.9625 - val_loss: 478307.8438 - val_rmse: 691.5980 - lr: 1.0000e-05\n",
            "Epoch 198/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 36197.3633 - rmse: 190.2560\n",
            "Epoch 00198: val_loss did not improve from 471654.65625\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 36197.3633 - rmse: 190.2560 - val_loss: 489775.8750 - val_rmse: 699.8399 - lr: 1.0000e-05\n",
            "Epoch 199/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 37798.1367 - rmse: 194.4174\n",
            "Epoch 00199: val_loss did not improve from 471654.65625\n",
            "250/250 [==============================] - 58s 230ms/step - loss: 37798.1367 - rmse: 194.4174 - val_loss: 480894.3750 - val_rmse: 693.4655 - lr: 1.0000e-05\n",
            "Epoch 200/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 40077.7148 - rmse: 200.1942\n",
            "Epoch 00200: val_loss did not improve from 471654.65625\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 40077.7148 - rmse: 200.1942 - val_loss: 473417.7812 - val_rmse: 688.0536 - lr: 1.0000e-05\n",
            "Epoch 201/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 36594.5625 - rmse: 191.2971\n",
            "Epoch 00201: val_loss improved from 471654.65625 to 466130.93750, saving model to /content/drive/My Drive/Colab Notebooks/applications-of-deep-learning-wustlfall-2021/city/xception_bestmodel.h5\n",
            "250/250 [==============================] - 63s 250ms/step - loss: 36594.5625 - rmse: 191.2971 - val_loss: 466130.9375 - val_rmse: 682.7379 - lr: 1.0000e-05\n",
            "Epoch 202/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 35084.3203 - rmse: 187.3081\n",
            "Epoch 00202: val_loss did not improve from 466130.93750\n",
            "250/250 [==============================] - 58s 233ms/step - loss: 35084.3203 - rmse: 187.3081 - val_loss: 472157.7812 - val_rmse: 687.1374 - lr: 1.0000e-05\n",
            "Epoch 203/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 34371.4727 - rmse: 185.3954\n",
            "Epoch 00203: val_loss did not improve from 466130.93750\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 34371.4727 - rmse: 185.3954 - val_loss: 512174.6562 - val_rmse: 715.6638 - lr: 1.0000e-05\n",
            "Epoch 204/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 32656.7207 - rmse: 180.7117\n",
            "Epoch 00204: val_loss did not improve from 466130.93750\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 32656.7207 - rmse: 180.7117 - val_loss: 488260.5625 - val_rmse: 698.7564 - lr: 1.0000e-05\n",
            "Epoch 205/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 37120.6562 - rmse: 192.6672\n",
            "Epoch 00205: val_loss did not improve from 466130.93750\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 37120.6562 - rmse: 192.6672 - val_loss: 485079.6250 - val_rmse: 696.4766 - lr: 1.0000e-05\n",
            "Epoch 206/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 33146.1719 - rmse: 182.0609\n",
            "Epoch 00206: val_loss did not improve from 466130.93750\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 33146.1719 - rmse: 182.0609 - val_loss: 491937.7188 - val_rmse: 701.3827 - lr: 1.0000e-05\n",
            "Epoch 207/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 36086.3672 - rmse: 189.9641\n",
            "Epoch 00207: val_loss did not improve from 466130.93750\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 36086.3672 - rmse: 189.9641 - val_loss: 525656.8125 - val_rmse: 725.0219 - lr: 1.0000e-05\n",
            "Epoch 208/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 36581.9727 - rmse: 191.2641\n",
            "Epoch 00208: val_loss did not improve from 466130.93750\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 36581.9727 - rmse: 191.2641 - val_loss: 466785.4375 - val_rmse: 683.2170 - lr: 1.0000e-05\n",
            "Epoch 209/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 35946.0117 - rmse: 189.5943\n",
            "Epoch 00209: val_loss did not improve from 466130.93750\n",
            "250/250 [==============================] - 58s 230ms/step - loss: 35946.0117 - rmse: 189.5943 - val_loss: 501256.7812 - val_rmse: 707.9949 - lr: 1.0000e-05\n",
            "Epoch 210/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 32931.0352 - rmse: 181.4691\n",
            "Epoch 00210: val_loss did not improve from 466130.93750\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 32931.0352 - rmse: 181.4691 - val_loss: 473125.1250 - val_rmse: 687.8409 - lr: 1.0000e-05\n",
            "Epoch 211/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 32156.8164 - rmse: 179.3232\n",
            "Epoch 00211: val_loss improved from 466130.93750 to 461804.03125, saving model to /content/drive/My Drive/Colab Notebooks/applications-of-deep-learning-wustlfall-2021/city/xception_bestmodel.h5\n",
            "250/250 [==============================] - 62s 248ms/step - loss: 32156.8164 - rmse: 179.3232 - val_loss: 461804.0312 - val_rmse: 679.5616 - lr: 1.0000e-05\n",
            "Epoch 212/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 34533.4258 - rmse: 185.8317\n",
            "Epoch 00212: val_loss did not improve from 461804.03125\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 34533.4258 - rmse: 185.8317 - val_loss: 488884.0000 - val_rmse: 699.2024 - lr: 1.0000e-05\n",
            "Epoch 213/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 34945.8750 - rmse: 186.9382\n",
            "Epoch 00213: val_loss did not improve from 461804.03125\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 34945.8750 - rmse: 186.9382 - val_loss: 477136.6562 - val_rmse: 690.7508 - lr: 1.0000e-05\n",
            "Epoch 214/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 35766.7617 - rmse: 189.1210\n",
            "Epoch 00214: val_loss did not improve from 461804.03125\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 35766.7617 - rmse: 189.1210 - val_loss: 467084.1562 - val_rmse: 683.4355 - lr: 1.0000e-05\n",
            "Epoch 215/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 35512.4219 - rmse: 188.4474\n",
            "Epoch 00215: val_loss did not improve from 461804.03125\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 35512.4219 - rmse: 188.4474 - val_loss: 468367.4375 - val_rmse: 684.3737 - lr: 1.0000e-05\n",
            "Epoch 216/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 35140.2461 - rmse: 187.4573\n",
            "Epoch 00216: val_loss did not improve from 461804.03125\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 35140.2461 - rmse: 187.4573 - val_loss: 508859.2188 - val_rmse: 713.3437 - lr: 1.0000e-05\n",
            "Epoch 217/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 37113.4805 - rmse: 192.6486\n",
            "Epoch 00217: val_loss did not improve from 461804.03125\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 37113.4805 - rmse: 192.6486 - val_loss: 496648.2188 - val_rmse: 704.7327 - lr: 1.0000e-05\n",
            "Epoch 218/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 30672.6777 - rmse: 175.1362\n",
            "Epoch 00218: val_loss did not improve from 461804.03125\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 30672.6777 - rmse: 175.1362 - val_loss: 467213.6250 - val_rmse: 683.5303 - lr: 1.0000e-05\n",
            "Epoch 219/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 32852.0664 - rmse: 181.2514\n",
            "Epoch 00219: val_loss improved from 461804.03125 to 459806.96875, saving model to /content/drive/My Drive/Colab Notebooks/applications-of-deep-learning-wustlfall-2021/city/xception_bestmodel.h5\n",
            "250/250 [==============================] - 62s 247ms/step - loss: 32852.0664 - rmse: 181.2514 - val_loss: 459806.9688 - val_rmse: 678.0906 - lr: 1.0000e-05\n",
            "Epoch 220/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 32135.4004 - rmse: 179.2635\n",
            "Epoch 00220: val_loss did not improve from 459806.96875\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 32135.4004 - rmse: 179.2635 - val_loss: 489593.4375 - val_rmse: 699.7095 - lr: 1.0000e-05\n",
            "Epoch 221/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 32916.7031 - rmse: 181.4296\n",
            "Epoch 00221: val_loss did not improve from 459806.96875\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 32916.7031 - rmse: 181.4296 - val_loss: 479809.7188 - val_rmse: 692.6830 - lr: 1.0000e-05\n",
            "Epoch 222/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 33125.8477 - rmse: 182.0051\n",
            "Epoch 00222: val_loss did not improve from 459806.96875\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 33125.8477 - rmse: 182.0051 - val_loss: 512164.1562 - val_rmse: 715.6564 - lr: 1.0000e-05\n",
            "Epoch 223/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 28973.5039 - rmse: 170.2160\n",
            "Epoch 00223: val_loss did not improve from 459806.96875\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 28973.5039 - rmse: 170.2160 - val_loss: 470322.5625 - val_rmse: 685.8007 - lr: 1.0000e-05\n",
            "Epoch 224/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 34319.8555 - rmse: 185.2562\n",
            "Epoch 00224: val_loss did not improve from 459806.96875\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 34319.8555 - rmse: 185.2562 - val_loss: 476833.2188 - val_rmse: 690.5311 - lr: 1.0000e-05\n",
            "Epoch 225/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 36787.7422 - rmse: 191.8013\n",
            "Epoch 00225: val_loss did not improve from 459806.96875\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 36787.7422 - rmse: 191.8013 - val_loss: 471576.9375 - val_rmse: 686.7146 - lr: 1.0000e-05\n",
            "Epoch 226/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 33738.7578 - rmse: 183.6811\n",
            "Epoch 00226: val_loss did not improve from 459806.96875\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 33738.7578 - rmse: 183.6811 - val_loss: 469001.5000 - val_rmse: 684.8369 - lr: 1.0000e-05\n",
            "Epoch 227/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 32594.5137 - rmse: 180.5395\n",
            "Epoch 00227: val_loss did not improve from 459806.96875\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 32594.5137 - rmse: 180.5395 - val_loss: 478770.5000 - val_rmse: 691.9324 - lr: 1.0000e-05\n",
            "Epoch 228/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 31148.7363 - rmse: 176.4901\n",
            "Epoch 00228: val_loss did not improve from 459806.96875\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 31148.7363 - rmse: 176.4901 - val_loss: 466752.4375 - val_rmse: 683.1928 - lr: 1.0000e-05\n",
            "Epoch 229/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 34764.5195 - rmse: 186.4525\n",
            "Epoch 00229: val_loss did not improve from 459806.96875\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 34764.5195 - rmse: 186.4525 - val_loss: 466713.3438 - val_rmse: 683.1642 - lr: 1.0000e-05\n",
            "Epoch 230/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 33618.5117 - rmse: 183.3535\n",
            "Epoch 00230: val_loss did not improve from 459806.96875\n",
            "250/250 [==============================] - 58s 230ms/step - loss: 33618.5117 - rmse: 183.3535 - val_loss: 486769.3750 - val_rmse: 697.6886 - lr: 1.0000e-05\n",
            "Epoch 231/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 31964.2539 - rmse: 178.7855\n",
            "Epoch 00231: val_loss did not improve from 459806.96875\n",
            "250/250 [==============================] - 58s 233ms/step - loss: 31964.2539 - rmse: 178.7855 - val_loss: 466390.6562 - val_rmse: 682.9280 - lr: 1.0000e-05\n",
            "Epoch 232/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 31295.4688 - rmse: 176.9053\n",
            "Epoch 00232: val_loss did not improve from 459806.96875\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 31295.4688 - rmse: 176.9053 - val_loss: 464786.0625 - val_rmse: 681.7522 - lr: 1.0000e-05\n",
            "Epoch 233/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 29074.9375 - rmse: 170.5137\n",
            "Epoch 00233: val_loss did not improve from 459806.96875\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 29074.9375 - rmse: 170.5137 - val_loss: 470949.0625 - val_rmse: 686.2573 - lr: 1.0000e-05\n",
            "Epoch 234/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 31101.7168 - rmse: 176.3568\n",
            "Epoch 00234: val_loss improved from 459806.96875 to 452539.62500, saving model to /content/drive/My Drive/Colab Notebooks/applications-of-deep-learning-wustlfall-2021/city/xception_bestmodel.h5\n",
            "250/250 [==============================] - 62s 247ms/step - loss: 31101.7168 - rmse: 176.3568 - val_loss: 452539.6250 - val_rmse: 672.7106 - lr: 1.0000e-05\n",
            "Epoch 235/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 31397.0645 - rmse: 177.1922\n",
            "Epoch 00235: val_loss did not improve from 452539.62500\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 31397.0645 - rmse: 177.1922 - val_loss: 472112.6250 - val_rmse: 687.1045 - lr: 1.0000e-05\n",
            "Epoch 236/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 30497.8164 - rmse: 174.6362\n",
            "Epoch 00236: val_loss did not improve from 452539.62500\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 30497.8164 - rmse: 174.6362 - val_loss: 458297.2188 - val_rmse: 676.9766 - lr: 1.0000e-05\n",
            "Epoch 237/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 30652.9453 - rmse: 175.0798\n",
            "Epoch 00237: val_loss did not improve from 452539.62500\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 30652.9453 - rmse: 175.0798 - val_loss: 473975.8438 - val_rmse: 688.4590 - lr: 1.0000e-05\n",
            "Epoch 238/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 31080.7695 - rmse: 176.2974\n",
            "Epoch 00238: val_loss did not improve from 452539.62500\n",
            "250/250 [==============================] - 58s 230ms/step - loss: 31080.7695 - rmse: 176.2974 - val_loss: 459519.7812 - val_rmse: 677.8788 - lr: 1.0000e-05\n",
            "Epoch 239/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 30380.9141 - rmse: 174.3012\n",
            "Epoch 00239: val_loss did not improve from 452539.62500\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 30380.9141 - rmse: 174.3012 - val_loss: 463725.2500 - val_rmse: 680.9738 - lr: 1.0000e-05\n",
            "Epoch 240/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 31449.3281 - rmse: 177.3396\n",
            "Epoch 00240: val_loss did not improve from 452539.62500\n",
            "250/250 [==============================] - 58s 230ms/step - loss: 31449.3281 - rmse: 177.3396 - val_loss: 460212.1562 - val_rmse: 678.3894 - lr: 1.0000e-05\n",
            "Epoch 241/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 30226.2188 - rmse: 173.8569\n",
            "Epoch 00241: val_loss did not improve from 452539.62500\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 30226.2188 - rmse: 173.8569 - val_loss: 459062.1250 - val_rmse: 677.5412 - lr: 1.0000e-05\n",
            "Epoch 242/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 33614.5625 - rmse: 183.3427\n",
            "Epoch 00242: val_loss did not improve from 452539.62500\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 33614.5625 - rmse: 183.3427 - val_loss: 456047.1875 - val_rmse: 675.3126 - lr: 1.0000e-05\n",
            "Epoch 243/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 31270.2578 - rmse: 176.8340\n",
            "Epoch 00243: val_loss did not improve from 452539.62500\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 31270.2578 - rmse: 176.8340 - val_loss: 460493.7812 - val_rmse: 678.5969 - lr: 1.0000e-05\n",
            "Epoch 244/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 30215.5566 - rmse: 173.8262\n",
            "Epoch 00244: val_loss did not improve from 452539.62500\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 30215.5566 - rmse: 173.8262 - val_loss: 464092.5312 - val_rmse: 681.2433 - lr: 1.0000e-05\n",
            "Epoch 245/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 26598.7520 - rmse: 163.0912\n",
            "Epoch 00245: val_loss did not improve from 452539.62500\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 26598.7520 - rmse: 163.0912 - val_loss: 459841.6875 - val_rmse: 678.1163 - lr: 1.0000e-05\n",
            "Epoch 246/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 31939.4258 - rmse: 178.7160\n",
            "Epoch 00246: val_loss did not improve from 452539.62500\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 31939.4258 - rmse: 178.7160 - val_loss: 464774.0938 - val_rmse: 681.7434 - lr: 1.0000e-05\n",
            "Epoch 247/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 29427.0703 - rmse: 171.5432\n",
            "Epoch 00247: val_loss did not improve from 452539.62500\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 29427.0703 - rmse: 171.5432 - val_loss: 460731.5312 - val_rmse: 678.7721 - lr: 1.0000e-05\n",
            "Epoch 248/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 31516.8477 - rmse: 177.5298\n",
            "Epoch 00248: val_loss did not improve from 452539.62500\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 31516.8477 - rmse: 177.5298 - val_loss: 466344.4375 - val_rmse: 682.8942 - lr: 1.0000e-05\n",
            "Epoch 249/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 27309.5605 - rmse: 165.2560\n",
            "Epoch 00249: val_loss did not improve from 452539.62500\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 27309.5605 - rmse: 165.2560 - val_loss: 499666.1562 - val_rmse: 706.8707 - lr: 1.0000e-05\n",
            "Epoch 250/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 29758.8027 - rmse: 172.5074\n",
            "Epoch 00250: val_loss did not improve from 452539.62500\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 29758.8027 - rmse: 172.5074 - val_loss: 460607.5938 - val_rmse: 678.6808 - lr: 1.0000e-05\n",
            "Epoch 251/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 29208.0566 - rmse: 170.9036\n",
            "Epoch 00251: val_loss did not improve from 452539.62500\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 29208.0566 - rmse: 170.9036 - val_loss: 457444.3750 - val_rmse: 676.3464 - lr: 1.0000e-05\n",
            "Epoch 252/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 29706.7617 - rmse: 172.3565\n",
            "Epoch 00252: val_loss did not improve from 452539.62500\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 29706.7617 - rmse: 172.3565 - val_loss: 484215.7188 - val_rmse: 695.8561 - lr: 1.0000e-05\n",
            "Epoch 253/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 32482.2246 - rmse: 180.2283\n",
            "Epoch 00253: val_loss did not improve from 452539.62500\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 32482.2246 - rmse: 180.2283 - val_loss: 454651.2812 - val_rmse: 674.2783 - lr: 1.0000e-05\n",
            "Epoch 254/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 32083.7578 - rmse: 179.1194\n",
            "Epoch 00254: val_loss did not improve from 452539.62500\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 32083.7578 - rmse: 179.1194 - val_loss: 464806.8750 - val_rmse: 681.7675 - lr: 1.0000e-05\n",
            "Epoch 255/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 30137.2637 - rmse: 173.6009\n",
            "Epoch 00255: val_loss did not improve from 452539.62500\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 30137.2637 - rmse: 173.6009 - val_loss: 489230.8438 - val_rmse: 699.4504 - lr: 1.0000e-05\n",
            "Epoch 256/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 30711.9902 - rmse: 175.2484\n",
            "Epoch 00256: val_loss did not improve from 452539.62500\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 30711.9902 - rmse: 175.2484 - val_loss: 471742.6562 - val_rmse: 686.8352 - lr: 1.0000e-05\n",
            "Epoch 257/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 30859.0879 - rmse: 175.6675\n",
            "Epoch 00257: val_loss did not improve from 452539.62500\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 30859.0879 - rmse: 175.6675 - val_loss: 477145.1562 - val_rmse: 690.7569 - lr: 1.0000e-05\n",
            "Epoch 258/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 28919.7246 - rmse: 170.0580\n",
            "Epoch 00258: val_loss did not improve from 452539.62500\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 28919.7246 - rmse: 170.0580 - val_loss: 484456.3750 - val_rmse: 696.0290 - lr: 1.0000e-05\n",
            "Epoch 259/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 30349.3652 - rmse: 174.2107\n",
            "Epoch 00259: val_loss did not improve from 452539.62500\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 30349.3652 - rmse: 174.2107 - val_loss: 460370.4688 - val_rmse: 678.5060 - lr: 1.0000e-05\n",
            "Epoch 260/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 25541.2422 - rmse: 159.8163\n",
            "Epoch 00260: val_loss did not improve from 452539.62500\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 25541.2422 - rmse: 159.8163 - val_loss: 469325.6562 - val_rmse: 685.0735 - lr: 1.0000e-05\n",
            "Epoch 261/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 27391.1914 - rmse: 165.5028\n",
            "Epoch 00261: val_loss did not improve from 452539.62500\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 27391.1914 - rmse: 165.5028 - val_loss: 456540.1250 - val_rmse: 675.6776 - lr: 1.0000e-05\n",
            "Epoch 262/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 28791.4941 - rmse: 169.6806\n",
            "Epoch 00262: val_loss did not improve from 452539.62500\n",
            "250/250 [==============================] - 58s 230ms/step - loss: 28791.4941 - rmse: 169.6806 - val_loss: 462744.7188 - val_rmse: 680.2534 - lr: 1.0000e-05\n",
            "Epoch 263/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 27368.0781 - rmse: 165.4330\n",
            "Epoch 00263: val_loss did not improve from 452539.62500\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 27368.0781 - rmse: 165.4330 - val_loss: 460222.8750 - val_rmse: 678.3973 - lr: 1.0000e-05\n",
            "Epoch 264/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 30383.9004 - rmse: 174.3098\n",
            "Epoch 00264: val_loss did not improve from 452539.62500\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 30383.9004 - rmse: 174.3098 - val_loss: 463610.4688 - val_rmse: 680.8895 - lr: 1.0000e-05\n",
            "Epoch 265/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 28546.6777 - rmse: 168.9576\n",
            "Epoch 00265: val_loss did not improve from 452539.62500\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 28546.6777 - rmse: 168.9576 - val_loss: 460985.0000 - val_rmse: 678.9587 - lr: 1.0000e-05\n",
            "Epoch 266/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 27387.1328 - rmse: 165.4906\n",
            "Epoch 00266: val_loss did not improve from 452539.62500\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 27387.1328 - rmse: 165.4906 - val_loss: 463504.2812 - val_rmse: 680.8115 - lr: 1.0000e-05\n",
            "Epoch 267/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 27000.5508 - rmse: 164.3184\n",
            "Epoch 00267: val_loss did not improve from 452539.62500\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 27000.5508 - rmse: 164.3184 - val_loss: 459902.5625 - val_rmse: 678.1611 - lr: 1.0000e-05\n",
            "Epoch 268/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 25138.5938 - rmse: 158.5515\n",
            "Epoch 00268: val_loss did not improve from 452539.62500\n",
            "250/250 [==============================] - 58s 233ms/step - loss: 25138.5938 - rmse: 158.5515 - val_loss: 472955.5000 - val_rmse: 687.7176 - lr: 1.0000e-05\n",
            "Epoch 269/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 27502.3906 - rmse: 165.8384\n",
            "Epoch 00269: val_loss improved from 452539.62500 to 450279.65625, saving model to /content/drive/My Drive/Colab Notebooks/applications-of-deep-learning-wustlfall-2021/city/xception_bestmodel.h5\n",
            "250/250 [==============================] - 62s 248ms/step - loss: 27502.3906 - rmse: 165.8384 - val_loss: 450279.6562 - val_rmse: 671.0288 - lr: 1.0000e-05\n",
            "Epoch 270/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 28622.1289 - rmse: 169.1808\n",
            "Epoch 00270: val_loss did not improve from 450279.65625\n",
            "250/250 [==============================] - 59s 233ms/step - loss: 28622.1289 - rmse: 169.1808 - val_loss: 464443.4688 - val_rmse: 681.5009 - lr: 1.0000e-05\n",
            "Epoch 271/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 26637.2832 - rmse: 163.2093\n",
            "Epoch 00271: val_loss did not improve from 450279.65625\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 26637.2832 - rmse: 163.2093 - val_loss: 455633.7812 - val_rmse: 675.0065 - lr: 1.0000e-05\n",
            "Epoch 272/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 24853.4336 - rmse: 157.6497\n",
            "Epoch 00272: val_loss did not improve from 450279.65625\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 24853.4336 - rmse: 157.6497 - val_loss: 454386.9375 - val_rmse: 674.0823 - lr: 1.0000e-05\n",
            "Epoch 273/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 26150.6777 - rmse: 161.7117\n",
            "Epoch 00273: val_loss did not improve from 450279.65625\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 26150.6777 - rmse: 161.7117 - val_loss: 462757.3750 - val_rmse: 680.2627 - lr: 1.0000e-05\n",
            "Epoch 274/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 26077.2617 - rmse: 161.4846\n",
            "Epoch 00274: val_loss did not improve from 450279.65625\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 26077.2617 - rmse: 161.4846 - val_loss: 459560.8750 - val_rmse: 677.9092 - lr: 1.0000e-05\n",
            "Epoch 275/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 26796.0703 - rmse: 163.6951\n",
            "Epoch 00275: val_loss improved from 450279.65625 to 448403.90625, saving model to /content/drive/My Drive/Colab Notebooks/applications-of-deep-learning-wustlfall-2021/city/xception_bestmodel.h5\n",
            "250/250 [==============================] - 63s 251ms/step - loss: 26796.0703 - rmse: 163.6951 - val_loss: 448403.9062 - val_rmse: 669.6297 - lr: 1.0000e-05\n",
            "Epoch 276/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 26733.4648 - rmse: 163.5037\n",
            "Epoch 00276: val_loss did not improve from 448403.90625\n",
            "250/250 [==============================] - 58s 233ms/step - loss: 26733.4648 - rmse: 163.5037 - val_loss: 471248.6562 - val_rmse: 686.4755 - lr: 1.0000e-05\n",
            "Epoch 277/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 27861.9336 - rmse: 166.9189\n",
            "Epoch 00277: val_loss did not improve from 448403.90625\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 27861.9336 - rmse: 166.9189 - val_loss: 463662.2188 - val_rmse: 680.9274 - lr: 1.0000e-05\n",
            "Epoch 278/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 26782.4297 - rmse: 163.6534\n",
            "Epoch 00278: val_loss did not improve from 448403.90625\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 26782.4297 - rmse: 163.6534 - val_loss: 451727.0625 - val_rmse: 672.1064 - lr: 1.0000e-05\n",
            "Epoch 279/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 25452.8066 - rmse: 159.5394\n",
            "Epoch 00279: val_loss did not improve from 448403.90625\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 25452.8066 - rmse: 159.5394 - val_loss: 450891.3750 - val_rmse: 671.4844 - lr: 1.0000e-05\n",
            "Epoch 280/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 27368.3105 - rmse: 165.4337\n",
            "Epoch 00280: val_loss did not improve from 448403.90625\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 27368.3105 - rmse: 165.4337 - val_loss: 491266.8750 - val_rmse: 700.9043 - lr: 1.0000e-05\n",
            "Epoch 281/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 27312.4473 - rmse: 165.2648\n",
            "Epoch 00281: val_loss did not improve from 448403.90625\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 27312.4473 - rmse: 165.2648 - val_loss: 466972.0000 - val_rmse: 683.3535 - lr: 1.0000e-05\n",
            "Epoch 282/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 25314.3789 - rmse: 159.1049\n",
            "Epoch 00282: val_loss did not improve from 448403.90625\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 25314.3789 - rmse: 159.1049 - val_loss: 448501.0625 - val_rmse: 669.7022 - lr: 1.0000e-05\n",
            "Epoch 283/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 25685.4902 - rmse: 160.2669\n",
            "Epoch 00283: val_loss did not improve from 448403.90625\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 25685.4902 - rmse: 160.2669 - val_loss: 488184.2188 - val_rmse: 698.7018 - lr: 1.0000e-05\n",
            "Epoch 284/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 24377.5293 - rmse: 156.1330\n",
            "Epoch 00284: val_loss did not improve from 448403.90625\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 24377.5293 - rmse: 156.1330 - val_loss: 450845.2812 - val_rmse: 671.4501 - lr: 1.0000e-05\n",
            "Epoch 285/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 27353.2051 - rmse: 165.3880\n",
            "Epoch 00285: val_loss did not improve from 448403.90625\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 27353.2051 - rmse: 165.3880 - val_loss: 450264.2500 - val_rmse: 671.0173 - lr: 1.0000e-05\n",
            "Epoch 286/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 26517.1641 - rmse: 162.8409\n",
            "Epoch 00286: val_loss did not improve from 448403.90625\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 26517.1641 - rmse: 162.8409 - val_loss: 451262.1875 - val_rmse: 671.7605 - lr: 1.0000e-05\n",
            "Epoch 287/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 23371.2930 - rmse: 152.8767\n",
            "Epoch 00287: val_loss did not improve from 448403.90625\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 23371.2930 - rmse: 152.8767 - val_loss: 452083.5625 - val_rmse: 672.3716 - lr: 1.0000e-05\n",
            "Epoch 288/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 30675.3418 - rmse: 175.1438\n",
            "Epoch 00288: val_loss did not improve from 448403.90625\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 30675.3418 - rmse: 175.1438 - val_loss: 454981.1562 - val_rmse: 674.5229 - lr: 1.0000e-05\n",
            "Epoch 289/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 24375.6855 - rmse: 156.1271\n",
            "Epoch 00289: val_loss did not improve from 448403.90625\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 24375.6855 - rmse: 156.1271 - val_loss: 463397.1250 - val_rmse: 680.7328 - lr: 1.0000e-05\n",
            "Epoch 290/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 24802.9785 - rmse: 157.4896\n",
            "Epoch 00290: val_loss improved from 448403.90625 to 444844.90625, saving model to /content/drive/My Drive/Colab Notebooks/applications-of-deep-learning-wustlfall-2021/city/xception_bestmodel.h5\n",
            "250/250 [==============================] - 63s 251ms/step - loss: 24802.9785 - rmse: 157.4896 - val_loss: 444844.9062 - val_rmse: 666.9669 - lr: 1.0000e-05\n",
            "Epoch 291/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 25847.5684 - rmse: 160.7718\n",
            "Epoch 00291: val_loss did not improve from 444844.90625\n",
            "250/250 [==============================] - 58s 233ms/step - loss: 25847.5684 - rmse: 160.7718 - val_loss: 479936.9375 - val_rmse: 692.7748 - lr: 1.0000e-05\n",
            "Epoch 292/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 23434.6484 - rmse: 153.0838\n",
            "Epoch 00292: val_loss did not improve from 444844.90625\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 23434.6484 - rmse: 153.0838 - val_loss: 468766.9375 - val_rmse: 684.6655 - lr: 1.0000e-05\n",
            "Epoch 293/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 24804.6992 - rmse: 157.4951\n",
            "Epoch 00293: val_loss did not improve from 444844.90625\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 24804.6992 - rmse: 157.4951 - val_loss: 460132.1562 - val_rmse: 678.3304 - lr: 1.0000e-05\n",
            "Epoch 294/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 24798.5938 - rmse: 157.4757\n",
            "Epoch 00294: val_loss did not improve from 444844.90625\n",
            "250/250 [==============================] - 58s 233ms/step - loss: 24798.5938 - rmse: 157.4757 - val_loss: 454410.1562 - val_rmse: 674.0995 - lr: 1.0000e-05\n",
            "Epoch 295/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 28951.8320 - rmse: 170.1524\n",
            "Epoch 00295: val_loss did not improve from 444844.90625\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 28951.8320 - rmse: 170.1524 - val_loss: 476771.5625 - val_rmse: 690.4865 - lr: 1.0000e-05\n",
            "Epoch 296/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 26806.4023 - rmse: 163.7266\n",
            "Epoch 00296: val_loss did not improve from 444844.90625\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 26806.4023 - rmse: 163.7266 - val_loss: 474411.6562 - val_rmse: 688.7755 - lr: 1.0000e-05\n",
            "Epoch 297/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 25873.5293 - rmse: 160.8525\n",
            "Epoch 00297: val_loss did not improve from 444844.90625\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 25873.5293 - rmse: 160.8525 - val_loss: 447116.6250 - val_rmse: 668.6678 - lr: 1.0000e-05\n",
            "Epoch 298/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 24651.1523 - rmse: 157.0069\n",
            "Epoch 00298: val_loss did not improve from 444844.90625\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 24651.1523 - rmse: 157.0069 - val_loss: 449697.0312 - val_rmse: 670.5945 - lr: 1.0000e-05\n",
            "Epoch 299/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 24276.9453 - rmse: 155.8106\n",
            "Epoch 00299: val_loss did not improve from 444844.90625\n",
            "250/250 [==============================] - 58s 233ms/step - loss: 24276.9453 - rmse: 155.8106 - val_loss: 480741.2812 - val_rmse: 693.3551 - lr: 1.0000e-05\n",
            "Epoch 300/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 28902.9531 - rmse: 170.0087\n",
            "Epoch 00300: val_loss improved from 444844.90625 to 442959.93750, saving model to /content/drive/My Drive/Colab Notebooks/applications-of-deep-learning-wustlfall-2021/city/xception_bestmodel.h5\n",
            "250/250 [==============================] - 63s 252ms/step - loss: 28902.9531 - rmse: 170.0087 - val_loss: 442959.9375 - val_rmse: 665.5524 - lr: 1.0000e-05\n",
            "Epoch 301/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 23876.4316 - rmse: 154.5200\n",
            "Epoch 00301: val_loss did not improve from 442959.93750\n",
            "250/250 [==============================] - 58s 233ms/step - loss: 23876.4316 - rmse: 154.5200 - val_loss: 446728.6562 - val_rmse: 668.3776 - lr: 1.0000e-05\n",
            "Epoch 302/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 21999.4668 - rmse: 148.3222\n",
            "Epoch 00302: val_loss did not improve from 442959.93750\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 21999.4668 - rmse: 148.3222 - val_loss: 443816.5312 - val_rmse: 666.1956 - lr: 1.0000e-05\n",
            "Epoch 303/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 22922.4453 - rmse: 151.4016\n",
            "Epoch 00303: val_loss did not improve from 442959.93750\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 22922.4453 - rmse: 151.4016 - val_loss: 443382.8438 - val_rmse: 665.8700 - lr: 1.0000e-05\n",
            "Epoch 304/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 24138.2480 - rmse: 155.3649\n",
            "Epoch 00304: val_loss did not improve from 442959.93750\n",
            "250/250 [==============================] - 58s 233ms/step - loss: 24138.2480 - rmse: 155.3649 - val_loss: 453434.0312 - val_rmse: 673.3751 - lr: 1.0000e-05\n",
            "Epoch 305/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 26960.1270 - rmse: 164.1954\n",
            "Epoch 00305: val_loss did not improve from 442959.93750\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 26960.1270 - rmse: 164.1954 - val_loss: 511269.7188 - val_rmse: 715.0312 - lr: 1.0000e-05\n",
            "Epoch 306/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 25458.0508 - rmse: 159.5558\n",
            "Epoch 00306: val_loss improved from 442959.93750 to 442938.84375, saving model to /content/drive/My Drive/Colab Notebooks/applications-of-deep-learning-wustlfall-2021/city/xception_bestmodel.h5\n",
            "250/250 [==============================] - 62s 249ms/step - loss: 25458.0508 - rmse: 159.5558 - val_loss: 442938.8438 - val_rmse: 665.5365 - lr: 1.0000e-05\n",
            "Epoch 307/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 24613.6523 - rmse: 156.8874\n",
            "Epoch 00307: val_loss improved from 442938.84375 to 441871.71875, saving model to /content/drive/My Drive/Colab Notebooks/applications-of-deep-learning-wustlfall-2021/city/xception_bestmodel.h5\n",
            "250/250 [==============================] - 63s 253ms/step - loss: 24613.6523 - rmse: 156.8874 - val_loss: 441871.7188 - val_rmse: 664.7343 - lr: 1.0000e-05\n",
            "Epoch 308/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 23917.7168 - rmse: 154.6535\n",
            "Epoch 00308: val_loss did not improve from 441871.71875\n",
            "250/250 [==============================] - 58s 233ms/step - loss: 23917.7168 - rmse: 154.6535 - val_loss: 454534.9375 - val_rmse: 674.1921 - lr: 1.0000e-05\n",
            "Epoch 309/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 25098.8535 - rmse: 158.4262\n",
            "Epoch 00309: val_loss did not improve from 441871.71875\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 25098.8535 - rmse: 158.4262 - val_loss: 443942.6250 - val_rmse: 666.2902 - lr: 1.0000e-05\n",
            "Epoch 310/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 24148.3926 - rmse: 155.3975\n",
            "Epoch 00310: val_loss improved from 441871.71875 to 436066.25000, saving model to /content/drive/My Drive/Colab Notebooks/applications-of-deep-learning-wustlfall-2021/city/xception_bestmodel.h5\n",
            "250/250 [==============================] - 63s 250ms/step - loss: 24148.3926 - rmse: 155.3975 - val_loss: 436066.2500 - val_rmse: 660.3531 - lr: 1.0000e-05\n",
            "Epoch 311/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 21453.8730 - rmse: 146.4714\n",
            "Epoch 00311: val_loss did not improve from 436066.25000\n",
            "250/250 [==============================] - 58s 233ms/step - loss: 21453.8730 - rmse: 146.4714 - val_loss: 457509.2500 - val_rmse: 676.3943 - lr: 1.0000e-05\n",
            "Epoch 312/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 23218.5000 - rmse: 152.3762\n",
            "Epoch 00312: val_loss did not improve from 436066.25000\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 23218.5000 - rmse: 152.3762 - val_loss: 449873.6250 - val_rmse: 670.7262 - lr: 1.0000e-05\n",
            "Epoch 313/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 24822.1309 - rmse: 157.5504\n",
            "Epoch 00313: val_loss did not improve from 436066.25000\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 24822.1309 - rmse: 157.5504 - val_loss: 439980.1562 - val_rmse: 663.3100 - lr: 1.0000e-05\n",
            "Epoch 314/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 23853.5879 - rmse: 154.4461\n",
            "Epoch 00314: val_loss did not improve from 436066.25000\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 23853.5879 - rmse: 154.4461 - val_loss: 440710.2188 - val_rmse: 663.8601 - lr: 1.0000e-05\n",
            "Epoch 315/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 24859.1719 - rmse: 157.6679\n",
            "Epoch 00315: val_loss did not improve from 436066.25000\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 24859.1719 - rmse: 157.6679 - val_loss: 449129.4062 - val_rmse: 670.1712 - lr: 1.0000e-05\n",
            "Epoch 316/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 24787.2383 - rmse: 157.4396\n",
            "Epoch 00316: val_loss did not improve from 436066.25000\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 24787.2383 - rmse: 157.4396 - val_loss: 447203.2188 - val_rmse: 668.7325 - lr: 1.0000e-05\n",
            "Epoch 317/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 24182.0859 - rmse: 155.5059\n",
            "Epoch 00317: val_loss did not improve from 436066.25000\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 24182.0859 - rmse: 155.5059 - val_loss: 443246.6562 - val_rmse: 665.7677 - lr: 1.0000e-05\n",
            "Epoch 318/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 23782.0645 - rmse: 154.2143\n",
            "Epoch 00318: val_loss did not improve from 436066.25000\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 23782.0645 - rmse: 154.2143 - val_loss: 452965.3750 - val_rmse: 673.0270 - lr: 1.0000e-05\n",
            "Epoch 319/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 22403.2324 - rmse: 149.6771\n",
            "Epoch 00319: val_loss did not improve from 436066.25000\n",
            "250/250 [==============================] - 68s 271ms/step - loss: 22403.2324 - rmse: 149.6771 - val_loss: 445187.7500 - val_rmse: 667.2239 - lr: 1.0000e-05\n",
            "Epoch 320/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 20738.7188 - rmse: 144.0094\n",
            "Epoch 00320: val_loss did not improve from 436066.25000\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 20738.7188 - rmse: 144.0094 - val_loss: 442807.5938 - val_rmse: 665.4379 - lr: 1.0000e-05\n",
            "Epoch 321/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 22942.3477 - rmse: 151.4673\n",
            "Epoch 00321: val_loss did not improve from 436066.25000\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 22942.3477 - rmse: 151.4673 - val_loss: 462120.6250 - val_rmse: 679.7946 - lr: 1.0000e-05\n",
            "Epoch 322/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 26406.6270 - rmse: 162.5012\n",
            "Epoch 00322: val_loss did not improve from 436066.25000\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 26406.6270 - rmse: 162.5012 - val_loss: 449571.0312 - val_rmse: 670.5006 - lr: 1.0000e-05\n",
            "Epoch 323/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 22562.8516 - rmse: 150.2094\n",
            "Epoch 00323: val_loss did not improve from 436066.25000\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 22562.8516 - rmse: 150.2094 - val_loss: 457666.0000 - val_rmse: 676.5102 - lr: 1.0000e-05\n",
            "Epoch 324/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 21822.0195 - rmse: 147.7228\n",
            "Epoch 00324: val_loss did not improve from 436066.25000\n",
            "250/250 [==============================] - 58s 233ms/step - loss: 21822.0195 - rmse: 147.7228 - val_loss: 466036.5000 - val_rmse: 682.6686 - lr: 1.0000e-05\n",
            "Epoch 325/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 22063.5176 - rmse: 148.5379\n",
            "Epoch 00325: val_loss did not improve from 436066.25000\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 22063.5176 - rmse: 148.5379 - val_loss: 451847.2812 - val_rmse: 672.1959 - lr: 1.0000e-05\n",
            "Epoch 326/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 23538.8535 - rmse: 153.4238\n",
            "Epoch 00326: val_loss did not improve from 436066.25000\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 23538.8535 - rmse: 153.4238 - val_loss: 440201.3125 - val_rmse: 663.4767 - lr: 1.0000e-05\n",
            "Epoch 327/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 23090.1230 - rmse: 151.9543\n",
            "Epoch 00327: val_loss did not improve from 436066.25000\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 23090.1230 - rmse: 151.9543 - val_loss: 466548.4375 - val_rmse: 683.0435 - lr: 1.0000e-05\n",
            "Epoch 328/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 24843.9551 - rmse: 157.6196\n",
            "Epoch 00328: val_loss did not improve from 436066.25000\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 24843.9551 - rmse: 157.6196 - val_loss: 513912.7812 - val_rmse: 716.8771 - lr: 1.0000e-05\n",
            "Epoch 329/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 23297.8047 - rmse: 152.6362\n",
            "Epoch 00329: val_loss did not improve from 436066.25000\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 23297.8047 - rmse: 152.6362 - val_loss: 440691.4062 - val_rmse: 663.8459 - lr: 1.0000e-05\n",
            "Epoch 330/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 21717.9023 - rmse: 147.3699\n",
            "Epoch 00330: val_loss did not improve from 436066.25000\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 21717.9023 - rmse: 147.3699 - val_loss: 471985.3438 - val_rmse: 687.0119 - lr: 1.0000e-05\n",
            "Epoch 331/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 23134.4648 - rmse: 152.1002\n",
            "Epoch 00331: val_loss did not improve from 436066.25000\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 23134.4648 - rmse: 152.1002 - val_loss: 493877.5000 - val_rmse: 702.7642 - lr: 1.0000e-05\n",
            "Epoch 332/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 23111.9551 - rmse: 152.0262\n",
            "Epoch 00332: val_loss did not improve from 436066.25000\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 23111.9551 - rmse: 152.0262 - val_loss: 460498.1250 - val_rmse: 678.6001 - lr: 1.0000e-05\n",
            "Epoch 333/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 20903.7617 - rmse: 144.5813\n",
            "Epoch 00333: val_loss improved from 436066.25000 to 435524.00000, saving model to /content/drive/My Drive/Colab Notebooks/applications-of-deep-learning-wustlfall-2021/city/xception_bestmodel.h5\n",
            "250/250 [==============================] - 62s 246ms/step - loss: 20903.7617 - rmse: 144.5813 - val_loss: 435524.0000 - val_rmse: 659.9424 - lr: 1.0000e-05\n",
            "Epoch 334/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 21774.4473 - rmse: 147.5617\n",
            "Epoch 00334: val_loss did not improve from 435524.00000\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 21774.4473 - rmse: 147.5617 - val_loss: 448022.4062 - val_rmse: 669.3447 - lr: 1.0000e-05\n",
            "Epoch 335/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 19453.3594 - rmse: 139.4753\n",
            "Epoch 00335: val_loss did not improve from 435524.00000\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 19453.3594 - rmse: 139.4753 - val_loss: 440842.6562 - val_rmse: 663.9598 - lr: 1.0000e-05\n",
            "Epoch 336/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 23911.4629 - rmse: 154.6333\n",
            "Epoch 00336: val_loss did not improve from 435524.00000\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 23911.4629 - rmse: 154.6333 - val_loss: 458723.8750 - val_rmse: 677.2916 - lr: 1.0000e-05\n",
            "Epoch 337/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 20651.2559 - rmse: 143.7054\n",
            "Epoch 00337: val_loss did not improve from 435524.00000\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 20651.2559 - rmse: 143.7054 - val_loss: 438752.0938 - val_rmse: 662.3836 - lr: 1.0000e-05\n",
            "Epoch 338/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 20976.9727 - rmse: 144.8343\n",
            "Epoch 00338: val_loss did not improve from 435524.00000\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 20976.9727 - rmse: 144.8343 - val_loss: 453535.4062 - val_rmse: 673.4504 - lr: 1.0000e-05\n",
            "Epoch 339/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 21821.5957 - rmse: 147.7213\n",
            "Epoch 00339: val_loss did not improve from 435524.00000\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 21821.5957 - rmse: 147.7213 - val_loss: 477842.3750 - val_rmse: 691.2614 - lr: 1.0000e-05\n",
            "Epoch 340/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 21901.0547 - rmse: 147.9901\n",
            "Epoch 00340: val_loss improved from 435524.00000 to 432583.93750, saving model to /content/drive/My Drive/Colab Notebooks/applications-of-deep-learning-wustlfall-2021/city/xception_bestmodel.h5\n",
            "250/250 [==============================] - 62s 246ms/step - loss: 21901.0547 - rmse: 147.9901 - val_loss: 432583.9375 - val_rmse: 657.7111 - lr: 1.0000e-05\n",
            "Epoch 341/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 19680.0332 - rmse: 140.2855\n",
            "Epoch 00341: val_loss did not improve from 432583.93750\n",
            "250/250 [==============================] - 59s 232ms/step - loss: 19680.0332 - rmse: 140.2855 - val_loss: 438089.0625 - val_rmse: 661.8830 - lr: 1.0000e-05\n",
            "Epoch 342/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 21369.4453 - rmse: 146.1829\n",
            "Epoch 00342: val_loss did not improve from 432583.93750\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 21369.4453 - rmse: 146.1829 - val_loss: 434190.2812 - val_rmse: 658.9312 - lr: 1.0000e-05\n",
            "Epoch 343/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 23708.7793 - rmse: 153.9765\n",
            "Epoch 00343: val_loss did not improve from 432583.93750\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 23708.7793 - rmse: 153.9765 - val_loss: 461292.0938 - val_rmse: 679.1849 - lr: 1.0000e-05\n",
            "Epoch 344/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 23251.6426 - rmse: 152.4849\n",
            "Epoch 00344: val_loss did not improve from 432583.93750\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 23251.6426 - rmse: 152.4849 - val_loss: 449938.1250 - val_rmse: 670.7743 - lr: 1.0000e-05\n",
            "Epoch 345/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 20004.5566 - rmse: 141.4375\n",
            "Epoch 00345: val_loss did not improve from 432583.93750\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 20004.5566 - rmse: 141.4375 - val_loss: 450870.0938 - val_rmse: 671.4686 - lr: 1.0000e-05\n",
            "Epoch 346/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 20605.2910 - rmse: 143.5454\n",
            "Epoch 00346: val_loss did not improve from 432583.93750\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 20605.2910 - rmse: 143.5454 - val_loss: 454312.1250 - val_rmse: 674.0268 - lr: 1.0000e-05\n",
            "Epoch 347/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 20336.9375 - rmse: 142.6076\n",
            "Epoch 00347: val_loss did not improve from 432583.93750\n",
            "250/250 [==============================] - 58s 232ms/step - loss: 20336.9375 - rmse: 142.6076 - val_loss: 444995.1562 - val_rmse: 667.0796 - lr: 1.0000e-05\n",
            "Epoch 348/500\n",
            "250/250 [==============================] - ETA: 0s - loss: 21877.3223 - rmse: 147.9098\n",
            "Epoch 00348: val_loss did not improve from 432583.93750\n",
            "250/250 [==============================] - 58s 230ms/step - loss: 21877.3223 - rmse: 147.9098 - val_loss: 437485.2500 - val_rmse: 661.4267 - lr: 1.0000e-05\n",
            "Epoch 349/500\n",
            "135/250 [===============>..............] - ETA: 21s - loss: 22561.6250 - rmse: 150.2053"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZKqdPztuLYZ"
      },
      "source": [
        "### Build ensemble model: improving prediction accuracy\n",
        "\n",
        "- resnet\n",
        "- xception\n",
        "- vgg"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UZouBmt0dH_"
      },
      "source": [
        "WIDTH = 150\n",
        "HEIGHT = 150\n",
        "\n",
        "def build_resnet():\n",
        "  input_tensor = Input(shape=(HEIGHT, WIDTH, 3))\n",
        "  base_model = ResNet152V2(\n",
        "      include_top=False, weights='imagenet', input_tensor=input_tensor,\n",
        "      classifier_activation='relu'\n",
        "  )\n",
        "\n",
        "  x=base_model.output\n",
        "  x=GlobalAveragePooling2D()(x)\n",
        "\n",
        "  x = Dense(1024,activation='relu')(x)\n",
        "  x = Dense(1024,activation='relu')(x)\n",
        "\n",
        "  output=Dense(1,activation='linear')(x)\n",
        "  model_resnet=Model(inputs=input_tensor,outputs=output)\n",
        "  return model_resnet\n",
        "\n",
        "def build_vgg():\n",
        "  input_tensor = Input(shape=(HEIGHT, WIDTH, 3))\n",
        "  base_model = VGG16(\n",
        "      include_top=False, weights='imagenet', input_tensor=input_tensor,\n",
        "      classifier_activation='relu'\n",
        "  )\n",
        "\n",
        "  x=base_model.output\n",
        "  x=GlobalAveragePooling2D()(x)\n",
        "\n",
        "  x = Dense(1024,activation='relu')(x)\n",
        "  x = Dense(1024,activation='relu')(x)\n",
        "\n",
        "  output=Dense(1,activation='linear')(x)\n",
        "  model_vgg=Model(inputs=input_tensor,outputs=output)\n",
        "  return model_vgg\n",
        "\n",
        "def build_xception():\n",
        "  input_tensor = Input(shape=(HEIGHT, WIDTH, 3))\n",
        "  base_model = Xception(\n",
        "      include_top=False, weights='imagenet', input_tensor=input_tensor,\n",
        "      classifier_activation='relu'\n",
        "  )\n",
        "\n",
        "  x=base_model.output\n",
        "  x=GlobalAveragePooling2D()(x)\n",
        "\n",
        "  x = Dense(1024,activation='relu')(x)\n",
        "  x = Dense(1024,activation='relu')(x)\n",
        "\n",
        "  output=Dense(1,activation='linear')(x)\n",
        "  model_xception=Model(inputs=input_tensor,outputs=output)\n",
        "  return model_xception"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QiLnISg22XRo"
      },
      "source": [
        "**Ensemble Models**\n",
        "- kfold: try 5 or 10\n",
        "- for every model, for every fold, train the model\n",
        "- predict submit file for every model and every fold, calculate the average value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uDFk_TzB2LLN",
        "outputId": "fbf05647-949e-49bf-cdae-e04282c26f0f"
      },
      "source": [
        "x = df_train[\"filename\"].values\n",
        "y = df_train[\"sqft\"].values\n",
        "x_submit = df_test[\"filename\"].values\n",
        "\n",
        "oos_y = []\n",
        "oos_pred = []\n",
        "\n",
        "models = [build_vgg(),build_xception()]\n",
        "dataset_blend_train = np.zeros((x.shape[0], len(models)))\n",
        "dataset_blend_test = np.zeros((x_submit.shape[0], len(models)))\n",
        "submit_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "submit_generator = submit_datagen.flow_from_dataframe(\n",
        "        dataframe=df_test,\n",
        "        directory=PATH,\n",
        "        x_col=\"filename\",\n",
        "        batch_size = 1,\n",
        "        shuffle = False,\n",
        "        target_size=(HEIGHT, WIDTH),\n",
        "        class_mode=None)\n",
        "submit_generator.reset()\n",
        "\n",
        "kf = KFold(3)\n",
        "folds = list(kf.split(x))\n",
        "fold=0\n",
        "\n",
        "for j, model in enumerate(models):\n",
        "  print(\"Model: {} : {}\".format(j, model))\n",
        "  fold_sums = np.zeros((x_submit.shape[0], len(folds)))\n",
        "  total_rmse = 0\n",
        "  for i, (train, test) in enumerate(folds):\n",
        "    #training set\n",
        "    train_cut = df_train.loc[train]\n",
        "    #testing set\n",
        "    val_cut = df_train.loc[test]\n",
        "\n",
        "    training_datagen = ImageDataGenerator(\n",
        "      rescale = 1./255,\n",
        "      horizontal_flip=True,\n",
        "      fill_mode='nearest')\n",
        "\n",
        "    train_generator = training_datagen.flow_from_dataframe(\n",
        "          dataframe= train_cut,\n",
        "          directory=PATH,\n",
        "          x_col=\"filename\",\n",
        "          y_col=\"sqft\",\n",
        "          target_size=(HEIGHT, WIDTH),\n",
        "          batch_size=32,\n",
        "          class_mode='raw')\n",
        "\n",
        "    validation_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "    val_generator = validation_datagen.flow_from_dataframe(\n",
        "          dataframe=val_cut,\n",
        "          directory=PATH,\n",
        "          x_col=\"filename\",\n",
        "          y_col=\"sqft\",\n",
        "          target_size=(HEIGHT, WIDTH),\n",
        "          batch_size=256, # Make the validation batch size as large as you have memory for\n",
        "          class_mode='raw')  \n",
        "    \n",
        "    STEP_SIZE_VALID=val_generator.n//val_generator.batch_size\n",
        "\n",
        "    model.compile(loss = 'mean_squared_error', optimizer=Adam(learning_rate=1e-5), metrics=[RootMeanSquaredError(name=\"rmse\")])\n",
        "    monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=20, verbose=1, mode='min',\n",
        "            restore_best_weights=True)\n",
        "    # model_checkpoint1=ModelCheckpoint(\n",
        "    #     os.path.join(PATH, 'blend{}_bestmodel.h5'.format(j)),\n",
        "    #     monitor=\"val_loss\",save_best_only=True,mode=\"min\",verbose=1)\n",
        "    # model_checkpoint2=ModelCheckpoint(os.path.join(PATH, 'resnet152_latestmodel.h5'),monitor=\"val_loss\",save_best_only=False,mode=\"min\",\n",
        "    #                                       save_weights_only=True, verbose=0)\n",
        "    lr_reduce = ReduceLROnPlateau(monitor='val_loss',factor=0.6,patience=20,verbose=1,mode='auto',min_lr=1e-7)\n",
        "    callback = [monitor,model_checkpoint1,lr_reduce]\n",
        "    model.fit(train_generator, epochs=50, steps_per_epoch=250, validation_data = val_generator, \n",
        "              callbacks=callback, verbose = 1, validation_steps=STEP_SIZE_VALID)\n",
        "\n",
        "    pred = model.predict(val_generator)\n",
        "    dataset_blend_train[test, j] = pred[:, 0]\n",
        "    pred2 = model.predict(submit_generator,steps=len(df_test))\n",
        "    fold_sums[:, i] = pred2[:, 0]\n",
        "\n",
        "    rmse = np.sqrt(mean_squared_error(val_cut[\"sqft\"].values, pred[:,0]))\n",
        "    total_rmse+=rmse\n",
        "    print(\"Fold #{}: rmse={}\".format(i,rmse))\n",
        "  \n",
        "  print(\"{}: Mean rmse={}\".format(model.__class__.__name__,total_rmse/len(folds)))\n",
        "  dataset_blend_test[:, j] = fold_sums.mean(1)\n",
        "\n",
        "print()\n",
        "print(\"Blending models.\")\n",
        "\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "blend = BaggingRegressor(SVR())\n",
        "blend.fit(dataset_blend_train, y)\n",
        "submit_data = blend.predict(dataset_blend_test)\n",
        "best_df_submit = pd.DataFrame({\"id\":df_test['id'],'sqft':submit_data.flatten()})\n",
        "best_df_submit.to_csv(\"/content/drive/My Drive/Colab Notebooks/applications-of-deep-learning-wustlfall-2021/best_df_submit0.csv\",index = False)\n",
        "best_df_submit.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1070 validated image filenames.\n",
            "Model: 0 : <keras.engine.functional.Functional object at 0x7fb405f61a10>\n",
            "Found 15998 validated image filenames.\n",
            "Found 8000 validated image filenames.\n",
            "Epoch 1/50\n",
            "250/250 [==============================] - ETA: 0s - loss: 9884918.0000 - rmse: 3144.0291\n",
            "Epoch 00001: val_loss did not improve from 38344.63281\n",
            "250/250 [==============================] - 80s 314ms/step - loss: 9884918.0000 - rmse: 3144.0291 - val_loss: 2978513.2500 - val_rmse: 1725.8369 - lr: 1.0000e-05\n",
            "Epoch 2/50\n",
            "250/250 [==============================] - ETA: 0s - loss: 2486400.5000 - rmse: 1576.8324\n",
            "Epoch 00002: val_loss did not improve from 38344.63281\n",
            "250/250 [==============================] - 76s 305ms/step - loss: 2486400.5000 - rmse: 1576.8324 - val_loss: 1894992.7500 - val_rmse: 1376.5873 - lr: 1.0000e-05\n",
            "Epoch 3/50\n",
            "250/250 [==============================] - ETA: 0s - loss: 1426387.3750 - rmse: 1194.3146\n",
            "Epoch 00003: val_loss did not improve from 38344.63281\n",
            "250/250 [==============================] - 76s 305ms/step - loss: 1426387.3750 - rmse: 1194.3146 - val_loss: 1190743.1250 - val_rmse: 1091.2118 - lr: 1.0000e-05\n",
            "Epoch 4/50\n",
            "250/250 [==============================] - ETA: 0s - loss: 1114439.3750 - rmse: 1055.6700\n",
            "Epoch 00004: val_loss did not improve from 38344.63281\n",
            "250/250 [==============================] - 76s 305ms/step - loss: 1114439.3750 - rmse: 1055.6700 - val_loss: 1027187.0625 - val_rmse: 1013.5024 - lr: 1.0000e-05\n",
            "Epoch 5/50\n",
            "250/250 [==============================] - ETA: 0s - loss: 949823.8125 - rmse: 974.5891\n",
            "Epoch 00005: val_loss did not improve from 38344.63281\n",
            "250/250 [==============================] - 76s 304ms/step - loss: 949823.8125 - rmse: 974.5891 - val_loss: 816194.5000 - val_rmse: 903.4348 - lr: 1.0000e-05\n",
            "Epoch 6/50\n",
            "250/250 [==============================] - ETA: 0s - loss: 860329.1875 - rmse: 927.5393\n",
            "Epoch 00006: val_loss did not improve from 38344.63281\n",
            "250/250 [==============================] - 76s 303ms/step - loss: 860329.1875 - rmse: 927.5393 - val_loss: 741742.0000 - val_rmse: 861.2444 - lr: 1.0000e-05\n",
            "Epoch 7/50\n",
            "250/250 [==============================] - ETA: 0s - loss: 770234.5625 - rmse: 877.6301\n",
            "Epoch 00007: val_loss did not improve from 38344.63281\n",
            "250/250 [==============================] - 76s 303ms/step - loss: 770234.5625 - rmse: 877.6301 - val_loss: 768436.3750 - val_rmse: 876.6050 - lr: 1.0000e-05\n",
            "Epoch 8/50\n",
            "250/250 [==============================] - ETA: 0s - loss: 768421.0000 - rmse: 876.5963\n",
            "Epoch 00008: val_loss did not improve from 38344.63281\n",
            "250/250 [==============================] - 76s 303ms/step - loss: 768421.0000 - rmse: 876.5963 - val_loss: 671567.8125 - val_rmse: 819.4924 - lr: 1.0000e-05\n",
            "Epoch 9/50\n",
            "250/250 [==============================] - ETA: 0s - loss: 727240.6250 - rmse: 852.7841\n",
            "Epoch 00009: val_loss did not improve from 38344.63281\n",
            "250/250 [==============================] - 76s 304ms/step - loss: 727240.6250 - rmse: 852.7841 - val_loss: 701688.4375 - val_rmse: 837.6685 - lr: 1.0000e-05\n",
            "Epoch 10/50\n",
            "250/250 [==============================] - ETA: 0s - loss: 697547.0625 - rmse: 835.1928\n",
            "Epoch 00010: val_loss did not improve from 38344.63281\n",
            "250/250 [==============================] - 75s 302ms/step - loss: 697547.0625 - rmse: 835.1928 - val_loss: 691427.6250 - val_rmse: 831.5212 - lr: 1.0000e-05\n",
            "Epoch 11/50\n",
            "250/250 [==============================] - ETA: 0s - loss: 670882.6875 - rmse: 819.0743\n",
            "Epoch 00011: val_loss did not improve from 38344.63281\n",
            "250/250 [==============================] - 75s 301ms/step - loss: 670882.6875 - rmse: 819.0743 - val_loss: 768572.1250 - val_rmse: 876.6824 - lr: 1.0000e-05\n",
            "Epoch 12/50\n",
            "250/250 [==============================] - ETA: 0s - loss: 666510.4375 - rmse: 816.4009\n",
            "Epoch 00012: val_loss did not improve from 38344.63281\n",
            "250/250 [==============================] - 75s 302ms/step - loss: 666510.4375 - rmse: 816.4009 - val_loss: 587809.1875 - val_rmse: 766.6871 - lr: 1.0000e-05\n",
            "Epoch 13/50\n",
            "246/250 [============================>.] - ETA: 0s - loss: 611990.1875 - rmse: 782.2980"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-73f88a2fa413>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mcallback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_checkpoint1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr_reduce\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     model.fit(train_generator, epochs=50, steps_per_epoch=250, validation_data = val_generator, \n\u001b[0;32m---> 77\u001b[0;31m               callbacks=callback, verbose = 1, validation_steps=STEP_SIZE_VALID)\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1214\u001b[0m                 _r=1):\n\u001b[1;32m   1215\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1217\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    940\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3129\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3130\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3131\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3133\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1958\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1959\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1960\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1962\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    601\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    604\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 59\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     60\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CH2reYxswXWI"
      },
      "source": [
        "#### use regression model to predict submit file's result"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8z2arui3BDAn"
      },
      "source": [
        "from sklearn.linear_model import SGDRegressor\n",
        "blend = SGDRegressor()\n",
        "blend.fit(dataset_blend_train, y)\n",
        "submit_data = blend.predict(dataset_blend_test)\n",
        "best_df_submit = pd.DataFrame({\"id\":df_test['id'],'sqft':submit_data.flatten()})\n",
        "best_df_submit.to_csv(\"/content/drive/My Drive/Colab Notebooks/applications-of-deep-learning-wustlfall-2021/best_df_submit2.csv\",index = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "-v3jo_gJBccP",
        "outputId": "be6dbf69-a0a0-4e9c-99b2-76f19897510d"
      },
      "source": [
        "from sklearn.svm import SVR\n",
        "blend = SVR()\n",
        "blend.fit(dataset_blend_train, y)\n",
        "submit_data = blend.predict(dataset_blend_test)\n",
        "best_df_submit = pd.DataFrame({\"id\":df_test['id'],'sqft':submit_data.flatten()})\n",
        "best_df_submit.to_csv(\"/content/drive/My Drive/Colab Notebooks/applications-of-deep-learning-wustlfall-2021/best_df_submit3.csv\",index = False)\n",
        "best_df_submit.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>sqft</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>24000</td>\n",
              "      <td>5822.377037</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>24001</td>\n",
              "      <td>5835.473316</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>24002</td>\n",
              "      <td>5834.567900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>24003</td>\n",
              "      <td>5817.118924</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>24004</td>\n",
              "      <td>5822.497574</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      id         sqft\n",
              "0  24000  5822.377037\n",
              "1  24001  5835.473316\n",
              "2  24002  5834.567900\n",
              "3  24003  5817.118924\n",
              "4  24004  5822.497574"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "ILT9QMbpCiUo",
        "outputId": "18a3be8c-15f7-40c8-fc60-3dde3396b3c0"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "blend = KNeighborsRegressor(n_neighbors=5)\n",
        "blend.fit(dataset_blend_train, y)\n",
        "submit_data = blend.predict(dataset_blend_test)\n",
        "best_df_submit = pd.DataFrame({\"id\":df_test['id'],'sqft':submit_data.flatten()})\n",
        "best_df_submit.to_csv(\"/content/drive/My Drive/Colab Notebooks/applications-of-deep-learning-wustlfall-2021/best_df_submit5.csv\",index = False)\n",
        "best_df_submit.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>sqft</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>24000</td>\n",
              "      <td>4675.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>24001</td>\n",
              "      <td>6828.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>24002</td>\n",
              "      <td>5757.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>24003</td>\n",
              "      <td>4643.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>24004</td>\n",
              "      <td>5999.6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      id    sqft\n",
              "0  24000  4675.2\n",
              "1  24001  6828.4\n",
              "2  24002  5757.0\n",
              "3  24003  4643.0\n",
              "4  24004  5999.6"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "j3kdhe-zC773",
        "outputId": "32170237-d89d-4e4b-b52c-69e5538f762b"
      },
      "source": [
        "from sklearn.ensemble import BaggingRegressor\n",
        "blend = BaggingRegressor(SVR())\n",
        "blend.fit(dataset_blend_train, y)\n",
        "submit_data = blend.predict(dataset_blend_test)\n",
        "best_df_submit = pd.DataFrame({\"id\":df_test['id'],'sqft':submit_data.flatten()})\n",
        "best_df_submit.to_csv(\"/content/drive/My Drive/Colab Notebooks/applications-of-deep-learning-wustlfall-2021/best_df_submit4.csv\",index = False)\n",
        "best_df_submit.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>sqft</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>24000</td>\n",
              "      <td>5835.957249</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>24001</td>\n",
              "      <td>5845.931830</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>24002</td>\n",
              "      <td>5830.816485</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>24003</td>\n",
              "      <td>5830.907638</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>24004</td>\n",
              "      <td>5837.341104</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      id         sqft\n",
              "0  24000  5835.957249\n",
              "1  24001  5845.931830\n",
              "2  24002  5830.816485\n",
              "3  24003  5830.907638\n",
              "4  24004  5837.341104"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "ywp_96SnDXEc",
        "outputId": "124033ce-e116-4d66-b909-9500c3d95130"
      },
      "source": [
        "from sklearn import tree\n",
        "blend = tree.DecisionTreeRegressor()\n",
        "blend.fit(dataset_blend_train, y)\n",
        "submit_data = blend.predict(dataset_blend_test)\n",
        "best_df_submit = pd.DataFrame({\"id\":df_test['id'],'sqft':submit_data.flatten()})\n",
        "best_df_submit.to_csv(\"/content/drive/My Drive/Colab Notebooks/applications-of-deep-learning-wustlfall-2021/best_df_submit6.csv\",index = False)\n",
        "best_df_submit.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>sqft</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>24000</td>\n",
              "      <td>5746.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>24001</td>\n",
              "      <td>7105.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>24002</td>\n",
              "      <td>7947.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>24003</td>\n",
              "      <td>6026.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>24004</td>\n",
              "      <td>4804.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      id    sqft\n",
              "0  24000  5746.0\n",
              "1  24001  7105.0\n",
              "2  24002  7947.0\n",
              "3  24003  6026.0\n",
              "4  24004  4804.0"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvA4PlhZ7v_p"
      },
      "source": [
        "# from sklearn.model_selection import KFold\n",
        "# from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# x = df_train[\"filename\"].values\n",
        "# oos_y = []\n",
        "# oos_pred = []\n",
        "\n",
        "# kf = KFold(10)\n",
        "# fold=0\n",
        "# for train, test in kf.split(x):\n",
        "#   fold+=1\n",
        "#   print(f\"fold #{fold}\")\n",
        "\n",
        "#   train_cut = df_train.loc[train]\n",
        "#   val_cut = df_train.loc[test]\n",
        "\n",
        "#   training_datagen = ImageDataGenerator(\n",
        "#     rescale = 1./255,\n",
        "#     horizontal_flip=True,\n",
        "#     fill_mode='nearest')\n",
        "\n",
        "#   train_generator = training_datagen.flow_from_dataframe(\n",
        "#         dataframe= train_cut,\n",
        "#         directory=PATH,\n",
        "#         x_col=\"filename\",\n",
        "#         y_col=\"sqft\",\n",
        "#         target_size=(150, 150),\n",
        "#         batch_size=32,\n",
        "#         class_mode='raw')\n",
        "\n",
        "#   validation_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "#   val_generator = validation_datagen.flow_from_dataframe(\n",
        "#         dataframe=val_cut,\n",
        "#         directory=PATH,\n",
        "#         x_col=\"filename\",\n",
        "#         y_col=\"sqft\",\n",
        "#         target_size=(150, 150),\n",
        "#         batch_size=256, # Make the validation batch size as large as you have memory for\n",
        "#         class_mode='raw')\n",
        "\n",
        "#   STEP_SIZE_VALID=val_generator.n//val_generator.batch_size\n",
        "\n",
        "#   model.compile(loss = 'mean_squared_error', optimizer=Adam(learning_rate=1e-5), metrics=[RootMeanSquaredError(name=\"rmse\")])\n",
        "#   monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=20, verbose=1, mode='min',\n",
        "#           restore_best_weights=True)\n",
        "#   model_checkpoint1=ModelCheckpoint(os.path.join(PATH, 'resnet152_bestmodel.h5'),monitor=\"val_loss\",save_best_only=True,mode=\"min\",verbose=1)\n",
        "#   # model_checkpoint2=ModelCheckpoint(os.path.join(PATH, 'resnet152_latestmodel.h5'),monitor=\"val_loss\",save_best_only=False,mode=\"min\",\n",
        "#   #                                       save_weights_only=True, verbose=0)\n",
        "#   lr_reduce = ReduceLROnPlateau(monitor='val_loss',factor=0.6,patience=20,verbose=1,mode='auto',min_lr=1e-7)\n",
        "#   callback = [monitor,model_checkpoint1,model_checkpoint2,lr_reduce]\n",
        "#   model.fit(train_generator, epochs=100, steps_per_epoch=250, validation_data = val_generator, \n",
        "#             callbacks=callback, verbose = 1, validation_steps=STEP_SIZE_VALID)\n",
        "\n",
        "#   model.load_weights(os.path.join(PATH, 'resnet152_bestmodel.h5'))\n",
        "#   pred = model.predict(val_generator)\n",
        "    \n",
        "#   oos_y.append(df_train.loc[test,\"sqft\"].values)\n",
        "#   oos_pred.append(pred)\n",
        "\n",
        "#   score = np.sqrt(mean_squared_error(pred,df_train.loc[test,\"sqft\"].values))\n",
        "#   print(f\"Fold score (RMSE): {score}\")\n",
        "\n",
        "# # Build the oos prediction list and calculate the error.\n",
        "# oos_y = np.concatenate(oos_y)\n",
        "# oos_pred = np.concatenate(oos_pred)\n",
        "# score = np.sqrt(mean_squared_error(oos_pred,oos_y))\n",
        "# print(f\"Final, out of sample score (RMSE): {score}\")  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbUj-pFpwrtH"
      },
      "source": [
        "## Bayesian-Optimization parameters tuning\n",
        "\n",
        "**reminder: it takes a lot GPU, and will be crashed if the GPU and RAM are full**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMx0_-TzxRaT"
      },
      "source": [
        "##### save pre-trained model results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYPgKfoZTR_K"
      },
      "source": [
        "# feature_train = model0.predict(train_generator)\n",
        "# feature_val = model0.predict(val_generator)\n",
        "# np.save(\"/content/drive/My Drive/Colab Notebooks/applications-of-deep-learning-wustlfall-2021/feature_train.npy\",feature_train)\n",
        "# np.save(\"/content/drive/My Drive/Colab Notebooks/applications-of-deep-learning-wustlfall-2021/feature_val.npy\",feature_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucfATCQ04KAL",
        "outputId": "64d9f5b2-57bf-4a2c-d38a-a3bf651e6bc7"
      },
      "source": [
        "# feature_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(21598, 5, 5, 2048)"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PftAJ-jWq4Jv"
      },
      "source": [
        "# STEP_SIZE_VALID=val_generator.n//val_generator.batch_size\n",
        "\n",
        "# base_model.compile(loss = 'mean_squared_error', optimizer=Adam(learning_rate=1e-4), metrics=[RootMeanSquaredError(name=\"rmse\")])\n",
        "# monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=50, verbose=1, mode='auto',\n",
        "#         restore_best_weights=True)\n",
        "# basemodel_checkpoint=ModelCheckpoint('basemodel.h5',monitor=\"val_loss\",save_best_only=True,mode=\"max\",\n",
        "#                                       save_weights_only=True, verbose=1)\n",
        "# lr_reduce = ReduceLROnPlateau(monitor='val_loss',factor=0.6,patience=30,verbose=1,mode='auto',min_lr=1e-7)\n",
        "# base_model.fit(train_generator, epochs=300, steps_per_epoch=250, validation_data = val_generator, \n",
        "#                callbacks=[monitor,basemodel_checkpoint,lr_reduce], verbose = 1, validation_steps=STEP_SIZE_VALID)\n",
        "\n",
        "# if os.path.isfile('basemodel.h5'):\n",
        "#     base_model.load_weights('basemodel.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c62ESr3xftA"
      },
      "source": [
        "##### clear RAM and GPU, start the following steps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbQz5YXer-bo"
      },
      "source": [
        "# feature_val = np.load(\"/content/drive/My Drive/Colab Notebooks/applications-of-deep-learning-wustlfall-2021/feature_val.npy\")\n",
        "# feature_train = np.load(\"/content/drive/My Drive/Colab Notebooks/applications-of-deep-learning-wustlfall-2021/feature_train.npy\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vo6_5ZCjxuEA"
      },
      "source": [
        "##### build the model that should be tuned"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhAJO6yFrxAU"
      },
      "source": [
        "# hyperparameters\n",
        "def build_model(neuronCount=256, learning_rate=1e-4,l2=1e-3,activation=3,rate=0.7,layer_number=2):\n",
        "  initializer = he_normal()\n",
        "  input_tensor = Input(shape=(HEIGHT, WIDTH, 3)) # !caution here\n",
        "  base = basemodel(input_tensor=input_tensor)\n",
        "  x=GlobalAveragePooling2D()(base.output)\n",
        "\n",
        "  activation_dict={1:\"relu\",2:\"elu\",3:\"tanh\"}\n",
        "  layer = 0\n",
        "  while layer<layer_number:\n",
        "    x = Dense(units=neuronCount,activation=activation_dict[activation],\n",
        "                  kernel_initializer=initializer,kernel_regularizer=tensorflow.keras.regularizers.l2(l2=l2))(x)\n",
        "    x=Dropout(rate)(x)\n",
        "    layer+=1\n",
        "\n",
        "  output=Dense(1,activation=\"linear\")(x)\n",
        "  model=Model(inputs=input_tensor,outputs=output)\n",
        "  model.compile(loss = 'mean_squared_error', optimizer=Adam(learning_rate=learning_rate), metrics=[RootMeanSquaredError(name=\"rmse\")])\n",
        "  lr_reduce = tensorflow.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',factor=0.6,patience=30,verbose=1,mode='min',min_lr=1e-7)\n",
        "  monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=50, verbose=1, mode='min',\n",
        "                          restore_best_weights=True)\n",
        "  checkpoint = ModelCheckpoint('city.h5',monitor='val_loss',verbose=1,save_weights_only=True,save_best_only=True,\n",
        "                                mode='min',save_freq='epoch')\n",
        "  return model, lr_reduce, monitor, checkpoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HG3j5iVx0Be"
      },
      "source": [
        "##### build training model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qkJ1Bn_r703"
      },
      "source": [
        "def fit_model(model, lr_reduce, monitor, checkpoint):\n",
        "  \"\"\"function that trains the head classifier\"\"\"\n",
        "  history = model.fit(feature_train, df_train_cut[\"sqft\"],\n",
        "                      batch_size=32,\n",
        "                      epochs=50,\n",
        "                      verbose=1,\n",
        "                      callbacks=[lr_reduce, monitor, checkpoint],\n",
        "                      validation_data=(feature_val, df_validate_cut['sqft']),\n",
        "                      shuffle=True)\n",
        "  return history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dIEGQGFx34w"
      },
      "source": [
        "##### evaluate the model and use the evaluation to optimize the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBNFb2Rvr44_"
      },
      "source": [
        "def evaluate_model(model):\n",
        "  \"\"\"function that evaluates the head classifier\"\"\"\n",
        "  evaluation = model.evaluate(feature_val, df_validate_cut['sqft'])\n",
        "  return evaluation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhWvPyEAyBon"
      },
      "source": [
        "##### tuning parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-acZedfEtD-J",
        "outputId": "3cb36560-90de-423f-b917-72dda6f7b964"
      },
      "source": [
        "from tensorflow.keras.backend import clear_session\n",
        "!pip install GPy\n",
        "!pip install GPyOpt\n",
        "import GPy\n",
        "import GPyOpt\n",
        "from GPyOpt.methods import BayesianOptimization\n",
        "from matplotlib import pyplot as plt\n",
        "# define the kernel for the Bayesian surrogate model using the \"radial basis function\" (RBF)\n",
        "kernel = GPy.kern.RBF(input_dim=1, variance=1.0, lengthscale=1.0)\n",
        "# hyperparameter bounds\n",
        "bounds = [\n",
        "          {'name': 'neuronCount', 'type': 'discrete', 'domain': (256,512,1024)},\n",
        "          {'name': 'learning_rate', 'type': 'discrete', 'domain': (1e-4, 1e-5)}, \n",
        "          {'name': 'l2', 'type': 'discrete', 'domain': (1e-4,1e-5)}, \n",
        "          {'name': 'activation', 'type': 'discrete', 'domain': (1, 2, 3)}, \n",
        "          {'name': 'rate', 'type': 'discrete', 'domain': (0.5,0.6,0.7)},\n",
        "          {'name': 'layer_number', 'type': 'discrete', 'domain': (4,5,6)}\n",
        "          ]\n",
        "# Note: 'activation' domain parameters (1, 2, 3) correspond to strings ('relu', 'elu', 'tanh'); dictionary defined in build_model()\n",
        "# objective function for the model optimization:\n",
        "def f(x):\n",
        "  \"\"\"objective function of the Bayesian surrogate model\"\"\"\n",
        "  print()\n",
        "  print(\"Hyperparameters:\", x)\n",
        "  # Retrieve 'accuracy' from the previously saved model\n",
        "  try:\n",
        "    previous_best_model = load_model('city.h5')\n",
        "    previous_evaluation = evaluate_model(previous_best_model)\n",
        "  except Exception:\n",
        "    previous_best_model = None\n",
        "  model, lr_reduce, monitor, checkpoint = build_model(\n",
        "                                        neuronCount=int(x[:,0]),\n",
        "                                        learning_rate=float(x[:,1]),\n",
        "                                        l2=float(x[:,2]),\n",
        "                                        activation=int(x[:,3]),\n",
        "                                        rate=float(x[:,4]),\n",
        "                                        layer_number=int(x[:,5])\n",
        "                                        )\n",
        "  history = fit_model(model, lr_reduce, monitor, checkpoint)\n",
        "  evaluation = evaluate_model(model)\n",
        "  print()\n",
        "  print(\"parameters:\\t{0} LOSS:\\t{1} \\t rmse:\\t{2}\".format(x,evaluation[0], evaluation[1]))\n",
        "  print(evaluation)\n",
        "  print()\n",
        "  # compare previous and current validation accuracies\n",
        "  if not previous_best_model:\n",
        "    save_model(model, 'city.h5', overwrite=False, include_optimizer=True)\n",
        "  if previous_best_model and evaluation[-1] < previous_evaluation[-1]:\n",
        "    save_model(model, 'city.h5', overwrite=True,include_optimizer=True)\n",
        "\n",
        "  # Get the dictionary containing each metric and the loss for each epoch\n",
        "  # history_dict = history.history\n",
        "  # print(history_dict)\n",
        "  def plot_history(history):\n",
        "    \"\"\"function that plots the model loss and accuracy\"\"\"\n",
        "    plt.figure(1, figsize = (15,8))\n",
        "    plt.subplot(221)\n",
        "    plt.plot(history.history['rmse'])\n",
        "    plt.plot(history.history['val_rmse'])\n",
        "    plt.title('model rmse')\n",
        "    plt.ylabel('rmse')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'val'])\n",
        "    plt.subplot(222)\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('model loss')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'val'])\n",
        "    plt.show()\n",
        "  # # plot the model accuracy and loss results\n",
        "  plot_history(history)\n",
        "  # delete the instantiated models from memory and clear the session\n",
        "  del model\n",
        "  del previous_best_model\n",
        "  clear_session()\n",
        "  return evaluation[1]\n",
        "\n",
        "optimizer = BayesianOptimization(f=f,\n",
        "                                 domain=bounds,\n",
        "                                 model_type='GP',\n",
        "                                 kernel=kernel,\n",
        "                                 acquisition_type ='EI',\n",
        "                                 acquisition_jitter = 0.01,\n",
        "                                 exact_feval=False,\n",
        "                                 normalize_Y=False,\n",
        "                                #  maximize=True, we try to minimize the loss/rmse, so keep false here\n",
        "                                 verbosity=True)\n",
        "\n",
        "print()\n",
        "print(\"=====================\")\n",
        "print(\"=====================\")\n",
        "print()\n",
        "optimizer.run_optimization(max_iter=30, verbosity=False)\n",
        "optimizer.plot_acquisition()\n",
        "optimizer.plot_convergence()\n",
        "optimizer.save_report('bayes_opt.txt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: GPy in /usr/local/lib/python3.7/dist-packages (1.10.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.7/dist-packages (from GPy) (1.19.5)\n",
            "Requirement already satisfied: cython>=0.29 in /usr/local/lib/python3.7/dist-packages (from GPy) (0.29.24)\n",
            "Requirement already satisfied: scipy>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from GPy) (1.4.1)\n",
            "Requirement already satisfied: paramz>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from GPy) (0.9.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from GPy) (1.15.0)\n",
            "Requirement already satisfied: decorator>=4.0.10 in /usr/local/lib/python3.7/dist-packages (from paramz>=0.9.0->GPy) (4.4.2)\n",
            "Requirement already satisfied: GPyOpt in /usr/local/lib/python3.7/dist-packages (1.2.6)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.7/dist-packages (from GPyOpt) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.16 in /usr/local/lib/python3.7/dist-packages (from GPyOpt) (1.4.1)\n",
            "Requirement already satisfied: GPy>=1.8 in /usr/local/lib/python3.7/dist-packages (from GPyOpt) (1.10.0)\n",
            "Requirement already satisfied: paramz>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from GPy>=1.8->GPyOpt) (0.9.5)\n",
            "Requirement already satisfied: cython>=0.29 in /usr/local/lib/python3.7/dist-packages (from GPy>=1.8->GPyOpt) (0.29.24)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from GPy>=1.8->GPyOpt) (1.15.0)\n",
            "Requirement already satisfied: decorator>=4.0.10 in /usr/local/lib/python3.7/dist-packages (from paramz>=0.9.0->GPy>=1.8->GPyOpt) (4.4.2)\n",
            "\n",
            "Hyperparameters: [[1.024e+03 1.000e-04 1.000e-05 2.000e+00 6.000e-01 4.000e+00]]\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name='input_1'), name='input_1', description=\"created by layer 'input_1'\"), but it was called on an input with incompatible shape (32, 5, 5, 2048).\n",
            "Epoch 1/50\n",
            "673/675 [============================>.] - ETA: 0s - loss: 7636229.0000 - rmse: 2763.3728\n",
            "Epoch 00001: val_loss improved from inf to 3888389.50000, saving model to city.h5\n",
            "675/675 [==============================] - 6s 7ms/step - loss: 7628205.5000 - rmse: 2761.9207 - val_loss: 3888389.5000 - val_rmse: 1971.8999 - lr: 1.0000e-04\n",
            "Epoch 2/50\n",
            "673/675 [============================>.] - ETA: 0s - loss: 4191992.5000 - rmse: 2047.4355\n",
            "Epoch 00002: val_loss improved from 3888389.50000 to 3817821.00000, saving model to city.h5\n",
            "675/675 [==============================] - 4s 6ms/step - loss: 4193941.0000 - rmse: 2047.9114 - val_loss: 3817821.0000 - val_rmse: 1953.9244 - lr: 1.0000e-04\n",
            "Epoch 3/50\n",
            "672/675 [============================>.] - ETA: 0s - loss: 4098836.2500 - rmse: 2024.5582\n",
            "Epoch 00003: val_loss improved from 3817821.00000 to 3808268.75000, saving model to city.h5\n",
            "675/675 [==============================] - 4s 6ms/step - loss: 4101833.2500 - rmse: 2025.2983 - val_loss: 3808268.7500 - val_rmse: 1951.4785 - lr: 1.0000e-04\n",
            "Epoch 4/50\n",
            "667/675 [============================>.] - ETA: 0s - loss: 4088750.7500 - rmse: 2022.0659\n",
            "Epoch 00004: val_loss did not improve from 3808268.75000\n",
            "675/675 [==============================] - 4s 6ms/step - loss: 4086600.2500 - rmse: 2021.5341 - val_loss: 3808471.0000 - val_rmse: 1951.5304 - lr: 1.0000e-04\n",
            "Epoch 5/50\n",
            "674/675 [============================>.] - ETA: 0s - loss: 4047378.5000 - rmse: 2011.8097\n",
            "Epoch 00005: val_loss improved from 3808268.75000 to 3804973.75000, saving model to city.h5\n",
            "675/675 [==============================] - 4s 6ms/step - loss: 4048124.7500 - rmse: 2011.9952 - val_loss: 3804973.7500 - val_rmse: 1950.6342 - lr: 1.0000e-04\n",
            "Epoch 6/50\n",
            "666/675 [============================>.] - ETA: 0s - loss: 4026465.7500 - rmse: 2006.6056\n",
            "Epoch 00006: val_loss did not improve from 3804973.75000\n",
            "675/675 [==============================] - 4s 6ms/step - loss: 4026746.2500 - rmse: 2006.6754 - val_loss: 3837821.7500 - val_rmse: 1959.0359 - lr: 1.0000e-04\n",
            "Epoch 7/50\n",
            "666/675 [============================>.] - ETA: 0s - loss: 4007218.0000 - rmse: 2001.8037\n",
            "Epoch 00007: val_loss did not improve from 3804973.75000\n",
            "675/675 [==============================] - 4s 6ms/step - loss: 4005704.7500 - rmse: 2001.4257 - val_loss: 3807305.0000 - val_rmse: 1951.2316 - lr: 1.0000e-04\n",
            "Epoch 8/50\n",
            "673/675 [============================>.] - ETA: 0s - loss: 3978178.5000 - rmse: 1994.5371\n",
            "Epoch 00008: val_loss did not improve from 3804973.75000\n",
            "675/675 [==============================] - 4s 6ms/step - loss: 3976215.0000 - rmse: 1994.0448 - val_loss: 3818672.2500 - val_rmse: 1954.1423 - lr: 1.0000e-04\n",
            "Epoch 9/50\n",
            "666/675 [============================>.] - ETA: 0s - loss: 3977568.0000 - rmse: 1994.3840\n",
            "Epoch 00009: val_loss did not improve from 3804973.75000\n",
            "675/675 [==============================] - 4s 6ms/step - loss: 3975298.2500 - rmse: 1993.8149 - val_loss: 3817626.5000 - val_rmse: 1953.8748 - lr: 1.0000e-04\n",
            "Epoch 10/50\n",
            "669/675 [============================>.] - ETA: 0s - loss: 3926880.0000 - rmse: 1981.6356\n",
            "Epoch 00010: val_loss did not improve from 3804973.75000\n",
            "675/675 [==============================] - 4s 6ms/step - loss: 3929741.2500 - rmse: 1982.3575 - val_loss: 3857577.0000 - val_rmse: 1964.0715 - lr: 1.0000e-04\n",
            "Epoch 11/50\n",
            "671/675 [============================>.] - ETA: 0s - loss: 3968261.7500 - rmse: 1992.0496\n",
            "Epoch 00011: val_loss did not improve from 3804973.75000\n",
            "675/675 [==============================] - 4s 6ms/step - loss: 3965847.0000 - rmse: 1991.4435 - val_loss: 3908897.7500 - val_rmse: 1977.0933 - lr: 1.0000e-04\n",
            "Epoch 12/50\n",
            "673/675 [============================>.] - ETA: 0s - loss: 3922565.2500 - rmse: 1980.5466\n",
            "Epoch 00012: val_loss did not improve from 3804973.75000\n",
            "675/675 [==============================] - 4s 6ms/step - loss: 3919410.5000 - rmse: 1979.7501 - val_loss: 3841650.2500 - val_rmse: 1960.0128 - lr: 1.0000e-04\n",
            "Epoch 13/50\n",
            "675/675 [==============================] - ETA: 0s - loss: 3934619.2500 - rmse: 1983.5874\n",
            "Epoch 00013: val_loss did not improve from 3804973.75000\n",
            "675/675 [==============================] - 4s 6ms/step - loss: 3934619.2500 - rmse: 1983.5874 - val_loss: 3866348.7500 - val_rmse: 1966.3032 - lr: 1.0000e-04\n",
            "Epoch 14/50\n",
            "674/675 [============================>.] - ETA: 0s - loss: 3898537.7500 - rmse: 1974.4714\n",
            "Epoch 00014: val_loss did not improve from 3804973.75000\n",
            "675/675 [==============================] - 4s 6ms/step - loss: 3896208.2500 - rmse: 1973.8815 - val_loss: 3878139.7500 - val_rmse: 1969.2993 - lr: 1.0000e-04\n",
            "Epoch 15/50\n",
            "666/675 [============================>.] - ETA: 0s - loss: 3912289.2500 - rmse: 1977.9507\n",
            "Epoch 00015: val_loss did not improve from 3804973.75000\n",
            "675/675 [==============================] - 4s 6ms/step - loss: 3912478.7500 - rmse: 1977.9987 - val_loss: 3889059.5000 - val_rmse: 1972.0698 - lr: 1.0000e-04\n",
            "Epoch 16/50\n",
            "669/675 [============================>.] - ETA: 0s - loss: 3868907.2500 - rmse: 1966.9537\n",
            "Epoch 00016: val_loss did not improve from 3804973.75000\n",
            "675/675 [==============================] - 4s 6ms/step - loss: 3863408.7500 - rmse: 1965.5555 - val_loss: 3880367.0000 - val_rmse: 1969.8647 - lr: 1.0000e-04\n",
            "Epoch 17/50\n",
            "672/675 [============================>.] - ETA: 0s - loss: 3870112.7500 - rmse: 1967.2601\n",
            "Epoch 00017: val_loss did not improve from 3804973.75000\n",
            "675/675 [==============================] - 4s 6ms/step - loss: 3866753.0000 - rmse: 1966.4061 - val_loss: 3894133.2500 - val_rmse: 1973.3558 - lr: 1.0000e-04\n",
            "Epoch 18/50\n",
            "669/675 [============================>.] - ETA: 0s - loss: 3847114.0000 - rmse: 1961.4061\n",
            "Epoch 00018: val_loss did not improve from 3804973.75000\n",
            "675/675 [==============================] - 4s 6ms/step - loss: 3844492.5000 - rmse: 1960.7378 - val_loss: 3898797.7500 - val_rmse: 1974.5374 - lr: 1.0000e-04\n",
            "Epoch 19/50\n",
            "666/675 [============================>.] - ETA: 0s - loss: 3847667.0000 - rmse: 1961.5471\n",
            "Epoch 00019: val_loss did not improve from 3804973.75000\n",
            "675/675 [==============================] - 4s 6ms/step - loss: 3847112.0000 - rmse: 1961.4056 - val_loss: 3947297.2500 - val_rmse: 1986.7806 - lr: 1.0000e-04\n",
            "Epoch 20/50\n",
            "668/675 [============================>.] - ETA: 0s - loss: 3823968.5000 - rmse: 1955.4969\n",
            "Epoch 00020: val_loss did not improve from 3804973.75000\n",
            "675/675 [==============================] - 4s 6ms/step - loss: 3823914.7500 - rmse: 1955.4833 - val_loss: 3979855.2500 - val_rmse: 1994.9574 - lr: 1.0000e-04\n",
            "Epoch 21/50\n",
            "666/675 [============================>.] - ETA: 0s - loss: 3814505.0000 - rmse: 1953.0758\n",
            "Epoch 00021: val_loss did not improve from 3804973.75000\n",
            "675/675 [==============================] - 4s 6ms/step - loss: 3811576.2500 - rmse: 1952.3258 - val_loss: 3954869.2500 - val_rmse: 1988.6853 - lr: 1.0000e-04\n",
            "Epoch 22/50\n",
            "671/675 [============================>.] - ETA: 0s - loss: 3804809.7500 - rmse: 1950.5922\n",
            "Epoch 00022: val_loss did not improve from 3804973.75000\n",
            "675/675 [==============================] - 4s 6ms/step - loss: 3802069.5000 - rmse: 1949.8895 - val_loss: 3937495.5000 - val_rmse: 1984.3124 - lr: 1.0000e-04\n",
            "Epoch 23/50\n",
            "675/675 [==============================] - ETA: 0s - loss: 3813216.7500 - rmse: 1952.7460\n",
            "Epoch 00023: val_loss did not improve from 3804973.75000\n",
            "675/675 [==============================] - 4s 6ms/step - loss: 3813216.7500 - rmse: 1952.7460 - val_loss: 3954656.5000 - val_rmse: 1988.6318 - lr: 1.0000e-04\n",
            "Epoch 24/50\n",
            "673/675 [============================>.] - ETA: 0s - loss: 3787444.2500 - rmse: 1946.1357\n",
            "Epoch 00024: val_loss did not improve from 3804973.75000\n",
            "675/675 [==============================] - 4s 6ms/step - loss: 3784402.7500 - rmse: 1945.3541 - val_loss: 4003771.2500 - val_rmse: 2000.9425 - lr: 1.0000e-04\n",
            "Epoch 25/50\n",
            "671/675 [============================>.] - ETA: 0s - loss: 3787580.7500 - rmse: 1946.1708\n",
            "Epoch 00025: val_loss did not improve from 3804973.75000\n",
            "675/675 [==============================] - 4s 6ms/step - loss: 3787740.7500 - rmse: 1946.2119 - val_loss: 3966931.7500 - val_rmse: 1991.7157 - lr: 1.0000e-04\n",
            "Epoch 26/50\n",
            "668/675 [============================>.] - ETA: 0s - loss: 3803474.5000 - rmse: 1950.2499\n",
            "Epoch 00026: val_loss did not improve from 3804973.75000\n",
            "675/675 [==============================] - 4s 6ms/step - loss: 3800813.0000 - rmse: 1949.5674 - val_loss: 4014832.7500 - val_rmse: 2003.7047 - lr: 1.0000e-04\n",
            "Epoch 27/50\n",
            "666/675 [============================>.] - ETA: 0s - loss: 3749107.2500 - rmse: 1936.2611\n",
            "Epoch 00027: val_loss did not improve from 3804973.75000\n",
            "675/675 [==============================] - 4s 6ms/step - loss: 3751129.5000 - rmse: 1936.7833 - val_loss: 4002334.7500 - val_rmse: 2000.5836 - lr: 1.0000e-04\n",
            "Epoch 28/50\n",
            "675/675 [==============================] - ETA: 0s - loss: 3779648.5000 - rmse: 1944.1317\n",
            "Epoch 00028: val_loss did not improve from 3804973.75000\n",
            "675/675 [==============================] - 4s 6ms/step - loss: 3779648.5000 - rmse: 1944.1317 - val_loss: 3973307.2500 - val_rmse: 1993.3156 - lr: 1.0000e-04\n",
            "Epoch 29/50\n",
            "668/675 [============================>.] - ETA: 0s - loss: 3732359.0000 - rmse: 1931.9314\n",
            "Epoch 00029: val_loss did not improve from 3804973.75000\n",
            "675/675 [==============================] - 4s 6ms/step - loss: 3736102.2500 - rmse: 1932.8999 - val_loss: 3998827.0000 - val_rmse: 1999.7067 - lr: 1.0000e-04\n",
            "Epoch 30/50\n",
            "675/675 [==============================] - ETA: 0s - loss: 3740702.0000 - rmse: 1934.0894\n",
            "Epoch 00030: val_loss did not improve from 3804973.75000\n",
            "675/675 [==============================] - 4s 6ms/step - loss: 3740702.0000 - rmse: 1934.0894 - val_loss: 4014245.5000 - val_rmse: 2003.5582 - lr: 1.0000e-04\n",
            "Epoch 31/50\n",
            "667/675 [============================>.] - ETA: 0s - loss: 3734931.5000 - rmse: 1932.5970\n",
            "Epoch 00031: val_loss did not improve from 3804973.75000\n",
            "675/675 [==============================] - 4s 6ms/step - loss: 3730088.7500 - rmse: 1931.3438 - val_loss: 4022309.0000 - val_rmse: 2005.5695 - lr: 1.0000e-04\n",
            "Epoch 32/50\n",
            "673/675 [============================>.] - ETA: 0s - loss: 3715800.0000 - rmse: 1927.6410\n",
            "Epoch 00032: val_loss did not improve from 3804973.75000\n",
            "675/675 [==============================] - 4s 6ms/step - loss: 3714484.7500 - rmse: 1927.2998 - val_loss: 4017845.7500 - val_rmse: 2004.4564 - lr: 1.0000e-04\n",
            "Epoch 33/50\n",
            "674/675 [============================>.] - ETA: 0s - loss: 3735171.7500 - rmse: 1932.6593\n",
            "Epoch 00033: val_loss did not improve from 3804973.75000\n",
            "675/675 [==============================] - 4s 6ms/step - loss: 3735478.5000 - rmse: 1932.7386 - val_loss: 3994528.5000 - val_rmse: 1998.6316 - lr: 1.0000e-04\n",
            "Epoch 34/50\n",
            "673/675 [============================>.] - ETA: 0s - loss: 3714288.7500 - rmse: 1927.2490\n",
            "Epoch 00034: val_loss did not improve from 3804973.75000\n",
            "675/675 [==============================] - 4s 6ms/step - loss: 3715188.7500 - rmse: 1927.4824 - val_loss: 4035787.5000 - val_rmse: 2008.9269 - lr: 1.0000e-04\n",
            "Epoch 35/50\n",
            "669/675 [============================>.] - ETA: 0s - loss: 3713328.2500 - rmse: 1926.9998\n",
            "Epoch 00035: ReduceLROnPlateau reducing learning rate to 5.999999848427251e-05.\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 3804973.75000\n",
            "675/675 [==============================] - 4s 6ms/step - loss: 3709428.0000 - rmse: 1925.9875 - val_loss: 4038407.2500 - val_rmse: 2009.5789 - lr: 1.0000e-04\n",
            "Epoch 36/50\n",
            "665/675 [============================>.] - ETA: 0s - loss: 3692290.2500 - rmse: 1921.5333\n",
            "Epoch 00036: val_loss did not improve from 3804973.75000\n",
            "675/675 [==============================] - 4s 6ms/step - loss: 3690311.2500 - rmse: 1921.0182 - val_loss: 4020971.0000 - val_rmse: 2005.2358 - lr: 6.0000e-05\n",
            "Epoch 37/50\n",
            "668/675 [============================>.] - ETA: 0s - loss: 3648098.0000 - rmse: 1909.9995\n",
            "Epoch 00037: val_loss did not improve from 3804973.75000\n",
            "675/675 [==============================] - 4s 6ms/step - loss: 3648875.0000 - rmse: 1910.2029 - val_loss: 4120865.7500 - val_rmse: 2029.9916 - lr: 6.0000e-05\n",
            "Epoch 38/50\n",
            "666/675 [============================>.] - ETA: 0s - loss: 3650361.7500 - rmse: 1910.5919\n",
            "Epoch 00038: val_loss did not improve from 3804973.75000\n",
            "675/675 [==============================] - 4s 6ms/step - loss: 3647033.7500 - rmse: 1909.7208 - val_loss: 4058342.5000 - val_rmse: 2014.5328 - lr: 6.0000e-05\n",
            "Epoch 39/50\n",
            "669/675 [============================>.] - ETA: 0s - loss: 3658054.7500 - rmse: 1912.6041\n",
            "Epoch 00039: val_loss did not improve from 3804973.75000\n",
            "675/675 [==============================] - 4s 6ms/step - loss: 3657556.5000 - rmse: 1912.4739 - val_loss: 4147043.5000 - val_rmse: 2036.4291 - lr: 6.0000e-05\n",
            "Epoch 40/50\n",
            "672/675 [============================>.] - ETA: 0s - loss: 3627600.7500 - rmse: 1904.6261\n",
            "Epoch 00040: val_loss did not improve from 3804973.75000\n",
            "675/675 [==============================] - 4s 6ms/step - loss: 3628933.0000 - rmse: 1904.9758 - val_loss: 4053551.0000 - val_rmse: 2013.3433 - lr: 6.0000e-05\n",
            "Epoch 41/50\n",
            "668/675 [============================>.] - ETA: 0s - loss: 3607835.2500 - rmse: 1899.4302\n",
            "Epoch 00041: val_loss did not improve from 3804973.75000\n",
            "675/675 [==============================] - 4s 6ms/step - loss: 3603634.2500 - rmse: 1898.3241 - val_loss: 4073013.2500 - val_rmse: 2018.1708 - lr: 6.0000e-05\n",
            "Epoch 42/50\n",
            "673/675 [============================>.] - ETA: 0s - loss: 3615997.2500 - rmse: 1901.5775\n",
            "Epoch 00042: val_loss did not improve from 3804973.75000\n",
            "675/675 [==============================] - 4s 6ms/step - loss: 3616454.0000 - rmse: 1901.6976 - val_loss: 4083989.7500 - val_rmse: 2020.8883 - lr: 6.0000e-05\n",
            "Epoch 43/50\n",
            "669/675 [============================>.] - ETA: 0s - loss: 3617001.7500 - rmse: 1901.8417\n",
            "Epoch 00043: val_loss did not improve from 3804973.75000\n",
            "675/675 [==============================] - 4s 6ms/step - loss: 3619236.0000 - rmse: 1902.4290 - val_loss: 4066207.2500 - val_rmse: 2016.4839 - lr: 6.0000e-05\n",
            "Epoch 44/50\n",
            "675/675 [==============================] - ETA: 0s - loss: 3605075.0000 - rmse: 1898.7035\n",
            "Epoch 00044: val_loss did not improve from 3804973.75000\n",
            "675/675 [==============================] - 4s 6ms/step - loss: 3605075.0000 - rmse: 1898.7035 - val_loss: 4037465.5000 - val_rmse: 2009.3445 - lr: 6.0000e-05\n",
            "Epoch 45/50\n",
            "670/675 [============================>.] - ETA: 0s - loss: 3608101.5000 - rmse: 1899.5002\n",
            "Epoch 00045: val_loss did not improve from 3804973.75000\n",
            "675/675 [==============================] - 4s 6ms/step - loss: 3609512.0000 - rmse: 1899.8716 - val_loss: 4096459.5000 - val_rmse: 2023.9712 - lr: 6.0000e-05\n",
            "Epoch 46/50\n",
            "672/675 [============================>.] - ETA: 0s - loss: 3601209.2500 - rmse: 1897.6852\n",
            "Epoch 00046: val_loss did not improve from 3804973.75000\n",
            "675/675 [==============================] - 4s 6ms/step - loss: 3602076.0000 - rmse: 1897.9136 - val_loss: 4058319.7500 - val_rmse: 2014.5271 - lr: 6.0000e-05\n",
            "Epoch 47/50\n",
            "675/675 [==============================] - ETA: 0s - loss: 3600305.5000 - rmse: 1897.4471\n",
            "Epoch 00047: val_loss did not improve from 3804973.75000\n",
            "675/675 [==============================] - 4s 6ms/step - loss: 3600305.5000 - rmse: 1897.4471 - val_loss: 4089992.5000 - val_rmse: 2022.3729 - lr: 6.0000e-05\n",
            "Epoch 48/50\n",
            "673/675 [============================>.] - ETA: 0s - loss: 3589125.0000 - rmse: 1894.4987\n",
            "Epoch 00048: val_loss did not improve from 3804973.75000\n",
            "675/675 [==============================] - 4s 6ms/step - loss: 3590800.7500 - rmse: 1894.9408 - val_loss: 4083991.5000 - val_rmse: 2020.8888 - lr: 6.0000e-05\n",
            "Epoch 49/50\n",
            "674/675 [============================>.] - ETA: 0s - loss: 3563469.7500 - rmse: 1887.7155\n",
            "Epoch 00049: val_loss did not improve from 3804973.75000\n",
            "675/675 [==============================] - 4s 6ms/step - loss: 3563199.5000 - rmse: 1887.6439 - val_loss: 4113274.0000 - val_rmse: 2028.1208 - lr: 6.0000e-05\n",
            "Epoch 50/50\n",
            "675/675 [==============================] - ETA: 0s - loss: 3546989.2500 - rmse: 1883.3452\n",
            "Epoch 00050: val_loss did not improve from 3804973.75000\n",
            "675/675 [==============================] - 4s 6ms/step - loss: 3546989.2500 - rmse: 1883.3452 - val_loss: 4073042.0000 - val_rmse: 2018.1779 - lr: 6.0000e-05\n",
            "75/75 [==============================] - 0s 4ms/step - loss: 4073042.0000 - rmse: 2018.1779\n",
            "\n",
            "parameters:\t[[1.024e+03 1.000e-04 1.000e-05 2.000e+00 6.000e-01 4.000e+00]] LOSS:\t4073042.0 \t rmse:\t2018.1778564453125\n",
            "[4073042.0, 2018.1778564453125]\n",
            "\n",
            "[WARNING] city.h5 already exists - overwrite? [y/n]y\n",
            "[TIP] Next time specify overwrite=True!\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x576 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Hyperparameters: [[1.024e+03 1.000e-04 1.000e-04 2.000e+00 6.000e-01 4.000e+00]]\n",
            "75/75 [==============================] - 0s 5ms/step - loss: 4073042.0000 - rmse: 2018.1779\n"
          ]
        },
        {
          "ename": "InternalError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-1dc267bc808a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     87\u001b[0m                                  \u001b[0mnormalize_Y\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                                 \u001b[0;31m#  maximize=True, 求最小值\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                                  verbosity=True)\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/GPyOpt/methods/bayesian_optimization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, domain, constraints, cost_withGradients, model_type, X, Y, initial_design_numdata, initial_design_type, acquisition_type, normalize_Y, exact_feval, acquisition_optimizer_type, model_update_interval, evaluator_type, batch_size, num_cores, verbosity, verbosity_model, maximize, de_duplication, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_design_type\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0minitial_design_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_design_numdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitial_design_numdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_design_chooser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;31m# --- CHOOSE the model type. If an instance of a GPyOpt model is passed (possibly user defined), it is used.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/GPyOpt/methods/bayesian_optimization.py\u001b[0m in \u001b[0;36m_init_design_chooser\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitial_design\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_design_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_design_numdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m         \u001b[0;31m# Case 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/GPyOpt/core/task/objective.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_procs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mf_evals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost_evals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eval_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/GPyOpt/core/task/objective.py\u001b[0m in \u001b[0;36m_eval_func\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mst_time\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mrlt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mf_evals\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf_evals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrlt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mcost_evals\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mst_time\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-53-1dc267bc808a>\u001b[0m in \u001b[0;36mf\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     37\u001b[0m                                         \u001b[0mlayer_number\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                                         )\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_reduce\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m   \u001b[0mevaluation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-51-6e6102e310c4>\u001b[0m in \u001b[0;36mfit_model\u001b[0;34m(model, lr_reduce, monitor, checkpoint)\u001b[0m\n\u001b[1;32m      7\u001b[0m                       \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlr_reduce\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m                       \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_validate_cut\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sqft'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m                       shuffle=True)\n\u001b[0m\u001b[1;32m     10\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    104\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Y0AT9XiyVTo"
      },
      "source": [
        "##### load the model to predict and submit file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcBY2My0YinV",
        "outputId": "b8f17523-1872-4fa4-c6da-d21098cbb9db"
      },
      "source": [
        "best_model = load_model('city.h5')\n",
        "\n",
        "submit_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "submit_generator = submit_datagen.flow_from_dataframe(\n",
        "        dataframe=df_test,\n",
        "        directory=PATH,\n",
        "        x_col=\"filename\",\n",
        "        batch_size = 1,\n",
        "        shuffle = False,\n",
        "        target_size=(HEIGHT, WIDTH),\n",
        "        class_mode=None)\n",
        "\n",
        "submit_generator.reset()\n",
        "pred = best_model.predict(submit_generator,steps=len(df_test))\n",
        "df_submit = pd.DataFrame({\"id\":df_test['id'],'sqft':pred[:,0].flatten()})\n",
        "df_submit.to_csv(\"/content/drive/My Drive/Colab Notebooks/applications-of-deep-learning-wustlfall-2021/submit11.csv\",index = False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "250/250 [==============================] - 66s 239ms/step - loss: 7363987.5000 - rmse: 2713.6667 - val_loss: 30798560.0000 - val_rmse: 5549.6450\n",
            "Epoch 2/300\n",
            "250/250 [==============================] - 58s 230ms/step - loss: 1669661.0000 - rmse: 1292.1537 - val_loss: 30058838.0000 - val_rmse: 5482.5942\n",
            "Epoch 3/300\n",
            "250/250 [==============================] - 58s 233ms/step - loss: 1517575.3750 - rmse: 1231.8990 - val_loss: 6049298.0000 - val_rmse: 2459.5320\n",
            "Epoch 4/300\n",
            "250/250 [==============================] - 58s 230ms/step - loss: 1416379.3750 - rmse: 1190.1173 - val_loss: 1383230.7500 - val_rmse: 1176.1083\n",
            "Epoch 5/300\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 1392923.3750 - rmse: 1180.2217 - val_loss: 1439791.1250 - val_rmse: 1199.9130\n",
            "Epoch 6/300\n",
            "250/250 [==============================] - 58s 230ms/step - loss: 1366217.2500 - rmse: 1168.8529 - val_loss: 809844.8125 - val_rmse: 899.9138\n",
            "Epoch 7/300\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 1290442.1250 - rmse: 1135.9763 - val_loss: 1786601.2500 - val_rmse: 1336.6381\n",
            "Epoch 8/300\n",
            "250/250 [==============================] - 58s 231ms/step - loss: 1390184.7500 - rmse: 1179.0609 - val_loss: 674030.0000 - val_rmse: 820.9933\n",
            "Epoch 9/300\n",
            "250/250 [==============================] - 58s 230ms/step - loss: 1271122.7500 - rmse: 1127.4408 - val_loss: 782630.6875 - val_rmse: 884.6642\n",
            "Epoch 10/300\n",
            "250/250 [==============================] - 57s 229ms/step - loss: 1186863.6250 - rmse: 1089.4327 - val_loss: 1505524.5000 - val_rmse: 1226.9982\n",
            "Epoch 11/300\n",
            "250/250 [==============================] - 57s 229ms/step - loss: 1227311.0000 - rmse: 1107.8407 - val_loss: 1629811.5000 - val_rmse: 1276.6407\n",
            "Epoch 12/300\n",
            "250/250 [==============================] - 57s 229ms/step - loss: 1194605.3750 - rmse: 1092.9801 - val_loss: 914377.7500 - val_rmse: 956.2310\n",
            "Epoch 13/300\n",
            "250/250 [==============================] - 57s 229ms/step - loss: 1223517.5000 - rmse: 1106.1272 - val_loss: 2625400.7500 - val_rmse: 1620.3088\n",
            "Epoch 14/300\n",
            "250/250 [==============================] - 57s 228ms/step - loss: 1162990.3750 - rmse: 1078.4203 - val_loss: 1960613.1250 - val_rmse: 1400.2190\n",
            "Epoch 15/300\n",
            "250/250 [==============================] - 57s 229ms/step - loss: 1099538.0000 - rmse: 1048.5885 - val_loss: 716293.3750 - val_rmse: 846.3412\n",
            "Epoch 16/300\n",
            "250/250 [==============================] - 57s 229ms/step - loss: 1119877.8750 - rmse: 1058.2428 - val_loss: 2433965.5000 - val_rmse: 1560.1171\n",
            "Epoch 17/300\n",
            "250/250 [==============================] - 57s 228ms/step - loss: 1068267.5000 - rmse: 1033.5702 - val_loss: 1942480.2500 - val_rmse: 1393.7289\n",
            "Epoch 18/300\n",
            "250/250 [==============================] - 57s 228ms/step - loss: 1071955.8750 - rmse: 1035.3530 - val_loss: 866538.3125 - val_rmse: 930.8804\n",
            "Epoch 19/300\n",
            "250/250 [==============================] - 57s 228ms/step - loss: 1124639.0000 - rmse: 1060.4900 - val_loss: 835275.7500 - val_rmse: 913.9342\n",
            "Epoch 20/300\n",
            "250/250 [==============================] - 57s 228ms/step - loss: 1077422.2500 - rmse: 1037.9895 - val_loss: 880124.5000 - val_rmse: 938.1495\n",
            "Epoch 21/300\n",
            "250/250 [==============================] - 57s 229ms/step - loss: 1089136.8750 - rmse: 1043.6172 - val_loss: 2083672.5000 - val_rmse: 1443.4932\n",
            "Epoch 22/300\n",
            "250/250 [==============================] - 58s 230ms/step - loss: 1055068.1250 - rmse: 1027.1650 - val_loss: 581115.4375 - val_rmse: 762.3093\n",
            "Epoch 23/300\n",
            "250/250 [==============================] - 58s 230ms/step - loss: 991621.6250 - rmse: 995.8020 - val_loss: 2833069.2500 - val_rmse: 1683.1724\n",
            "Epoch 24/300\n",
            "250/250 [==============================] - 57s 229ms/step - loss: 1073996.2500 - rmse: 1036.3379 - val_loss: 1479350.5000 - val_rmse: 1216.2855\n",
            "Epoch 25/300\n",
            "250/250 [==============================] - 57s 228ms/step - loss: 994070.3125 - rmse: 997.0307 - val_loss: 593756.6250 - val_rmse: 770.5560\n",
            "Epoch 26/300\n",
            "250/250 [==============================] - 57s 230ms/step - loss: 1048044.9375 - rmse: 1023.7407 - val_loss: 1062563.5000 - val_rmse: 1030.8073\n",
            "Epoch 27/300\n",
            "250/250 [==============================] - 57s 229ms/step - loss: 1027593.0000 - rmse: 1013.7026 - val_loss: 1169439.3750 - val_rmse: 1081.4061\n",
            "Epoch 28/300\n",
            "250/250 [==============================] - 57s 228ms/step - loss: 979836.6875 - rmse: 989.8670 - val_loss: 992023.8750 - val_rmse: 996.0040\n",
            "Epoch 29/300\n",
            "250/250 [==============================] - 57s 228ms/step - loss: 978159.6250 - rmse: 989.0195 - val_loss: 710686.7500 - val_rmse: 843.0224\n",
            "Epoch 30/300\n",
            "250/250 [==============================] - 57s 228ms/step - loss: 987145.3750 - rmse: 993.5519 - val_loss: 805934.3125 - val_rmse: 897.7384\n",
            "Epoch 31/300\n",
            "250/250 [==============================] - 58s 230ms/step - loss: 1009493.5625 - rmse: 1004.7355 - val_loss: 533021.0000 - val_rmse: 730.0829\n",
            "Epoch 32/300\n",
            "250/250 [==============================] - 57s 229ms/step - loss: 996348.3750 - rmse: 998.1725 - val_loss: 3467080.7500 - val_rmse: 1862.0099\n",
            "Epoch 33/300\n",
            "250/250 [==============================] - 57s 229ms/step - loss: 1004072.0000 - rmse: 1002.0339 - val_loss: 729081.4375 - val_rmse: 853.8627\n",
            "Epoch 34/300\n",
            "250/250 [==============================] - 57s 229ms/step - loss: 952283.3125 - rmse: 975.8500 - val_loss: 652136.0000 - val_rmse: 807.5494\n",
            "Epoch 35/300\n",
            "250/250 [==============================] - 57s 229ms/step - loss: 928059.4375 - rmse: 963.3584 - val_loss: 2080464.5000 - val_rmse: 1442.3815\n",
            "Epoch 36/300\n",
            "250/250 [==============================] - 57s 228ms/step - loss: 998802.3125 - rmse: 999.4010 - val_loss: 942816.3125 - val_rmse: 970.9872\n",
            "Epoch 37/300\n",
            "250/250 [==============================] - 57s 229ms/step - loss: 956340.4375 - rmse: 977.9266 - val_loss: 1562640.2500 - val_rmse: 1250.0560\n",
            "Epoch 38/300\n",
            "250/250 [==============================] - 57s 229ms/step - loss: 924349.0000 - rmse: 961.4307 - val_loss: 2630684.5000 - val_rmse: 1621.9385\n",
            "Epoch 39/300\n",
            "250/250 [==============================] - 57s 229ms/step - loss: 936139.5625 - rmse: 967.5430 - val_loss: 1356518.8750 - val_rmse: 1164.6969\n",
            "Epoch 40/300\n",
            "250/250 [==============================] - 58s 230ms/step - loss: 934578.3750 - rmse: 966.7359 - val_loss: 931028.1250 - val_rmse: 964.8980\n",
            "Epoch 41/300\n",
            "250/250 [==============================] - 57s 229ms/step - loss: 912096.7500 - rmse: 955.0375 - val_loss: 2478848.5000 - val_rmse: 1574.4359\n",
            "Epoch 42/300\n",
            "250/250 [==============================] - 57s 228ms/step - loss: 908891.2500 - rmse: 953.3579 - val_loss: 544152.6875 - val_rmse: 737.6671\n",
            "Epoch 43/300\n",
            "250/250 [==============================] - 57s 229ms/step - loss: 915517.1250 - rmse: 956.8266 - val_loss: 665232.5000 - val_rmse: 815.6179\n",
            "Epoch 44/300\n",
            "250/250 [==============================] - 57s 228ms/step - loss: 917737.6875 - rmse: 957.9863 - val_loss: 1194972.8750 - val_rmse: 1093.1482\n",
            "Epoch 45/300\n",
            "250/250 [==============================] - 57s 229ms/step - loss: 864459.0000 - rmse: 929.7629 - val_loss: 1613074.5000 - val_rmse: 1270.0687\n",
            "Epoch 46/300\n",
            " 48/250 [====>.........................] - ETA: 37s - loss: 934500.8750 - rmse: 966.6959"
          ]
        }
      ]
    }
  ]
}